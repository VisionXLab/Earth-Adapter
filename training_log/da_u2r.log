2025/03/29 11:47:08 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.8.19 (default, Mar 20 2024, 19:58:24) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 0
    GPU 0: NVIDIA RTX A6000
    CUDA_HOME: /usr/local/cuda-11.2
    NVCC: Cuda compilation tools, release 11.2, V11.2.67
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
    PyTorch: 2.1.1+cu118
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.1+cu118
    OpenCV: 4.9.0
    MMEngine: 0.10.4

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 0
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/03/29 11:47:08 - mmengine - INFO - Config:
crop_size = (
    512,
    512,
)
data_root = '/data/xiaoxinghhh/dataset/remote_sensing_new'
dataset_type = 'LoveDADataset'
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=2000,
        max_keep_ckpts=1,
        save_best='mIoU',
        type='CheckpointHook'),
    logger=dict(interval=50, log_metric_by_epoch=False, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='SegVisualizationHook'))
default_scope = 'mmseg'
embed_multi = dict(decay_mult=0.0, lr_mult=1.0)
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
exp_name = 'DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1'
img_ratios = [
    0.5,
    0.75,
    1.0,
    1.25,
    1.5,
    1.75,
]
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
model = dict(
    backbone=dict(
        adapter_config=dict(
            cutoff_ratio=0.2,
            dim=32,
            fft_layer=[
                18,
                19,
                20,
                21,
                22,
                23,
            ],
            scale=0.1),
        block_chunks=0,
        depth=24,
        embed_dim=1024,
        ffn_bias=True,
        ffn_layer='mlp',
        img_size=512,
        init_cfg=dict(
            checkpoint='checkpoints/dinov2_converted.pth', type='Pretrained'),
        init_values=1e-05,
        mlp_ratio=4,
        moe_adapter_type='earth_adapter',
        num_heads=16,
        patch_size=16,
        proj_bias=True,
        qkv_bias=True,
        type='MOE_Adpter_DinoVisionTransformer'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_val=0,
        seg_pad_val=255,
        size=(
            512,
            512,
        ),
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='SegDataPreProcessor'),
    decode_head=dict(
        align_corners=False,
        enforce_decoder_input_project=False,
        feat_channels=256,
        in_channels=[
            1024,
            1024,
            1024,
            1024,
        ],
        loss_cls=dict(
            class_weight=[
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                0.1,
            ],
            loss_weight=2.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False),
        loss_dice=dict(
            activate=True,
            eps=1.0,
            loss_weight=5.0,
            naive_dice=True,
            reduction='mean',
            type='mmdet.DiceLoss',
            use_sigmoid=True),
        loss_mask=dict(
            loss_weight=5.0,
            reduction='mean',
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True),
        num_classes=7,
        num_queries=100,
        num_transformer_feat_level=3,
        out_channels=256,
        pixel_decoder=dict(
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                init_cfg=None,
                layer_cfg=dict(
                    ffn_cfg=dict(
                        act_cfg=dict(inplace=True, type='ReLU'),
                        embed_dims=256,
                        feedforward_channels=1024,
                        ffn_drop=0.0,
                        num_fcs=2),
                    self_attn_cfg=dict(
                        batch_first=True,
                        dropout=0.0,
                        embed_dims=256,
                        im2col_step=64,
                        init_cfg=None,
                        norm_cfg=None,
                        num_heads=8,
                        num_levels=3,
                        num_points=4)),
                num_layers=6),
            init_cfg=None,
            norm_cfg=dict(num_groups=32, type='GN'),
            num_outs=3,
            positional_encoding=dict(normalize=True, num_feats=128),
            type='mmdet.MSDeformAttnPixelDecoder'),
        positional_encoding=dict(normalize=True, num_feats=128),
        strides=[
            4,
            8,
            16,
            32,
        ],
        train_cfg=dict(
            assigner=dict(
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        use_sigmoid=True,
                        weight=5.0),
                    dict(
                        eps=1.0,
                        pred_act=True,
                        type='mmdet.DiceCost',
                        weight=5.0),
                ],
                type='mmdet.HungarianAssigner'),
            importance_sample_ratio=0.75,
            num_points=12544,
            oversample_ratio=3.0,
            sampler=dict(type='mmdet.MaskPseudoSampler')),
        transformer_decoder=dict(
            init_cfg=None,
            layer_cfg=dict(
                cross_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0),
                ffn_cfg=dict(
                    act_cfg=dict(inplace=True, type='ReLU'),
                    add_identity=True,
                    dropout_layer=None,
                    embed_dims=256,
                    feedforward_channels=2048,
                    ffn_drop=0.0,
                    num_fcs=2),
                self_attn_cfg=dict(
                    attn_drop=0.0,
                    batch_first=True,
                    dropout_layer=None,
                    embed_dims=256,
                    num_heads=8,
                    proj_drop=0.0)),
            num_layers=9,
            return_intermediate=True),
        type='Mask2FormerHead'),
    test_cfg=dict(crop_size=(
        512,
        512,
    ), mode='slide', stride=(
        341,
        341,
    )),
    train_cfg=dict(),
    type='DACS_encoder_decoder')
num_classes = 19
optim_wrapper = dict(
    constructor='PEFTOptimWrapperConstructor',
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        eps=1e-08,
        lr=0.0001,
        type='AdamW',
        weight_decay=0.05),
    paramwise_cfg=dict(
        custom_keys=dict({
            'learnable_tokens': dict(decay_mult=0.0, lr_mult=1.0),
            'level_embed': dict(decay_mult=0.0, lr_mult=1.0),
            'norm': dict(decay_mult=0.0),
            'query_embed': dict(decay_mult=0.0, lr_mult=1.0),
            'reins.scale': dict(decay_mult=0.0, lr_mult=1.0)
        }),
        norm_decay_mult=0.0))
param_scheduler = [
    dict(
        begin=0,
        by_epoch=False,
        end=20000,
        eta_min=0,
        power=0.9,
        type='PolyLR'),
]
randomness = dict(seed=0)
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(
            img_path='loveda_uda/rural/val/img_dir',
            seg_map_path='loveda_uda/rural/val/ann_dir'),
        data_root='/data/xiaoxinghhh/dataset/remote_sensing_new',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(reduce_zero_label=True, type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='LoveDADataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    iou_metrics=[
        'mIoU',
    ], type='IoUMetric')
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        512,
        512,
    ), type='Resize'),
    dict(reduce_zero_label=True, type='LoadAnnotations'),
    dict(type='PackSegInputs'),
]
train_cfg = dict(max_iters=20000, type='IterBasedTrainLoop', val_interval=2000)
train_dataloader = dict(
    batch_size=2,
    dataset=dict(
        source_dataset=dict(
            data_prefix=dict(
                img_path='loveda_uda/urban/train/img_dir',
                seg_map_path='loveda_uda/urban/train/ann_dir'),
            data_root='/data/xiaoxinghhh/dataset/remote_sensing_new',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(reduce_zero_label=True, type='LoadAnnotations'),
                dict(
                    keep_ratio=True,
                    ratio_range=(
                        0.5,
                        2.0,
                    ),
                    scale=(
                        512,
                        512,
                    ),
                    type='RandomResize'),
                dict(
                    cat_max_ratio=0.75,
                    crop_size=(
                        512,
                        512,
                    ),
                    type='RandomCrop'),
                dict(prob=0.5, type='RandomFlip'),
                dict(type='PhotoMetricDistortion'),
                dict(type='PackSegInputs'),
            ],
            type='LoveDADataset'),
        target_dataset=dict(
            data_prefix=dict(
                img_path='loveda_uda/rural/train/img_dir',
                seg_map_path='loveda_uda/rural/train/ann_dir'),
            data_root='/data/xiaoxinghhh/dataset/remote_sensing_new',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(reduce_zero_label=True, type='LoadAnnotations'),
                dict(
                    keep_ratio=True,
                    ratio_range=(
                        0.5,
                        2.0,
                    ),
                    scale=(
                        512,
                        512,
                    ),
                    type='RandomResize'),
                dict(
                    cat_max_ratio=0.75,
                    crop_size=(
                        512,
                        512,
                    ),
                    type='RandomCrop'),
                dict(prob=0.5, type='RandomFlip'),
                dict(type='PhotoMetricDistortion'),
                dict(type='PackSegInputs'),
            ],
            type='LoveDADataset'),
        type='UDA_dataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='InfiniteSampler'))
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(reduce_zero_label=True, type='LoadAnnotations'),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.5,
            2.0,
        ),
        scale=(
            512,
            512,
        ),
        type='RandomResize'),
    dict(cat_max_ratio=0.75, crop_size=(
        512,
        512,
    ), type='RandomCrop'),
    dict(prob=0.5, type='RandomFlip'),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs'),
]
tta_model = dict(type='SegTTAModel')
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_prefix=dict(
            img_path='loveda_uda/rural/val/img_dir',
            seg_map_path='loveda_uda/rural/val/ann_dir'),
        data_root='/data/xiaoxinghhh/dataset/remote_sensing_new',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                512,
                512,
            ), type='Resize'),
            dict(reduce_zero_label=True, type='LoadAnnotations'),
            dict(type='PackSegInputs'),
        ],
        type='LoveDADataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    iou_metrics=[
        'mIoU',
    ], type='IoUMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='SegLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0'

2025/03/29 11:47:13 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/03/29 11:47:13 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SegVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SegVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/03/29 11:47:14 - mmengine - WARNING - Dataset UDA_dataset has no metainfo. ``dataset_meta`` in visualizer will be None.
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.scale
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.1.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.1.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.3.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.3.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.4.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.4.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.5.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.5.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.6.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.6.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.7.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.7.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.8.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.8.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.9.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.9.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.10.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.10.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.11.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.11.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.12.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.12.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.13.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.13.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.14.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.14.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.15.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.15.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.16.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.16.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.17.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.17.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.18.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.18.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.19.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.19.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.20.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.20.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.21.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.21.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.22.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.22.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.23.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.layer_norm.23.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.0.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.0.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.0.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.0.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.1.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.1.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.1.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.1.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.2.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.2.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.2.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.2.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.3.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.3.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.3.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.3.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.4.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.4.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.4.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.4.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.5.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.5.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.5.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.5.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.6.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.6.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.6.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.6.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.7.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.7.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.7.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.7.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.8.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.8.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.8.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.8.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.9.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.9.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.9.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.9.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.10.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.10.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.10.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.10.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.11.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.11.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.11.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.11.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.12.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.12.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.12.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.12.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.13.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.13.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.13.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.13.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.14.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.14.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.14.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.14.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.15.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.15.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.15.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.15.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.16.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.16.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.16.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.16.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.17.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.17.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.17.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.17.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.18.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.18.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.18.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.18.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.19.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.19.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.19.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.19.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.20.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.20.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.20.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.20.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.21.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.21.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.21.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.21.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.22.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.22.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.22.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.22.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.23.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.23.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.23.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list1.23.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.0.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.0.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.0.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.0.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.1.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.1.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.1.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.1.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.2.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.2.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.2.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.2.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.3.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.3.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.3.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.3.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.4.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.4.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.4.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.4.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.5.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.5.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.5.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.5.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.6.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.6.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.6.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.6.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.7.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.7.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.7.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.7.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.8.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.8.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.8.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.8.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.9.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.9.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.9.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.9.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.10.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.10.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.10.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.10.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.11.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.11.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.11.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.11.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.12.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.12.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.12.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.12.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.13.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.13.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.13.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.13.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.14.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.14.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.14.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.14.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.15.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.15.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.15.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.15.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.16.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.16.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.16.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.16.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.17.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.17.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.17.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.17.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.18.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.18.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.18.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.18.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.19.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.19.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.19.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.19.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.20.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.20.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.20.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.20.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.21.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.21.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.21.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.21.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.22.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.22.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.22.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.22.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.23.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.23.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.23.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list2.23.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.0.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.0.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.0.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.0.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.1.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.1.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.1.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.1.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.2.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.2.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.2.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.2.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.3.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.3.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.3.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.3.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.4.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.4.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.4.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.4.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.5.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.5.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.5.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.5.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.6.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.6.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.6.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.6.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.7.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.7.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.7.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.7.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.8.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.8.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.8.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.8.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.9.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.9.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.9.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.9.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.10.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.10.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.10.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.10.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.11.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.11.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.11.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.11.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.12.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.12.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.12.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.12.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.13.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.13.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.13.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.13.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.14.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.14.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.14.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.14.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.15.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.15.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.15.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.15.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.16.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.16.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.16.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.16.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.17.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.17.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.17.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.17.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.18.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.18.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.18.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.18.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.19.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.19.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.19.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.19.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.20.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.20.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.20.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.20.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.21.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.21.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.21.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.21.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.22.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.22.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.22.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.22.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.23.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.23.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.23.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.mlp_list3.23.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.0.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.0.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.1.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.1.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.2.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.2.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.3.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.3.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.4.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.4.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.5.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.5.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.6.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.6.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.7.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.7.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.8.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.8.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.9.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.9.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.10.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.10.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.11.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.11.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.12.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.12.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.13.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.13.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.14.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.14.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.15.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.15.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.16.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.16.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.17.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.17.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.18.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.18.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.19.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.19.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.20.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.20.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.21.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.21.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.22.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.22.bias
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.23.weight
2025/03/29 11:47:14 - mmengine - INFO - set_requires_grad----refine_feat.router.23.bias
2025/03/29 11:47:14 - mmengine - INFO - Total trainable params--4917600, All params--309117280, Ratio--1.6%
2025/03/29 11:47:14 - mmengine - INFO - set_train----.refine_feat
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.scale:num of params=24
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.3.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.4.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.5.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.6.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.7.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.8.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.9.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.10.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.11.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.12.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.13.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.14.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.15.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.16.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.17.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.18.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.19.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.20.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.21.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.22.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.layer_norm.23.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.0.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.0.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.0.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.0.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.1.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.1.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.1.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.1.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.2.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.2.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.2.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.2.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.3.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.3.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.3.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.3.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.4.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.4.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.4.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.4.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.5.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.5.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.5.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.5.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.6.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.6.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.6.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.6.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.7.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.7.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.7.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.7.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.8.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.8.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.8.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.8.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.9.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.9.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.9.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.9.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.10.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.10.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.10.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.10.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.11.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.11.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.11.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.11.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.12.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.12.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.12.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.12.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.13.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.13.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.13.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.13.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.14.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.14.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.14.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.14.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.15.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.15.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.15.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.15.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.16.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.16.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.16.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.16.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.17.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.17.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.17.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.17.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.18.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.18.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.18.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.18.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.19.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.19.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.19.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.19.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.20.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.20.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.20.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.20.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.21.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.21.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.21.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.21.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.22.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.22.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.22.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.22.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.23.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.23.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.23.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list1.23.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.0.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.0.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.0.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.0.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.1.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.1.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.1.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.1.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.2.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.2.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.2.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.2.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.3.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.3.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.3.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.3.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.4.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.4.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.4.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.4.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.5.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.5.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.5.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.5.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.6.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.6.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.6.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.6.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.7.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.7.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.7.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.7.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.8.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.8.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.8.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.8.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.9.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.9.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.9.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.9.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.10.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.10.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.10.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.10.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.11.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.11.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.11.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.11.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.12.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.12.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.12.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.12.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.13.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.13.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.13.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.13.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.14.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.14.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.14.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.14.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.15.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.15.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.15.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.15.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.16.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.16.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.16.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.16.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.17.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.17.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.17.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.17.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.18.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.18.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.18.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.18.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.19.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.19.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.19.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.19.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.20.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.20.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.20.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.20.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.21.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.21.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.21.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.21.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.22.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.22.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.22.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.22.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.23.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.23.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.23.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list2.23.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.0.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.0.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.0.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.0.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.1.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.1.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.1.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.1.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.2.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.2.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.2.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.2.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.3.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.3.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.3.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.3.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.4.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.4.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.4.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.4.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.5.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.5.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.5.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.5.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.6.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.6.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.6.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.6.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.7.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.7.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.7.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.7.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.8.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.8.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.8.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.8.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.9.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.9.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.9.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.9.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.10.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.10.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.10.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.10.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.11.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.11.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.11.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.11.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.12.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.12.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.12.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.12.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.13.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.13.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.13.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.13.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.14.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.14.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.14.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.14.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.15.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.15.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.15.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.15.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.16.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.16.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.16.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.16.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.17.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.17.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.17.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.17.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.18.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.18.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.18.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.18.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.19.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.19.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.19.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.19.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.20.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.20.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.20.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.20.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.21.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.21.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.21.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.21.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.22.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.22.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.22.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.22.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.23.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.23.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.23.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.mlp_list3.23.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.0.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.0.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.1.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.1.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.2.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.2.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.3.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.3.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.4.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.4.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.5.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.5.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.6.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.6.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.7.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.7.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.8.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.8.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.9.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.9.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.10.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.10.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.11.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.11.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.12.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.12.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.13.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.13.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.14.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.14.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.15.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.15.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.16.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.16.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.17.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.17.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.18.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.18.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.19.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.19.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.20.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.20.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.21.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.21.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.22.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.22.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.23.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- backbone.refine_feat.router.23.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.conv.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.conv.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.conv.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.conv.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.conv.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.conv.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight:num of params=49152
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias:num of params=192
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight:num of params=24576
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias:num of params=96
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight:num of params=49152
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias:num of params=192
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight:num of params=24576
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias:num of params=96
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight:num of params=49152
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias:num of params=192
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight:num of params=24576
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias:num of params=96
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight:num of params=49152
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias:num of params=192
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight:num of params=24576
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias:num of params=96
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight:num of params=49152
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias:num of params=192
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight:num of params=24576
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias:num of params=96
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight:num of params=49152
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias:num of params=192
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight:num of params=24576
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias:num of params=96
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.level_encoding.weight:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.conv.weight:num of params=262144
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.conv.weight:num of params=589824
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.mask_feature.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.mask_feature.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight:num of params=196608
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.ffn.layers.1.weight:num of params=524288
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.ffn.layers.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:num of params=25600
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:num of params=25600
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:num of params=768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.cls_embed.weight:num of params=2048
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.cls_embed.bias:num of params=8
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.mask_embed.0.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.mask_embed.0.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.mask_embed.2.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.mask_embed.2.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.mask_embed.4.weight:num of params=65536
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- decode_head.mask_embed.4.bias:num of params=256
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.scale:num of params=24
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.0.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.1.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.2.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.3.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.4.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.5.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.6.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.7.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.8.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.9.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.10.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.11.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.12.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.13.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.14.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.15.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.16.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.17.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.18.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.19.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.20.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.21.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.22.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.weight:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.weight:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.weight:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.weight:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.bias:lr=0.0001
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.bias:weight_decay=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.layer_norm.23.bias:decay_mult=0.0
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.0.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.0.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.0.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.0.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.1.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.1.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.1.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.1.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.2.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.2.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.2.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.2.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.3.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.3.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.3.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.3.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.4.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.4.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.4.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.4.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.5.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.5.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.5.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.5.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.6.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.6.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.6.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.6.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.7.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.7.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.7.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.7.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.8.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.8.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.8.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.8.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.9.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.9.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.9.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.9.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.10.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.10.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.10.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.10.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.11.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.11.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.11.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.11.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.12.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.12.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.12.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.12.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.13.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.13.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.13.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.13.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.14.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.14.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.14.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.14.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.15.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.15.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.15.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.15.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.16.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.16.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.16.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.16.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.17.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.17.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.17.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.17.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.18.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.18.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.18.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.18.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.19.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.19.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.19.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.19.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.20.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.20.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.20.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.20.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.21.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.21.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.21.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.21.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.22.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.22.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.22.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.22.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.23.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.23.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.23.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list1.23.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.0.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.0.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.0.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.0.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.1.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.1.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.1.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.1.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.2.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.2.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.2.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.2.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.3.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.3.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.3.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.3.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.4.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.4.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.4.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.4.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.5.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.5.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.5.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.5.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.6.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.6.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.6.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.6.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.7.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.7.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.7.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.7.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.8.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.8.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.8.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.8.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.9.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.9.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.9.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.9.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.10.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.10.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.10.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.10.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.11.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.11.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.11.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.11.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.12.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.12.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.12.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.12.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.13.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.13.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.13.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.13.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.14.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.14.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.14.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.14.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.15.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.15.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.15.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.15.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.16.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.16.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.16.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.16.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.17.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.17.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.17.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.17.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.18.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.18.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.18.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.18.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.19.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.19.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.19.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.19.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.20.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.20.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.20.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.20.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.21.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.21.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.21.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.21.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.22.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.22.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.22.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.22.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.23.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.23.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.23.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list2.23.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.0.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.0.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.0.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.0.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.1.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.1.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.1.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.1.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.2.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.2.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.2.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.2.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.3.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.3.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.3.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.3.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.4.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.4.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.4.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.4.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.5.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.5.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.5.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.5.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.6.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.6.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.6.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.6.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.7.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.7.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.7.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.7.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.8.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.8.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.8.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.8.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.9.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.9.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.9.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.9.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.10.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.10.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.10.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.10.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.11.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.11.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.11.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.11.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.12.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.12.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.12.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.12.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.13.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.13.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.13.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.13.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.14.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.14.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.14.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.14.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.15.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.15.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.15.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.15.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.16.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.16.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.16.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.16.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.17.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.17.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.17.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.17.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.18.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.18.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.18.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.18.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.19.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.19.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.19.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.19.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.20.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.20.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.20.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.20.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.21.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.21.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.21.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.21.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.22.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.22.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.22.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.22.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.23.0.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.23.0.bias:num of params=32
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.23.2.weight:num of params=32768
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.mlp_list3.23.2.bias:num of params=1024
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.0.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.0.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.1.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.1.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.2.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.2.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.3.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.3.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.4.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.4.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.5.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.5.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.6.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.6.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.7.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.7.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.8.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.8.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.9.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.9.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.10.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.10.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.11.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.11.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.12.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.12.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.13.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.13.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.14.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.14.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.15.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.15.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.16.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.16.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.17.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.17.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.18.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.18.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.19.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.19.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.20.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.20.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.21.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.21.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.22.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.22.bias:num of params=3
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.23.weight:num of params=3072
2025/03/29 11:47:14 - mmengine - INFO - paramwise_options -- ema_model.backbone.refine_feat.router.23.bias:num of params=3
2025/03/29 11:47:15 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
2025/03/29 11:47:15 - mmengine - INFO - load model from: checkpoints/dinov2_converted.pth
2025/03/29 11:47:15 - mmengine - INFO - Loads checkpoint by local backend from path: checkpoints/dinov2_converted.pth
2025/03/29 11:47:16 - mmengine - WARNING - The model and loaded state dict do not match exactly

missing keys in source state_dict: refine_feat.scale, refine_feat.layer_norm.0.weight, refine_feat.layer_norm.0.bias, refine_feat.layer_norm.1.weight, refine_feat.layer_norm.1.bias, refine_feat.layer_norm.2.weight, refine_feat.layer_norm.2.bias, refine_feat.layer_norm.3.weight, refine_feat.layer_norm.3.bias, refine_feat.layer_norm.4.weight, refine_feat.layer_norm.4.bias, refine_feat.layer_norm.5.weight, refine_feat.layer_norm.5.bias, refine_feat.layer_norm.6.weight, refine_feat.layer_norm.6.bias, refine_feat.layer_norm.7.weight, refine_feat.layer_norm.7.bias, refine_feat.layer_norm.8.weight, refine_feat.layer_norm.8.bias, refine_feat.layer_norm.9.weight, refine_feat.layer_norm.9.bias, refine_feat.layer_norm.10.weight, refine_feat.layer_norm.10.bias, refine_feat.layer_norm.11.weight, refine_feat.layer_norm.11.bias, refine_feat.layer_norm.12.weight, refine_feat.layer_norm.12.bias, refine_feat.layer_norm.13.weight, refine_feat.layer_norm.13.bias, refine_feat.layer_norm.14.weight, refine_feat.layer_norm.14.bias, refine_feat.layer_norm.15.weight, refine_feat.layer_norm.15.bias, refine_feat.layer_norm.16.weight, refine_feat.layer_norm.16.bias, refine_feat.layer_norm.17.weight, refine_feat.layer_norm.17.bias, refine_feat.layer_norm.18.weight, refine_feat.layer_norm.18.bias, refine_feat.layer_norm.19.weight, refine_feat.layer_norm.19.bias, refine_feat.layer_norm.20.weight, refine_feat.layer_norm.20.bias, refine_feat.layer_norm.21.weight, refine_feat.layer_norm.21.bias, refine_feat.layer_norm.22.weight, refine_feat.layer_norm.22.bias, refine_feat.layer_norm.23.weight, refine_feat.layer_norm.23.bias, refine_feat.mlp_list1.0.0.weight, refine_feat.mlp_list1.0.0.bias, refine_feat.mlp_list1.0.2.weight, refine_feat.mlp_list1.0.2.bias, refine_feat.mlp_list1.1.0.weight, refine_feat.mlp_list1.1.0.bias, refine_feat.mlp_list1.1.2.weight, refine_feat.mlp_list1.1.2.bias, refine_feat.mlp_list1.2.0.weight, refine_feat.mlp_list1.2.0.bias, refine_feat.mlp_list1.2.2.weight, refine_feat.mlp_list1.2.2.bias, refine_feat.mlp_list1.3.0.weight, refine_feat.mlp_list1.3.0.bias, refine_feat.mlp_list1.3.2.weight, refine_feat.mlp_list1.3.2.bias, refine_feat.mlp_list1.4.0.weight, refine_feat.mlp_list1.4.0.bias, refine_feat.mlp_list1.4.2.weight, refine_feat.mlp_list1.4.2.bias, refine_feat.mlp_list1.5.0.weight, refine_feat.mlp_list1.5.0.bias, refine_feat.mlp_list1.5.2.weight, refine_feat.mlp_list1.5.2.bias, refine_feat.mlp_list1.6.0.weight, refine_feat.mlp_list1.6.0.bias, refine_feat.mlp_list1.6.2.weight, refine_feat.mlp_list1.6.2.bias, refine_feat.mlp_list1.7.0.weight, refine_feat.mlp_list1.7.0.bias, refine_feat.mlp_list1.7.2.weight, refine_feat.mlp_list1.7.2.bias, refine_feat.mlp_list1.8.0.weight, refine_feat.mlp_list1.8.0.bias, refine_feat.mlp_list1.8.2.weight, refine_feat.mlp_list1.8.2.bias, refine_feat.mlp_list1.9.0.weight, refine_feat.mlp_list1.9.0.bias, refine_feat.mlp_list1.9.2.weight, refine_feat.mlp_list1.9.2.bias, refine_feat.mlp_list1.10.0.weight, refine_feat.mlp_list1.10.0.bias, refine_feat.mlp_list1.10.2.weight, refine_feat.mlp_list1.10.2.bias, refine_feat.mlp_list1.11.0.weight, refine_feat.mlp_list1.11.0.bias, refine_feat.mlp_list1.11.2.weight, refine_feat.mlp_list1.11.2.bias, refine_feat.mlp_list1.12.0.weight, refine_feat.mlp_list1.12.0.bias, refine_feat.mlp_list1.12.2.weight, refine_feat.mlp_list1.12.2.bias, refine_feat.mlp_list1.13.0.weight, refine_feat.mlp_list1.13.0.bias, refine_feat.mlp_list1.13.2.weight, refine_feat.mlp_list1.13.2.bias, refine_feat.mlp_list1.14.0.weight, refine_feat.mlp_list1.14.0.bias, refine_feat.mlp_list1.14.2.weight, refine_feat.mlp_list1.14.2.bias, refine_feat.mlp_list1.15.0.weight, refine_feat.mlp_list1.15.0.bias, refine_feat.mlp_list1.15.2.weight, refine_feat.mlp_list1.15.2.bias, refine_feat.mlp_list1.16.0.weight, refine_feat.mlp_list1.16.0.bias, refine_feat.mlp_list1.16.2.weight, refine_feat.mlp_list1.16.2.bias, refine_feat.mlp_list1.17.0.weight, refine_feat.mlp_list1.17.0.bias, refine_feat.mlp_list1.17.2.weight, refine_feat.mlp_list1.17.2.bias, refine_feat.mlp_list1.18.0.weight, refine_feat.mlp_list1.18.0.bias, refine_feat.mlp_list1.18.2.weight, refine_feat.mlp_list1.18.2.bias, refine_feat.mlp_list1.19.0.weight, refine_feat.mlp_list1.19.0.bias, refine_feat.mlp_list1.19.2.weight, refine_feat.mlp_list1.19.2.bias, refine_feat.mlp_list1.20.0.weight, refine_feat.mlp_list1.20.0.bias, refine_feat.mlp_list1.20.2.weight, refine_feat.mlp_list1.20.2.bias, refine_feat.mlp_list1.21.0.weight, refine_feat.mlp_list1.21.0.bias, refine_feat.mlp_list1.21.2.weight, refine_feat.mlp_list1.21.2.bias, refine_feat.mlp_list1.22.0.weight, refine_feat.mlp_list1.22.0.bias, refine_feat.mlp_list1.22.2.weight, refine_feat.mlp_list1.22.2.bias, refine_feat.mlp_list1.23.0.weight, refine_feat.mlp_list1.23.0.bias, refine_feat.mlp_list1.23.2.weight, refine_feat.mlp_list1.23.2.bias, refine_feat.mlp_list2.0.0.weight, refine_feat.mlp_list2.0.0.bias, refine_feat.mlp_list2.0.2.weight, refine_feat.mlp_list2.0.2.bias, refine_feat.mlp_list2.1.0.weight, refine_feat.mlp_list2.1.0.bias, refine_feat.mlp_list2.1.2.weight, refine_feat.mlp_list2.1.2.bias, refine_feat.mlp_list2.2.0.weight, refine_feat.mlp_list2.2.0.bias, refine_feat.mlp_list2.2.2.weight, refine_feat.mlp_list2.2.2.bias, refine_feat.mlp_list2.3.0.weight, refine_feat.mlp_list2.3.0.bias, refine_feat.mlp_list2.3.2.weight, refine_feat.mlp_list2.3.2.bias, refine_feat.mlp_list2.4.0.weight, refine_feat.mlp_list2.4.0.bias, refine_feat.mlp_list2.4.2.weight, refine_feat.mlp_list2.4.2.bias, refine_feat.mlp_list2.5.0.weight, refine_feat.mlp_list2.5.0.bias, refine_feat.mlp_list2.5.2.weight, refine_feat.mlp_list2.5.2.bias, refine_feat.mlp_list2.6.0.weight, refine_feat.mlp_list2.6.0.bias, refine_feat.mlp_list2.6.2.weight, refine_feat.mlp_list2.6.2.bias, refine_feat.mlp_list2.7.0.weight, refine_feat.mlp_list2.7.0.bias, refine_feat.mlp_list2.7.2.weight, refine_feat.mlp_list2.7.2.bias, refine_feat.mlp_list2.8.0.weight, refine_feat.mlp_list2.8.0.bias, refine_feat.mlp_list2.8.2.weight, refine_feat.mlp_list2.8.2.bias, refine_feat.mlp_list2.9.0.weight, refine_feat.mlp_list2.9.0.bias, refine_feat.mlp_list2.9.2.weight, refine_feat.mlp_list2.9.2.bias, refine_feat.mlp_list2.10.0.weight, refine_feat.mlp_list2.10.0.bias, refine_feat.mlp_list2.10.2.weight, refine_feat.mlp_list2.10.2.bias, refine_feat.mlp_list2.11.0.weight, refine_feat.mlp_list2.11.0.bias, refine_feat.mlp_list2.11.2.weight, refine_feat.mlp_list2.11.2.bias, refine_feat.mlp_list2.12.0.weight, refine_feat.mlp_list2.12.0.bias, refine_feat.mlp_list2.12.2.weight, refine_feat.mlp_list2.12.2.bias, refine_feat.mlp_list2.13.0.weight, refine_feat.mlp_list2.13.0.bias, refine_feat.mlp_list2.13.2.weight, refine_feat.mlp_list2.13.2.bias, refine_feat.mlp_list2.14.0.weight, refine_feat.mlp_list2.14.0.bias, refine_feat.mlp_list2.14.2.weight, refine_feat.mlp_list2.14.2.bias, refine_feat.mlp_list2.15.0.weight, refine_feat.mlp_list2.15.0.bias, refine_feat.mlp_list2.15.2.weight, refine_feat.mlp_list2.15.2.bias, refine_feat.mlp_list2.16.0.weight, refine_feat.mlp_list2.16.0.bias, refine_feat.mlp_list2.16.2.weight, refine_feat.mlp_list2.16.2.bias, refine_feat.mlp_list2.17.0.weight, refine_feat.mlp_list2.17.0.bias, refine_feat.mlp_list2.17.2.weight, refine_feat.mlp_list2.17.2.bias, refine_feat.mlp_list2.18.0.weight, refine_feat.mlp_list2.18.0.bias, refine_feat.mlp_list2.18.2.weight, refine_feat.mlp_list2.18.2.bias, refine_feat.mlp_list2.19.0.weight, refine_feat.mlp_list2.19.0.bias, refine_feat.mlp_list2.19.2.weight, refine_feat.mlp_list2.19.2.bias, refine_feat.mlp_list2.20.0.weight, refine_feat.mlp_list2.20.0.bias, refine_feat.mlp_list2.20.2.weight, refine_feat.mlp_list2.20.2.bias, refine_feat.mlp_list2.21.0.weight, refine_feat.mlp_list2.21.0.bias, refine_feat.mlp_list2.21.2.weight, refine_feat.mlp_list2.21.2.bias, refine_feat.mlp_list2.22.0.weight, refine_feat.mlp_list2.22.0.bias, refine_feat.mlp_list2.22.2.weight, refine_feat.mlp_list2.22.2.bias, refine_feat.mlp_list2.23.0.weight, refine_feat.mlp_list2.23.0.bias, refine_feat.mlp_list2.23.2.weight, refine_feat.mlp_list2.23.2.bias, refine_feat.mlp_list3.0.0.weight, refine_feat.mlp_list3.0.0.bias, refine_feat.mlp_list3.0.2.weight, refine_feat.mlp_list3.0.2.bias, refine_feat.mlp_list3.1.0.weight, refine_feat.mlp_list3.1.0.bias, refine_feat.mlp_list3.1.2.weight, refine_feat.mlp_list3.1.2.bias, refine_feat.mlp_list3.2.0.weight, refine_feat.mlp_list3.2.0.bias, refine_feat.mlp_list3.2.2.weight, refine_feat.mlp_list3.2.2.bias, refine_feat.mlp_list3.3.0.weight, refine_feat.mlp_list3.3.0.bias, refine_feat.mlp_list3.3.2.weight, refine_feat.mlp_list3.3.2.bias, refine_feat.mlp_list3.4.0.weight, refine_feat.mlp_list3.4.0.bias, refine_feat.mlp_list3.4.2.weight, refine_feat.mlp_list3.4.2.bias, refine_feat.mlp_list3.5.0.weight, refine_feat.mlp_list3.5.0.bias, refine_feat.mlp_list3.5.2.weight, refine_feat.mlp_list3.5.2.bias, refine_feat.mlp_list3.6.0.weight, refine_feat.mlp_list3.6.0.bias, refine_feat.mlp_list3.6.2.weight, refine_feat.mlp_list3.6.2.bias, refine_feat.mlp_list3.7.0.weight, refine_feat.mlp_list3.7.0.bias, refine_feat.mlp_list3.7.2.weight, refine_feat.mlp_list3.7.2.bias, refine_feat.mlp_list3.8.0.weight, refine_feat.mlp_list3.8.0.bias, refine_feat.mlp_list3.8.2.weight, refine_feat.mlp_list3.8.2.bias, refine_feat.mlp_list3.9.0.weight, refine_feat.mlp_list3.9.0.bias, refine_feat.mlp_list3.9.2.weight, refine_feat.mlp_list3.9.2.bias, refine_feat.mlp_list3.10.0.weight, refine_feat.mlp_list3.10.0.bias, refine_feat.mlp_list3.10.2.weight, refine_feat.mlp_list3.10.2.bias, refine_feat.mlp_list3.11.0.weight, refine_feat.mlp_list3.11.0.bias, refine_feat.mlp_list3.11.2.weight, refine_feat.mlp_list3.11.2.bias, refine_feat.mlp_list3.12.0.weight, refine_feat.mlp_list3.12.0.bias, refine_feat.mlp_list3.12.2.weight, refine_feat.mlp_list3.12.2.bias, refine_feat.mlp_list3.13.0.weight, refine_feat.mlp_list3.13.0.bias, refine_feat.mlp_list3.13.2.weight, refine_feat.mlp_list3.13.2.bias, refine_feat.mlp_list3.14.0.weight, refine_feat.mlp_list3.14.0.bias, refine_feat.mlp_list3.14.2.weight, refine_feat.mlp_list3.14.2.bias, refine_feat.mlp_list3.15.0.weight, refine_feat.mlp_list3.15.0.bias, refine_feat.mlp_list3.15.2.weight, refine_feat.mlp_list3.15.2.bias, refine_feat.mlp_list3.16.0.weight, refine_feat.mlp_list3.16.0.bias, refine_feat.mlp_list3.16.2.weight, refine_feat.mlp_list3.16.2.bias, refine_feat.mlp_list3.17.0.weight, refine_feat.mlp_list3.17.0.bias, refine_feat.mlp_list3.17.2.weight, refine_feat.mlp_list3.17.2.bias, refine_feat.mlp_list3.18.0.weight, refine_feat.mlp_list3.18.0.bias, refine_feat.mlp_list3.18.2.weight, refine_feat.mlp_list3.18.2.bias, refine_feat.mlp_list3.19.0.weight, refine_feat.mlp_list3.19.0.bias, refine_feat.mlp_list3.19.2.weight, refine_feat.mlp_list3.19.2.bias, refine_feat.mlp_list3.20.0.weight, refine_feat.mlp_list3.20.0.bias, refine_feat.mlp_list3.20.2.weight, refine_feat.mlp_list3.20.2.bias, refine_feat.mlp_list3.21.0.weight, refine_feat.mlp_list3.21.0.bias, refine_feat.mlp_list3.21.2.weight, refine_feat.mlp_list3.21.2.bias, refine_feat.mlp_list3.22.0.weight, refine_feat.mlp_list3.22.0.bias, refine_feat.mlp_list3.22.2.weight, refine_feat.mlp_list3.22.2.bias, refine_feat.mlp_list3.23.0.weight, refine_feat.mlp_list3.23.0.bias, refine_feat.mlp_list3.23.2.weight, refine_feat.mlp_list3.23.2.bias, refine_feat.router.0.weight, refine_feat.router.0.bias, refine_feat.router.1.weight, refine_feat.router.1.bias, refine_feat.router.2.weight, refine_feat.router.2.bias, refine_feat.router.3.weight, refine_feat.router.3.bias, refine_feat.router.4.weight, refine_feat.router.4.bias, refine_feat.router.5.weight, refine_feat.router.5.bias, refine_feat.router.6.weight, refine_feat.router.6.bias, refine_feat.router.7.weight, refine_feat.router.7.bias, refine_feat.router.8.weight, refine_feat.router.8.bias, refine_feat.router.9.weight, refine_feat.router.9.bias, refine_feat.router.10.weight, refine_feat.router.10.bias, refine_feat.router.11.weight, refine_feat.router.11.bias, refine_feat.router.12.weight, refine_feat.router.12.bias, refine_feat.router.13.weight, refine_feat.router.13.bias, refine_feat.router.14.weight, refine_feat.router.14.bias, refine_feat.router.15.weight, refine_feat.router.15.bias, refine_feat.router.16.weight, refine_feat.router.16.bias, refine_feat.router.17.weight, refine_feat.router.17.bias, refine_feat.router.18.weight, refine_feat.router.18.bias, refine_feat.router.19.weight, refine_feat.router.19.bias, refine_feat.router.20.weight, refine_feat.router.20.bias, refine_feat.router.21.weight, refine_feat.router.21.bias, refine_feat.router.22.weight, refine_feat.router.22.bias, refine_feat.router.23.weight, refine_feat.router.23.bias

2025/03/29 11:47:16 - mmengine - INFO - load model from: checkpoints/dinov2_converted.pth
2025/03/29 11:47:16 - mmengine - INFO - Loads checkpoint by local backend from path: checkpoints/dinov2_converted.pth
Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.pos_embed - torch.Size([1, 1025, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.mask_token - torch.Size([1, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.patch_embed.proj.weight - torch.Size([1024, 3, 16, 16]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.patch_embed.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.0.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.1.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.2.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.3.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.4.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.5.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.6.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.7.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.8.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.9.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.10.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.11.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.12.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.13.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.14.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.15.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.16.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.17.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.18.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.19.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.20.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.21.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.22.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.blocks.23.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.norm.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.norm.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

backbone.refine_feat.scale - torch.Size([24]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.4.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.5.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.5.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.6.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.6.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.7.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.7.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.8.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.8.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.9.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.9.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.10.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.10.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.11.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.11.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.12.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.12.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.13.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.13.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.14.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.14.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.15.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.15.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.16.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.16.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.17.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.17.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.18.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.18.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.19.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.19.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.20.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.20.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.21.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.21.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.22.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.22.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.23.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.23.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.0.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.0.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.1.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.2.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.2.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.3.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.3.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.4.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.4.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.5.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.5.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.6.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.6.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.7.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.7.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.8.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.8.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.9.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.9.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.10.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.10.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.11.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.11.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.12.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.12.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.13.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.13.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.14.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.14.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.15.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.15.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.16.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.16.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.17.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.17.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.18.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.18.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.19.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.19.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.20.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.20.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.21.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.21.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.22.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.22.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.23.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.23.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.cls_embed.weight - torch.Size([8, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.cls_embed.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.cls_token - torch.Size([1, 1, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.pos_embed - torch.Size([1, 1025, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.mask_token - torch.Size([1, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.patch_embed.proj.weight - torch.Size([1024, 3, 16, 16]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.patch_embed.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.0.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.1.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.2.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.3.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.4.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.5.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.6.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.7.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.8.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.9.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.10.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.11.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.12.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.13.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.14.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.15.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.16.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.17.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.18.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.19.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.20.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.21.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.22.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.norm1.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.norm1.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.attn.qkv.weight - torch.Size([3072, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.attn.qkv.bias - torch.Size([3072]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.attn.proj.weight - torch.Size([1024, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.attn.proj.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.ls1.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.norm2.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.norm2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.mlp.fc1.weight - torch.Size([4096, 1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.mlp.fc1.bias - torch.Size([4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.mlp.fc2.weight - torch.Size([1024, 4096]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.mlp.fc2.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.blocks.23.ls2.gamma - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.norm.weight - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.norm.bias - torch.Size([1024]): 
PretrainedInit: load from checkpoints/dinov2_converted.pth 

ema_model.backbone.refine_feat.scale - torch.Size([24]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.4.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.5.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.5.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.6.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.6.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.7.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.7.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.8.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.8.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.9.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.9.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.10.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.10.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.11.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.11.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.12.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.12.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.13.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.13.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.14.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.14.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.15.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.15.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.16.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.16.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.17.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.17.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.18.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.18.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.19.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.19.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.20.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.20.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.21.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.21.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.22.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.22.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.23.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.23.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.0.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.0.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.1.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.2.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.2.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.3.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.3.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.4.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.4.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.5.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.5.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.6.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.6.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.7.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.7.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.8.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.8.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.9.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.9.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.10.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.10.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.11.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.11.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.12.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.12.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.13.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.13.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.14.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.14.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.15.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.15.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.16.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.16.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.17.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.17.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.18.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.18.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.19.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.19.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.20.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.20.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.21.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.21.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.22.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.22.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.23.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.23.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.cls_embed.weight - torch.Size([8, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.cls_embed.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  
2025/03/29 11:47:17 - mmengine - WARNING - init_weights of DACS_encoder_decoder has been called more than once.
Name of parameter - Initialization information

backbone.cls_token - torch.Size([1, 1, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.pos_embed - torch.Size([1, 1025, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.mask_token - torch.Size([1, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.patch_embed.proj.weight - torch.Size([1024, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.patch_embed.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.0.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.1.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.2.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.3.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.4.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.5.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.6.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.7.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.8.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.9.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.10.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.11.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.12.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.13.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.14.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.15.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.16.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.17.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.18.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.19.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.20.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.21.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.22.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.blocks.23.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.scale - torch.Size([24]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.4.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.5.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.5.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.6.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.6.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.7.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.7.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.8.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.8.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.9.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.9.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.10.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.10.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.11.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.11.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.12.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.12.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.13.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.13.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.14.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.14.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.15.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.15.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.16.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.16.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.17.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.17.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.18.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.18.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.19.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.19.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.20.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.20.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.21.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.21.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.22.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.22.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.23.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.layer_norm.23.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list1.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list2.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.mlp_list3.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.0.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.0.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.1.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.2.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.2.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.3.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.3.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.4.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.4.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.5.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.5.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.6.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.6.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.7.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.7.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.8.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.8.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.9.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.9.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.10.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.10.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.11.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.11.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.12.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.12.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.13.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.13.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.14.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.14.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.15.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.15.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.16.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.16.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.17.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.17.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.18.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.18.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.19.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.19.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.20.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.20.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.21.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.21.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.22.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.22.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.23.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

backbone.refine_feat.router.23.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.cls_embed.weight - torch.Size([8, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.cls_embed.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

decode_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.cls_token - torch.Size([1, 1, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.pos_embed - torch.Size([1, 1025, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.mask_token - torch.Size([1, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.patch_embed.proj.weight - torch.Size([1024, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.patch_embed.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.0.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.1.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.2.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.3.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.4.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.5.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.6.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.7.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.8.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.9.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.10.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.11.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.12.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.13.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.14.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.15.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.16.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.17.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.18.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.19.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.20.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.21.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.22.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.norm1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.norm1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.attn.qkv.weight - torch.Size([3072, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.attn.qkv.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.attn.proj.weight - torch.Size([1024, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.attn.proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.ls1.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.norm2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.norm2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.mlp.fc1.weight - torch.Size([4096, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.mlp.fc1.bias - torch.Size([4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.mlp.fc2.weight - torch.Size([1024, 4096]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.mlp.fc2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.blocks.23.ls2.gamma - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.norm.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.norm.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.scale - torch.Size([24]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.0.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.2.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.4.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.4.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.5.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.5.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.6.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.6.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.7.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.7.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.8.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.8.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.9.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.9.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.10.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.10.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.11.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.11.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.12.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.12.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.13.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.13.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.14.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.14.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.15.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.15.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.16.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.16.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.17.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.17.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.18.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.18.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.19.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.19.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.20.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.20.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.21.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.21.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.22.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.22.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.23.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.layer_norm.23.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list1.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list2.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.0.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.1.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.2.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.3.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.4.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.5.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.6.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.7.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.8.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.9.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.10.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.11.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.12.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.13.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.14.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.15.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.16.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.17.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.18.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.19.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.20.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.21.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.22.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.0.weight - torch.Size([32, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.2.weight - torch.Size([1024, 32]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.mlp_list3.23.2.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.0.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.0.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.1.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.2.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.2.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.3.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.3.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.4.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.4.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.5.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.5.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.6.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.6.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.7.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.7.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.8.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.8.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.9.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.9.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.10.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.10.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.11.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.11.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.12.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.12.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.13.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.13.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.14.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.14.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.15.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.15.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.16.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.16.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.17.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.17.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.18.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.18.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.19.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.19.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.20.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.20.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.21.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.21.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.22.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.22.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.23.weight - torch.Size([3, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.backbone.refine_feat.router.23.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.cls_embed.weight - torch.Size([8, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.cls_embed.bias - torch.Size([8]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  

ema_model.decode_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of DACS_encoder_decoder  
2025/03/29 11:47:17 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/03/29 11:47:17 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/03/29 11:47:17 - mmengine - INFO - Checkpoints will be saved to /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0.
2025/03/29 11:47:45 - mmengine - INFO - Iter(train) [   50/20000]  base_lr: 9.9779e-05 lr: 9.9779e-05  eta: 3:07:50  time: 0.5708  data_time: 0.0273  memory: 14001  loss: 80.5793  decode.loss_cls: 1.9390  decode.loss_mask: 3.2220  decode.loss_dice: 3.4758  decode.d0.loss_cls: 3.8346  decode.d0.loss_mask: 2.6226  decode.d0.loss_dice: 3.0802  decode.d1.loss_cls: 1.5942  decode.d1.loss_mask: 2.5701  decode.d1.loss_dice: 3.0471  decode.d2.loss_cls: 1.5634  decode.d2.loss_mask: 2.5934  decode.d2.loss_dice: 3.1351  decode.d3.loss_cls: 1.7307  decode.d3.loss_mask: 2.5838  decode.d3.loss_dice: 3.1514  decode.d4.loss_cls: 1.8420  decode.d4.loss_mask: 2.5551  decode.d4.loss_dice: 3.1441  decode.d5.loss_cls: 1.8389  decode.d5.loss_mask: 2.6777  decode.d5.loss_dice: 3.2640  decode.d6.loss_cls: 1.8846  decode.d6.loss_mask: 2.9194  decode.d6.loss_dice: 3.2444  decode.d7.loss_cls: 1.9131  decode.d7.loss_mask: 3.1709  decode.d7.loss_dice: 3.3650  decode.d8.loss_cls: 1.9558  decode.d8.loss_mask: 3.2048  decode.d8.loss_dice: 3.4561
2025/03/29 11:48:14 - mmengine - INFO - Iter(train) [  100/20000]  base_lr: 9.9554e-05 lr: 9.9554e-05  eta: 3:10:52  time: 0.5953  data_time: 0.0278  memory: 7057  loss: 70.5376  decode.loss_cls: 1.6732  decode.loss_mask: 2.2668  decode.loss_dice: 3.0612  decode.d0.loss_cls: 3.8040  decode.d0.loss_mask: 2.3754  decode.d0.loss_dice: 2.9236  decode.d1.loss_cls: 1.4135  decode.d1.loss_mask: 2.4817  decode.d1.loss_dice: 2.8417  decode.d2.loss_cls: 1.3674  decode.d2.loss_mask: 2.3800  decode.d2.loss_dice: 2.9035  decode.d3.loss_cls: 1.4293  decode.d3.loss_mask: 2.3312  decode.d3.loss_dice: 2.9232  decode.d4.loss_cls: 1.4087  decode.d4.loss_mask: 2.4365  decode.d4.loss_dice: 2.8661  decode.d5.loss_cls: 1.3873  decode.d5.loss_mask: 2.3335  decode.d5.loss_dice: 2.9623  decode.d6.loss_cls: 1.5663  decode.d6.loss_mask: 2.3141  decode.d6.loss_dice: 3.0214  decode.d7.loss_cls: 1.5343  decode.d7.loss_mask: 2.3843  decode.d7.loss_dice: 3.0251  decode.d8.loss_cls: 1.7824  decode.d8.loss_mask: 2.2443  decode.d8.loss_dice: 3.0952
2025/03/29 11:48:41 - mmengine - INFO - Iter(train) [  150/20000]  base_lr: 9.9329e-05 lr: 9.9329e-05  eta: 3:05:35  time: 0.5280  data_time: 0.0209  memory: 7081  loss: 69.3342  decode.loss_cls: 1.6486  decode.loss_mask: 2.3910  decode.loss_dice: 2.8005  decode.d0.loss_cls: 3.7236  decode.d0.loss_mask: 2.4368  decode.d0.loss_dice: 2.7431  decode.d1.loss_cls: 1.2778  decode.d1.loss_mask: 2.5172  decode.d1.loss_dice: 2.7045  decode.d2.loss_cls: 1.3091  decode.d2.loss_mask: 2.5724  decode.d2.loss_dice: 2.7495  decode.d3.loss_cls: 1.3734  decode.d3.loss_mask: 2.5732  decode.d3.loss_dice: 2.7016  decode.d4.loss_cls: 1.4658  decode.d4.loss_mask: 2.4987  decode.d4.loss_dice: 2.6801  decode.d5.loss_cls: 1.4132  decode.d5.loss_mask: 2.6521  decode.d5.loss_dice: 2.7514  decode.d6.loss_cls: 1.5234  decode.d6.loss_mask: 2.5353  decode.d6.loss_dice: 2.7291  decode.d7.loss_cls: 1.5664  decode.d7.loss_mask: 2.5064  decode.d7.loss_dice: 2.7318  decode.d8.loss_cls: 1.5648  decode.d8.loss_mask: 2.4588  decode.d8.loss_dice: 2.7344
2025/03/29 11:49:08 - mmengine - INFO - Iter(train) [  200/20000]  base_lr: 9.9104e-05 lr: 9.9104e-05  eta: 3:04:10  time: 0.5813  data_time: 0.0267  memory: 7072  loss: 66.9766  decode.loss_cls: 1.2974  decode.loss_mask: 2.6048  decode.loss_dice: 2.6045  decode.d0.loss_cls: 3.5851  decode.d0.loss_mask: 2.4754  decode.d0.loss_dice: 2.6003  decode.d1.loss_cls: 1.1025  decode.d1.loss_mask: 2.6234  decode.d1.loss_dice: 2.4678  decode.d2.loss_cls: 1.2287  decode.d2.loss_mask: 2.7054  decode.d2.loss_dice: 2.5068  decode.d3.loss_cls: 1.4036  decode.d3.loss_mask: 2.5807  decode.d3.loss_dice: 2.4960  decode.d4.loss_cls: 1.5013  decode.d4.loss_mask: 2.5818  decode.d4.loss_dice: 2.4764  decode.d5.loss_cls: 1.4532  decode.d5.loss_mask: 2.6006  decode.d5.loss_dice: 2.5033  decode.d6.loss_cls: 1.4745  decode.d6.loss_mask: 2.5193  decode.d6.loss_dice: 2.5477  decode.d7.loss_cls: 1.3895  decode.d7.loss_mask: 2.6244  decode.d7.loss_dice: 2.5743  decode.d8.loss_cls: 1.4212  decode.d8.loss_mask: 2.5001  decode.d8.loss_dice: 2.5266
2025/03/29 11:49:37 - mmengine - INFO - Iter(train) [  250/20000]  base_lr: 9.8879e-05 lr: 9.8879e-05  eta: 3:04:52  time: 0.5806  data_time: 0.0261  memory: 7061  loss: 64.0456  decode.loss_cls: 1.4327  decode.loss_mask: 2.3426  decode.loss_dice: 2.5915  decode.d0.loss_cls: 3.4910  decode.d0.loss_mask: 2.3511  decode.d0.loss_dice: 2.5833  decode.d1.loss_cls: 1.1212  decode.d1.loss_mask: 2.4038  decode.d1.loss_dice: 2.6062  decode.d2.loss_cls: 0.9920  decode.d2.loss_mask: 2.2679  decode.d2.loss_dice: 2.5400  decode.d3.loss_cls: 1.2830  decode.d3.loss_mask: 2.2607  decode.d3.loss_dice: 2.4973  decode.d4.loss_cls: 1.2932  decode.d4.loss_mask: 2.2621  decode.d4.loss_dice: 2.5009  decode.d5.loss_cls: 1.1951  decode.d5.loss_mask: 2.3148  decode.d5.loss_dice: 2.5916  decode.d6.loss_cls: 1.3996  decode.d6.loss_mask: 2.3194  decode.d6.loss_dice: 2.5907  decode.d7.loss_cls: 1.3894  decode.d7.loss_mask: 2.3584  decode.d7.loss_dice: 2.7077  decode.d8.loss_cls: 1.3742  decode.d8.loss_mask: 2.3960  decode.d8.loss_dice: 2.5881
2025/03/29 11:50:06 - mmengine - INFO - Iter(train) [  300/20000]  base_lr: 9.8653e-05 lr: 9.8653e-05  eta: 3:04:54  time: 0.5556  data_time: 0.0243  memory: 7059  loss: 60.8598  decode.loss_cls: 1.2573  decode.loss_mask: 2.3190  decode.loss_dice: 2.4053  decode.d0.loss_cls: 3.3352  decode.d0.loss_mask: 2.3087  decode.d0.loss_dice: 2.5436  decode.d1.loss_cls: 0.8693  decode.d1.loss_mask: 2.3829  decode.d1.loss_dice: 2.4835  decode.d2.loss_cls: 0.7517  decode.d2.loss_mask: 2.4281  decode.d2.loss_dice: 2.4006  decode.d3.loss_cls: 0.9402  decode.d3.loss_mask: 2.2852  decode.d3.loss_dice: 2.3897  decode.d4.loss_cls: 1.0692  decode.d4.loss_mask: 2.2963  decode.d4.loss_dice: 2.4190  decode.d5.loss_cls: 1.2358  decode.d5.loss_mask: 2.3453  decode.d5.loss_dice: 2.4128  decode.d6.loss_cls: 1.2228  decode.d6.loss_mask: 2.3820  decode.d6.loss_dice: 2.3652  decode.d7.loss_cls: 1.3009  decode.d7.loss_mask: 2.3572  decode.d7.loss_dice: 2.3947  decode.d8.loss_cls: 1.3022  decode.d8.loss_mask: 2.2920  decode.d8.loss_dice: 2.3642
2025/03/29 11:50:34 - mmengine - INFO - Iter(train) [  350/20000]  base_lr: 9.8428e-05 lr: 9.8428e-05  eta: 3:04:20  time: 0.5623  data_time: 0.0270  memory: 7048  loss: 63.1131  decode.loss_cls: 1.4886  decode.loss_mask: 2.1487  decode.loss_dice: 2.4974  decode.d0.loss_cls: 3.3069  decode.d0.loss_mask: 2.0993  decode.d0.loss_dice: 2.6428  decode.d1.loss_cls: 1.1790  decode.d1.loss_mask: 2.1371  decode.d1.loss_dice: 2.6055  decode.d2.loss_cls: 1.1138  decode.d2.loss_mask: 2.2563  decode.d2.loss_dice: 2.6848  decode.d3.loss_cls: 1.2126  decode.d3.loss_mask: 2.1840  decode.d3.loss_dice: 2.6032  decode.d4.loss_cls: 1.3081  decode.d4.loss_mask: 2.2850  decode.d4.loss_dice: 2.5400  decode.d5.loss_cls: 1.3385  decode.d5.loss_mask: 2.2348  decode.d5.loss_dice: 2.5687  decode.d6.loss_cls: 1.4460  decode.d6.loss_mask: 2.2246  decode.d6.loss_dice: 2.5259  decode.d7.loss_cls: 1.5013  decode.d7.loss_mask: 2.1889  decode.d7.loss_dice: 2.5638  decode.d8.loss_cls: 1.5762  decode.d8.loss_mask: 2.1409  decode.d8.loss_dice: 2.5105
2025/03/29 11:51:02 - mmengine - INFO - Iter(train) [  400/20000]  base_lr: 9.8203e-05 lr: 9.8203e-05  eta: 3:03:45  time: 0.5591  data_time: 0.0253  memory: 7070  loss: 59.7752  decode.loss_cls: 1.2855  decode.loss_mask: 2.1329  decode.loss_dice: 2.3975  decode.d0.loss_cls: 3.1264  decode.d0.loss_mask: 2.0952  decode.d0.loss_dice: 2.4692  decode.d1.loss_cls: 1.0313  decode.d1.loss_mask: 2.2205  decode.d1.loss_dice: 2.4944  decode.d2.loss_cls: 1.0048  decode.d2.loss_mask: 2.2790  decode.d2.loss_dice: 2.5140  decode.d3.loss_cls: 1.0925  decode.d3.loss_mask: 2.1932  decode.d3.loss_dice: 2.3996  decode.d4.loss_cls: 1.2859  decode.d4.loss_mask: 2.1328  decode.d4.loss_dice: 2.4282  decode.d5.loss_cls: 1.2997  decode.d5.loss_mask: 2.1276  decode.d5.loss_dice: 2.3832  decode.d6.loss_cls: 1.2414  decode.d6.loss_mask: 2.1585  decode.d6.loss_dice: 2.3790  decode.d7.loss_cls: 1.3460  decode.d7.loss_mask: 2.1115  decode.d7.loss_dice: 2.3442  decode.d8.loss_cls: 1.3281  decode.d8.loss_mask: 2.1177  decode.d8.loss_dice: 2.3552
2025/03/29 11:51:29 - mmengine - INFO - Iter(train) [  450/20000]  base_lr: 9.7977e-05 lr: 9.7977e-05  eta: 3:02:34  time: 0.5265  data_time: 0.0202  memory: 7057  loss: 58.9551  decode.loss_cls: 1.3568  decode.loss_mask: 2.0281  decode.loss_dice: 2.4417  decode.d0.loss_cls: 3.1307  decode.d0.loss_mask: 2.0389  decode.d0.loss_dice: 2.4829  decode.d1.loss_cls: 1.1244  decode.d1.loss_mask: 2.0707  decode.d1.loss_dice: 2.4683  decode.d2.loss_cls: 0.9955  decode.d2.loss_mask: 2.1057  decode.d2.loss_dice: 2.4155  decode.d3.loss_cls: 1.0447  decode.d3.loss_mask: 2.1239  decode.d3.loss_dice: 2.4525  decode.d4.loss_cls: 1.1164  decode.d4.loss_mask: 2.0781  decode.d4.loss_dice: 2.3888  decode.d5.loss_cls: 1.2321  decode.d5.loss_mask: 2.1298  decode.d5.loss_dice: 2.3662  decode.d6.loss_cls: 1.3223  decode.d6.loss_mask: 2.0396  decode.d6.loss_dice: 2.3595  decode.d7.loss_cls: 1.2455  decode.d7.loss_mask: 2.1408  decode.d7.loss_dice: 2.3955  decode.d8.loss_cls: 1.3959  decode.d8.loss_mask: 2.0720  decode.d8.loss_dice: 2.3923
2025/03/29 11:51:55 - mmengine - INFO - Iter(train) [  500/20000]  base_lr: 9.7752e-05 lr: 9.7752e-05  eta: 3:01:12  time: 0.5469  data_time: 0.0231  memory: 7073  loss: 50.5510  decode.loss_cls: 1.0375  decode.loss_mask: 1.8623  decode.loss_dice: 2.0493  decode.d0.loss_cls: 3.0588  decode.d0.loss_mask: 1.8808  decode.d0.loss_dice: 2.1331  decode.d1.loss_cls: 0.8219  decode.d1.loss_mask: 1.8704  decode.d1.loss_dice: 2.0556  decode.d2.loss_cls: 0.7465  decode.d2.loss_mask: 1.9406  decode.d2.loss_dice: 2.0283  decode.d3.loss_cls: 0.8467  decode.d3.loss_mask: 1.9504  decode.d3.loss_dice: 1.9993  decode.d4.loss_cls: 0.9838  decode.d4.loss_mask: 1.8574  decode.d4.loss_dice: 2.0040  decode.d5.loss_cls: 1.0193  decode.d5.loss_mask: 1.8843  decode.d5.loss_dice: 1.9803  decode.d6.loss_cls: 1.1011  decode.d6.loss_mask: 1.8007  decode.d6.loss_dice: 1.9631  decode.d7.loss_cls: 1.0713  decode.d7.loss_mask: 1.7993  decode.d7.loss_dice: 1.9816  decode.d8.loss_cls: 1.0133  decode.d8.loss_mask: 1.7859  decode.d8.loss_dice: 2.0241
2025/03/29 11:52:23 - mmengine - INFO - Iter(train) [  550/20000]  base_lr: 9.7526e-05 lr: 9.7526e-05  eta: 3:00:47  time: 0.5588  data_time: 0.0240  memory: 7063  loss: 58.8613  decode.loss_cls: 1.0764  decode.loss_mask: 2.2716  decode.loss_dice: 2.3926  decode.d0.loss_cls: 3.0155  decode.d0.loss_mask: 2.1250  decode.d0.loss_dice: 2.5607  decode.d1.loss_cls: 0.9649  decode.d1.loss_mask: 2.1900  decode.d1.loss_dice: 2.5060  decode.d2.loss_cls: 1.0751  decode.d2.loss_mask: 2.0874  decode.d2.loss_dice: 2.4564  decode.d3.loss_cls: 0.9945  decode.d3.loss_mask: 2.0826  decode.d3.loss_dice: 2.4240  decode.d4.loss_cls: 1.0592  decode.d4.loss_mask: 2.1447  decode.d4.loss_dice: 2.4216  decode.d5.loss_cls: 1.1305  decode.d5.loss_mask: 2.1233  decode.d5.loss_dice: 2.3787  decode.d6.loss_cls: 1.2302  decode.d6.loss_mask: 2.1793  decode.d6.loss_dice: 2.3878  decode.d7.loss_cls: 1.2045  decode.d7.loss_mask: 2.1564  decode.d7.loss_dice: 2.3635  decode.d8.loss_cls: 1.0975  decode.d8.loss_mask: 2.3191  decode.d8.loss_dice: 2.4420
2025/03/29 11:52:51 - mmengine - INFO - Iter(train) [  600/20000]  base_lr: 9.7300e-05 lr: 9.7300e-05  eta: 3:00:25  time: 0.5618  data_time: 0.0262  memory: 7046  loss: 49.2496  decode.loss_cls: 0.7979  decode.loss_mask: 2.0212  decode.loss_dice: 1.9730  decode.d0.loss_cls: 2.8369  decode.d0.loss_mask: 1.9830  decode.d0.loss_dice: 2.0416  decode.d1.loss_cls: 0.7447  decode.d1.loss_mask: 1.9757  decode.d1.loss_dice: 2.0177  decode.d2.loss_cls: 0.7315  decode.d2.loss_mask: 2.0328  decode.d2.loss_dice: 2.0113  decode.d3.loss_cls: 0.6589  decode.d3.loss_mask: 1.9901  decode.d3.loss_dice: 1.9693  decode.d4.loss_cls: 0.7106  decode.d4.loss_mask: 1.9765  decode.d4.loss_dice: 1.9845  decode.d5.loss_cls: 0.7471  decode.d5.loss_mask: 2.0082  decode.d5.loss_dice: 1.9981  decode.d6.loss_cls: 0.7721  decode.d6.loss_mask: 1.9501  decode.d6.loss_dice: 1.9393  decode.d7.loss_cls: 0.7211  decode.d7.loss_mask: 2.0631  decode.d7.loss_dice: 1.9063  decode.d8.loss_cls: 0.6801  decode.d8.loss_mask: 2.0112  decode.d8.loss_dice: 1.9954
2025/03/29 11:53:19 - mmengine - INFO - Iter(train) [  650/20000]  base_lr: 9.7075e-05 lr: 9.7075e-05  eta: 2:59:53  time: 0.5507  data_time: 0.0237  memory: 7074  loss: 59.7546  decode.loss_cls: 1.3812  decode.loss_mask: 2.2553  decode.loss_dice: 2.4259  decode.d0.loss_cls: 2.7955  decode.d0.loss_mask: 2.2631  decode.d0.loss_dice: 2.5582  decode.d1.loss_cls: 0.9564  decode.d1.loss_mask: 2.3748  decode.d1.loss_dice: 2.5493  decode.d2.loss_cls: 0.9468  decode.d2.loss_mask: 2.2129  decode.d2.loss_dice: 2.3928  decode.d3.loss_cls: 1.0136  decode.d3.loss_mask: 2.2360  decode.d3.loss_dice: 2.3991  decode.d4.loss_cls: 1.0739  decode.d4.loss_mask: 2.1819  decode.d4.loss_dice: 2.3792  decode.d5.loss_cls: 1.1029  decode.d5.loss_mask: 2.1905  decode.d5.loss_dice: 2.4186  decode.d6.loss_cls: 1.1385  decode.d6.loss_mask: 2.1820  decode.d6.loss_dice: 2.4161  decode.d7.loss_cls: 1.0953  decode.d7.loss_mask: 2.2897  decode.d7.loss_dice: 2.4587  decode.d8.loss_cls: 1.3989  decode.d8.loss_mask: 2.2469  decode.d8.loss_dice: 2.4205
2025/03/29 11:53:46 - mmengine - INFO - Iter(train) [  700/20000]  base_lr: 9.6849e-05 lr: 9.6849e-05  eta: 2:58:48  time: 0.5364  data_time: 0.0215  memory: 7072  loss: 53.5249  decode.loss_cls: 0.8565  decode.loss_mask: 2.2066  decode.loss_dice: 2.2225  decode.d0.loss_cls: 2.7454  decode.d0.loss_mask: 2.1333  decode.d0.loss_dice: 2.2469  decode.d1.loss_cls: 0.7757  decode.d1.loss_mask: 2.1217  decode.d1.loss_dice: 2.2076  decode.d2.loss_cls: 0.8423  decode.d2.loss_mask: 2.0652  decode.d2.loss_dice: 2.0680  decode.d3.loss_cls: 0.7696  decode.d3.loss_mask: 2.1486  decode.d3.loss_dice: 2.0917  decode.d4.loss_cls: 0.8033  decode.d4.loss_mask: 2.0397  decode.d4.loss_dice: 2.1438  decode.d5.loss_cls: 0.9975  decode.d5.loss_mask: 2.0702  decode.d5.loss_dice: 2.0827  decode.d6.loss_cls: 1.0480  decode.d6.loss_mask: 2.0340  decode.d6.loss_dice: 2.2040  decode.d7.loss_cls: 0.9781  decode.d7.loss_mask: 2.1228  decode.d7.loss_dice: 2.2216  decode.d8.loss_cls: 0.8839  decode.d8.loss_mask: 2.1545  decode.d8.loss_dice: 2.2394
2025/03/29 11:54:13 - mmengine - INFO - Iter(train) [  750/20000]  base_lr: 9.6623e-05 lr: 9.6623e-05  eta: 2:58:02  time: 0.5309  data_time: 0.0213  memory: 7049  loss: 50.6474  decode.loss_cls: 0.7242  decode.loss_mask: 2.0829  decode.loss_dice: 2.1741  decode.d0.loss_cls: 2.6233  decode.d0.loss_mask: 1.9863  decode.d0.loss_dice: 2.1367  decode.d1.loss_cls: 0.6855  decode.d1.loss_mask: 2.0372  decode.d1.loss_dice: 2.1659  decode.d2.loss_cls: 0.7166  decode.d2.loss_mask: 2.0156  decode.d2.loss_dice: 2.1243  decode.d3.loss_cls: 0.6262  decode.d3.loss_mask: 2.0661  decode.d3.loss_dice: 2.1468  decode.d4.loss_cls: 0.6818  decode.d4.loss_mask: 2.0997  decode.d4.loss_dice: 2.1797  decode.d5.loss_cls: 0.7723  decode.d5.loss_mask: 2.0103  decode.d5.loss_dice: 2.0639  decode.d6.loss_cls: 0.8155  decode.d6.loss_mask: 1.9879  decode.d6.loss_dice: 2.0885  decode.d7.loss_cls: 0.6442  decode.d7.loss_mask: 2.0279  decode.d7.loss_dice: 2.1200  decode.d8.loss_cls: 0.7173  decode.d8.loss_mask: 2.0342  decode.d8.loss_dice: 2.0925
2025/03/29 11:54:41 - mmengine - INFO - Iter(train) [  800/20000]  base_lr: 9.6397e-05 lr: 9.6397e-05  eta: 2:57:39  time: 0.5732  data_time: 0.0252  memory: 7058  loss: 55.2778  decode.loss_cls: 0.7111  decode.loss_mask: 2.3452  decode.loss_dice: 2.4619  decode.d0.loss_cls: 2.5516  decode.d0.loss_mask: 2.2357  decode.d0.loss_dice: 2.3818  decode.d1.loss_cls: 0.5709  decode.d1.loss_mask: 2.2993  decode.d1.loss_dice: 2.3968  decode.d2.loss_cls: 0.5892  decode.d2.loss_mask: 2.2337  decode.d2.loss_dice: 2.3656  decode.d3.loss_cls: 0.7040  decode.d3.loss_mask: 2.2334  decode.d3.loss_dice: 2.3450  decode.d4.loss_cls: 0.7759  decode.d4.loss_mask: 2.2345  decode.d4.loss_dice: 2.2715  decode.d5.loss_cls: 0.7951  decode.d5.loss_mask: 2.2532  decode.d5.loss_dice: 2.3356  decode.d6.loss_cls: 0.7749  decode.d6.loss_mask: 2.2377  decode.d6.loss_dice: 2.3846  decode.d7.loss_cls: 0.7343  decode.d7.loss_mask: 2.3103  decode.d7.loss_dice: 2.4201  decode.d8.loss_cls: 0.7208  decode.d8.loss_mask: 2.2333  decode.d8.loss_dice: 2.3708
2025/03/29 11:55:09 - mmengine - INFO - Iter(train) [  850/20000]  base_lr: 9.6171e-05 lr: 9.6171e-05  eta: 2:57:15  time: 0.5589  data_time: 0.0246  memory: 7059  loss: 52.3715  decode.loss_cls: 0.5965  decode.loss_mask: 2.2640  decode.loss_dice: 2.1330  decode.d0.loss_cls: 2.4212  decode.d0.loss_mask: 2.2461  decode.d0.loss_dice: 2.2795  decode.d1.loss_cls: 0.7889  decode.d1.loss_mask: 2.1668  decode.d1.loss_dice: 2.0908  decode.d2.loss_cls: 0.7196  decode.d2.loss_mask: 2.2204  decode.d2.loss_dice: 2.0891  decode.d3.loss_cls: 0.7623  decode.d3.loss_mask: 2.1898  decode.d3.loss_dice: 2.0431  decode.d4.loss_cls: 0.7741  decode.d4.loss_mask: 2.2514  decode.d4.loss_dice: 2.0974  decode.d5.loss_cls: 0.7123  decode.d5.loss_mask: 2.2945  decode.d5.loss_dice: 2.1103  decode.d6.loss_cls: 0.6295  decode.d6.loss_mask: 2.2833  decode.d6.loss_dice: 2.0795  decode.d7.loss_cls: 0.6400  decode.d7.loss_mask: 2.3249  decode.d7.loss_dice: 2.1632  decode.d8.loss_cls: 0.6344  decode.d8.loss_mask: 2.2648  decode.d8.loss_dice: 2.1009
2025/03/29 11:55:36 - mmengine - INFO - Iter(train) [  900/20000]  base_lr: 9.5945e-05 lr: 9.5945e-05  eta: 2:56:33  time: 0.5289  data_time: 0.0204  memory: 7072  loss: 51.7813  decode.loss_cls: 0.7138  decode.loss_mask: 2.0301  decode.loss_dice: 2.2951  decode.d0.loss_cls: 2.3425  decode.d0.loss_mask: 1.9262  decode.d0.loss_dice: 2.3514  decode.d1.loss_cls: 0.5576  decode.d1.loss_mask: 2.0222  decode.d1.loss_dice: 2.3537  decode.d2.loss_cls: 0.6740  decode.d2.loss_mask: 2.0315  decode.d2.loss_dice: 2.2892  decode.d3.loss_cls: 0.7207  decode.d3.loss_mask: 1.9895  decode.d3.loss_dice: 2.2932  decode.d4.loss_cls: 0.7220  decode.d4.loss_mask: 2.0245  decode.d4.loss_dice: 2.3333  decode.d5.loss_cls: 0.8131  decode.d5.loss_mask: 1.9473  decode.d5.loss_dice: 2.2915  decode.d6.loss_cls: 0.7210  decode.d6.loss_mask: 2.0443  decode.d6.loss_dice: 2.3191  decode.d7.loss_cls: 0.7027  decode.d7.loss_mask: 1.9589  decode.d7.loss_dice: 2.2921  decode.d8.loss_cls: 0.6204  decode.d8.loss_mask: 2.0689  decode.d8.loss_dice: 2.3317
2025/03/29 11:56:02 - mmengine - INFO - Iter(train) [  950/20000]  base_lr: 9.5719e-05 lr: 9.5719e-05  eta: 2:55:41  time: 0.5306  data_time: 0.0201  memory: 7065  loss: 53.9313  decode.loss_cls: 0.7274  decode.loss_mask: 2.3528  decode.loss_dice: 2.2521  decode.d0.loss_cls: 2.2420  decode.d0.loss_mask: 2.2575  decode.d0.loss_dice: 2.2471  decode.d1.loss_cls: 0.4591  decode.d1.loss_mask: 2.3910  decode.d1.loss_dice: 2.2740  decode.d2.loss_cls: 0.5308  decode.d2.loss_mask: 2.3822  decode.d2.loss_dice: 2.2866  decode.d3.loss_cls: 0.6060  decode.d3.loss_mask: 2.3666  decode.d3.loss_dice: 2.2300  decode.d4.loss_cls: 0.6745  decode.d4.loss_mask: 2.4016  decode.d4.loss_dice: 2.1925  decode.d5.loss_cls: 0.7751  decode.d5.loss_mask: 2.4009  decode.d5.loss_dice: 2.1900  decode.d6.loss_cls: 0.7348  decode.d6.loss_mask: 2.2817  decode.d6.loss_dice: 2.1630  decode.d7.loss_cls: 0.7114  decode.d7.loss_mask: 2.2956  decode.d7.loss_dice: 2.1626  decode.d8.loss_cls: 0.7223  decode.d8.loss_mask: 2.3718  decode.d8.loss_dice: 2.2482
2025/03/29 11:56:29 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 11:56:29 - mmengine - INFO - Iter(train) [ 1000/20000]  base_lr: 9.5493e-05 lr: 9.5493e-05  eta: 2:55:02  time: 0.5299  data_time: 0.0203  memory: 7051  loss: 51.9303  decode.loss_cls: 0.7110  decode.loss_mask: 2.0807  decode.loss_dice: 2.1968  decode.d0.loss_cls: 2.2185  decode.d0.loss_mask: 2.0678  decode.d0.loss_dice: 2.2376  decode.d1.loss_cls: 0.6875  decode.d1.loss_mask: 2.1093  decode.d1.loss_dice: 2.2172  decode.d2.loss_cls: 0.7507  decode.d2.loss_mask: 2.0292  decode.d2.loss_dice: 2.1312  decode.d3.loss_cls: 0.8746  decode.d3.loss_mask: 2.0112  decode.d3.loss_dice: 2.1129  decode.d4.loss_cls: 0.8784  decode.d4.loss_mask: 2.0737  decode.d4.loss_dice: 2.1114  decode.d5.loss_cls: 0.9677  decode.d5.loss_mask: 2.0670  decode.d5.loss_dice: 2.1576  decode.d6.loss_cls: 0.8758  decode.d6.loss_mask: 2.0523  decode.d6.loss_dice: 2.1275  decode.d7.loss_cls: 0.8100  decode.d7.loss_mask: 2.0953  decode.d7.loss_dice: 2.2072  decode.d8.loss_cls: 0.7888  decode.d8.loss_mask: 2.0799  decode.d8.loss_dice: 2.2013
2025/03/29 11:56:56 - mmengine - INFO - Iter(train) [ 1050/20000]  base_lr: 9.5267e-05 lr: 9.5267e-05  eta: 2:54:18  time: 0.5291  data_time: 0.0206  memory: 7054  loss: 47.4115  decode.loss_cls: 0.5319  decode.loss_mask: 2.0270  decode.loss_dice: 1.9750  decode.d0.loss_cls: 2.1440  decode.d0.loss_mask: 1.9053  decode.d0.loss_dice: 2.0791  decode.d1.loss_cls: 0.6226  decode.d1.loss_mask: 1.9510  decode.d1.loss_dice: 2.0620  decode.d2.loss_cls: 0.6001  decode.d2.loss_mask: 2.0112  decode.d2.loss_dice: 2.0003  decode.d3.loss_cls: 0.6729  decode.d3.loss_mask: 1.9751  decode.d3.loss_dice: 2.0098  decode.d4.loss_cls: 0.5633  decode.d4.loss_mask: 1.9358  decode.d4.loss_dice: 2.0025  decode.d5.loss_cls: 0.6625  decode.d5.loss_mask: 1.9829  decode.d5.loss_dice: 2.0021  decode.d6.loss_cls: 0.6198  decode.d6.loss_mask: 1.9596  decode.d6.loss_dice: 2.0198  decode.d7.loss_cls: 0.5138  decode.d7.loss_mask: 2.0648  decode.d7.loss_dice: 1.9754  decode.d8.loss_cls: 0.5360  decode.d8.loss_mask: 2.0142  decode.d8.loss_dice: 1.9919
2025/03/29 11:57:23 - mmengine - INFO - Iter(train) [ 1100/20000]  base_lr: 9.5040e-05 lr: 9.5040e-05  eta: 2:53:38  time: 0.5356  data_time: 0.0205  memory: 7056  loss: 50.4873  decode.loss_cls: 0.7144  decode.loss_mask: 2.0524  decode.loss_dice: 2.2034  decode.d0.loss_cls: 2.0868  decode.d0.loss_mask: 2.0023  decode.d0.loss_dice: 2.1551  decode.d1.loss_cls: 0.5775  decode.d1.loss_mask: 2.0591  decode.d1.loss_dice: 2.2298  decode.d2.loss_cls: 0.6868  decode.d2.loss_mask: 2.0575  decode.d2.loss_dice: 2.1807  decode.d3.loss_cls: 0.7235  decode.d3.loss_mask: 2.0531  decode.d3.loss_dice: 2.1481  decode.d4.loss_cls: 0.7271  decode.d4.loss_mask: 2.0454  decode.d4.loss_dice: 2.1862  decode.d5.loss_cls: 0.7371  decode.d5.loss_mask: 1.9967  decode.d5.loss_dice: 2.0724  decode.d6.loss_cls: 0.6601  decode.d6.loss_mask: 2.0537  decode.d6.loss_dice: 2.1464  decode.d7.loss_cls: 0.6524  decode.d7.loss_mask: 2.0445  decode.d7.loss_dice: 2.1894  decode.d8.loss_cls: 0.6462  decode.d8.loss_mask: 2.1303  decode.d8.loss_dice: 2.2692
2025/03/29 11:57:50 - mmengine - INFO - Iter(train) [ 1150/20000]  base_lr: 9.4814e-05 lr: 9.4814e-05  eta: 2:52:54  time: 0.5302  data_time: 0.0201  memory: 7074  loss: 48.6843  decode.loss_cls: 0.8760  decode.loss_mask: 1.9276  decode.loss_dice: 1.8619  decode.d0.loss_cls: 2.1146  decode.d0.loss_mask: 2.0096  decode.d0.loss_dice: 2.0236  decode.d1.loss_cls: 0.7070  decode.d1.loss_mask: 2.0780  decode.d1.loss_dice: 1.9892  decode.d2.loss_cls: 0.7025  decode.d2.loss_mask: 2.0246  decode.d2.loss_dice: 1.8892  decode.d3.loss_cls: 0.8010  decode.d3.loss_mask: 1.9771  decode.d3.loss_dice: 1.8572  decode.d4.loss_cls: 0.8339  decode.d4.loss_mask: 1.9970  decode.d4.loss_dice: 1.8847  decode.d5.loss_cls: 0.8571  decode.d5.loss_mask: 2.0157  decode.d5.loss_dice: 1.8744  decode.d6.loss_cls: 0.9590  decode.d6.loss_mask: 1.9933  decode.d6.loss_dice: 1.8390  decode.d7.loss_cls: 0.9605  decode.d7.loss_mask: 1.9976  decode.d7.loss_dice: 1.8601  decode.d8.loss_cls: 0.8604  decode.d8.loss_mask: 2.0277  decode.d8.loss_dice: 1.8848
2025/03/29 11:58:17 - mmengine - INFO - Iter(train) [ 1200/20000]  base_lr: 9.4588e-05 lr: 9.4588e-05  eta: 2:52:26  time: 0.5487  data_time: 0.0236  memory: 7062  loss: 45.9919  decode.loss_cls: 0.5725  decode.loss_mask: 2.0617  decode.loss_dice: 1.8760  decode.d0.loss_cls: 1.9395  decode.d0.loss_mask: 2.0157  decode.d0.loss_dice: 1.8703  decode.d1.loss_cls: 0.4648  decode.d1.loss_mask: 2.0928  decode.d1.loss_dice: 1.9171  decode.d2.loss_cls: 0.5809  decode.d2.loss_mask: 1.9667  decode.d2.loss_dice: 1.8284  decode.d3.loss_cls: 0.5816  decode.d3.loss_mask: 1.9857  decode.d3.loss_dice: 1.8367  decode.d4.loss_cls: 0.5829  decode.d4.loss_mask: 1.9572  decode.d4.loss_dice: 1.8604  decode.d5.loss_cls: 0.5972  decode.d5.loss_mask: 1.9441  decode.d5.loss_dice: 1.8790  decode.d6.loss_cls: 0.5884  decode.d6.loss_mask: 2.0252  decode.d6.loss_dice: 1.8817  decode.d7.loss_cls: 0.5934  decode.d7.loss_mask: 1.9878  decode.d7.loss_dice: 1.9683  decode.d8.loss_cls: 0.5940  decode.d8.loss_mask: 2.0469  decode.d8.loss_dice: 1.8952
2025/03/29 11:58:44 - mmengine - INFO - Iter(train) [ 1250/20000]  base_lr: 9.4361e-05 lr: 9.4361e-05  eta: 2:51:44  time: 0.5290  data_time: 0.0201  memory: 7048  loss: 55.7770  decode.loss_cls: 0.6829  decode.loss_mask: 2.5226  decode.loss_dice: 2.1686  decode.d0.loss_cls: 1.9586  decode.d0.loss_mask: 2.4548  decode.d0.loss_dice: 2.2254  decode.d1.loss_cls: 0.5856  decode.d1.loss_mask: 2.5672  decode.d1.loss_dice: 2.2400  decode.d2.loss_cls: 0.6706  decode.d2.loss_mask: 2.6022  decode.d2.loss_dice: 2.2220  decode.d3.loss_cls: 0.7303  decode.d3.loss_mask: 2.6088  decode.d3.loss_dice: 2.2783  decode.d4.loss_cls: 0.7106  decode.d4.loss_mask: 2.5357  decode.d4.loss_dice: 2.2349  decode.d5.loss_cls: 0.7406  decode.d5.loss_mask: 2.6025  decode.d5.loss_dice: 2.1602  decode.d6.loss_cls: 0.7023  decode.d6.loss_mask: 2.5214  decode.d6.loss_dice: 2.1946  decode.d7.loss_cls: 0.6629  decode.d7.loss_mask: 2.5521  decode.d7.loss_dice: 2.1849  decode.d8.loss_cls: 0.6648  decode.d8.loss_mask: 2.6048  decode.d8.loss_dice: 2.1869
2025/03/29 11:59:10 - mmengine - INFO - Iter(train) [ 1300/20000]  base_lr: 9.4135e-05 lr: 9.4135e-05  eta: 2:51:05  time: 0.5510  data_time: 0.0231  memory: 7072  loss: 50.4036  decode.loss_cls: 0.6648  decode.loss_mask: 2.1063  decode.loss_dice: 2.2205  decode.d0.loss_cls: 1.7591  decode.d0.loss_mask: 2.1327  decode.d0.loss_dice: 2.2455  decode.d1.loss_cls: 0.4245  decode.d1.loss_mask: 2.1290  decode.d1.loss_dice: 2.2014  decode.d2.loss_cls: 0.4655  decode.d2.loss_mask: 2.1137  decode.d2.loss_dice: 2.1893  decode.d3.loss_cls: 0.6038  decode.d3.loss_mask: 2.1285  decode.d3.loss_dice: 2.1668  decode.d4.loss_cls: 0.7436  decode.d4.loss_mask: 2.1291  decode.d4.loss_dice: 2.2059  decode.d5.loss_cls: 0.6518  decode.d5.loss_mask: 2.1629  decode.d5.loss_dice: 2.1987  decode.d6.loss_cls: 0.7097  decode.d6.loss_mask: 2.0458  decode.d6.loss_dice: 2.1909  decode.d7.loss_cls: 0.6520  decode.d7.loss_mask: 2.0879  decode.d7.loss_dice: 2.1532  decode.d8.loss_cls: 0.6079  decode.d8.loss_mask: 2.0908  decode.d8.loss_dice: 2.2220
2025/03/29 11:59:39 - mmengine - INFO - Iter(train) [ 1350/20000]  base_lr: 9.3908e-05 lr: 9.3908e-05  eta: 2:50:57  time: 0.6042  data_time: 0.0301  memory: 7048  loss: 52.7167  decode.loss_cls: 0.8161  decode.loss_mask: 2.2794  decode.loss_dice: 2.1201  decode.d0.loss_cls: 1.7960  decode.d0.loss_mask: 2.2825  decode.d0.loss_dice: 2.1896  decode.d1.loss_cls: 0.6076  decode.d1.loss_mask: 2.2745  decode.d1.loss_dice: 2.1477  decode.d2.loss_cls: 0.7183  decode.d2.loss_mask: 2.2150  decode.d2.loss_dice: 2.0867  decode.d3.loss_cls: 0.6596  decode.d3.loss_mask: 2.2840  decode.d3.loss_dice: 2.1435  decode.d4.loss_cls: 0.7724  decode.d4.loss_mask: 2.3194  decode.d4.loss_dice: 2.0651  decode.d5.loss_cls: 0.8008  decode.d5.loss_mask: 2.3633  decode.d5.loss_dice: 2.0690  decode.d6.loss_cls: 0.7317  decode.d6.loss_mask: 2.4663  decode.d6.loss_dice: 2.0467  decode.d7.loss_cls: 0.6727  decode.d7.loss_mask: 2.4046  decode.d7.loss_dice: 2.1620  decode.d8.loss_cls: 0.7051  decode.d8.loss_mask: 2.3523  decode.d8.loss_dice: 2.1645
2025/03/29 12:00:08 - mmengine - INFO - Iter(train) [ 1400/20000]  base_lr: 9.3682e-05 lr: 9.3682e-05  eta: 2:50:43  time: 0.5589  data_time: 0.0241  memory: 7059  loss: 42.9213  decode.loss_cls: 0.5264  decode.loss_mask: 1.8324  decode.loss_dice: 1.8864  decode.d0.loss_cls: 1.6655  decode.d0.loss_mask: 1.7824  decode.d0.loss_dice: 1.9047  decode.d1.loss_cls: 0.4733  decode.d1.loss_mask: 1.8134  decode.d1.loss_dice: 1.9030  decode.d2.loss_cls: 0.4551  decode.d2.loss_mask: 1.7827  decode.d2.loss_dice: 1.8815  decode.d3.loss_cls: 0.5391  decode.d3.loss_mask: 1.7535  decode.d3.loss_dice: 1.8250  decode.d4.loss_cls: 0.5579  decode.d4.loss_mask: 1.7498  decode.d4.loss_dice: 1.7862  decode.d5.loss_cls: 0.5268  decode.d5.loss_mask: 1.7854  decode.d5.loss_dice: 1.8410  decode.d6.loss_cls: 0.5327  decode.d6.loss_mask: 1.7849  decode.d6.loss_dice: 1.8875  decode.d7.loss_cls: 0.4872  decode.d7.loss_mask: 1.8335  decode.d7.loss_dice: 1.9011  decode.d8.loss_cls: 0.4854  decode.d8.loss_mask: 1.8757  decode.d8.loss_dice: 1.8621
2025/03/29 12:00:35 - mmengine - INFO - Iter(train) [ 1450/20000]  base_lr: 9.3455e-05 lr: 9.3455e-05  eta: 2:50:18  time: 0.5390  data_time: 0.0219  memory: 7068  loss: 47.8494  decode.loss_cls: 0.7115  decode.loss_mask: 2.1706  decode.loss_dice: 2.0199  decode.d0.loss_cls: 1.5954  decode.d0.loss_mask: 2.0990  decode.d0.loss_dice: 2.0201  decode.d1.loss_cls: 0.4383  decode.d1.loss_mask: 2.1644  decode.d1.loss_dice: 2.0387  decode.d2.loss_cls: 0.4493  decode.d2.loss_mask: 2.1011  decode.d2.loss_dice: 1.9816  decode.d3.loss_cls: 0.4693  decode.d3.loss_mask: 2.1794  decode.d3.loss_dice: 2.0485  decode.d4.loss_cls: 0.5268  decode.d4.loss_mask: 2.1122  decode.d4.loss_dice: 1.9839  decode.d5.loss_cls: 0.5041  decode.d5.loss_mask: 2.1304  decode.d5.loss_dice: 2.0399  decode.d6.loss_cls: 0.5347  decode.d6.loss_mask: 2.1693  decode.d6.loss_dice: 1.9704  decode.d7.loss_cls: 0.5506  decode.d7.loss_mask: 2.1222  decode.d7.loss_dice: 2.0127  decode.d8.loss_cls: 0.6549  decode.d8.loss_mask: 2.0507  decode.d8.loss_dice: 1.9996
2025/03/29 12:01:03 - mmengine - INFO - Iter(train) [ 1500/20000]  base_lr: 9.3228e-05 lr: 9.3228e-05  eta: 2:49:50  time: 0.5501  data_time: 0.0238  memory: 7061  loss: 49.7002  decode.loss_cls: 0.6646  decode.loss_mask: 1.9405  decode.loss_dice: 2.2585  decode.d0.loss_cls: 1.6114  decode.d0.loss_mask: 1.9157  decode.d0.loss_dice: 2.3330  decode.d1.loss_cls: 0.6558  decode.d1.loss_mask: 1.8623  decode.d1.loss_dice: 2.2529  decode.d2.loss_cls: 0.5966  decode.d2.loss_mask: 1.9019  decode.d2.loss_dice: 2.2429  decode.d3.loss_cls: 0.6713  decode.d3.loss_mask: 1.9669  decode.d3.loss_dice: 2.2523  decode.d4.loss_cls: 0.7557  decode.d4.loss_mask: 1.8580  decode.d4.loss_dice: 2.2413  decode.d5.loss_cls: 0.6981  decode.d5.loss_mask: 1.9232  decode.d5.loss_dice: 2.2370  decode.d6.loss_cls: 0.7040  decode.d6.loss_mask: 1.9277  decode.d6.loss_dice: 2.2794  decode.d7.loss_cls: 0.7529  decode.d7.loss_mask: 1.9121  decode.d7.loss_dice: 2.2790  decode.d8.loss_cls: 0.7436  decode.d8.loss_mask: 1.9771  decode.d8.loss_dice: 2.2845
2025/03/29 12:01:30 - mmengine - INFO - Iter(train) [ 1550/20000]  base_lr: 9.3001e-05 lr: 9.3001e-05  eta: 2:49:22  time: 0.5450  data_time: 0.0224  memory: 7073  loss: 52.7603  decode.loss_cls: 0.6483  decode.loss_mask: 2.2154  decode.loss_dice: 2.4069  decode.d0.loss_cls: 1.5578  decode.d0.loss_mask: 2.1883  decode.d0.loss_dice: 2.4011  decode.d1.loss_cls: 0.5482  decode.d1.loss_mask: 2.1879  decode.d1.loss_dice: 2.4172  decode.d2.loss_cls: 0.6520  decode.d2.loss_mask: 2.1130  decode.d2.loss_dice: 2.3614  decode.d3.loss_cls: 0.6656  decode.d3.loss_mask: 2.1873  decode.d3.loss_dice: 2.3408  decode.d4.loss_cls: 0.6949  decode.d4.loss_mask: 2.1687  decode.d4.loss_dice: 2.3163  decode.d5.loss_cls: 0.6436  decode.d5.loss_mask: 2.1880  decode.d5.loss_dice: 2.3644  decode.d6.loss_cls: 0.5154  decode.d6.loss_mask: 2.2019  decode.d6.loss_dice: 2.4554  decode.d7.loss_cls: 0.4396  decode.d7.loss_mask: 2.2707  decode.d7.loss_dice: 2.4648  decode.d8.loss_cls: 0.5981  decode.d8.loss_mask: 2.1818  decode.d8.loss_dice: 2.3652
2025/03/29 12:01:57 - mmengine - INFO - Iter(train) [ 1600/20000]  base_lr: 9.2774e-05 lr: 9.2774e-05  eta: 2:48:40  time: 0.5252  data_time: 0.0204  memory: 7073  loss: 51.4405  decode.loss_cls: 0.5791  decode.loss_mask: 2.1003  decode.loss_dice: 2.3202  decode.d0.loss_cls: 1.5450  decode.d0.loss_mask: 2.0514  decode.d0.loss_dice: 2.2832  decode.d1.loss_cls: 0.6565  decode.d1.loss_mask: 2.1208  decode.d1.loss_dice: 2.3118  decode.d2.loss_cls: 0.6176  decode.d2.loss_mask: 2.1841  decode.d2.loss_dice: 2.3214  decode.d3.loss_cls: 0.7196  decode.d3.loss_mask: 2.0781  decode.d3.loss_dice: 2.3187  decode.d4.loss_cls: 0.6611  decode.d4.loss_mask: 2.1284  decode.d4.loss_dice: 2.2719  decode.d5.loss_cls: 0.6927  decode.d5.loss_mask: 2.0670  decode.d5.loss_dice: 2.2822  decode.d6.loss_cls: 0.6465  decode.d6.loss_mask: 2.0757  decode.d6.loss_dice: 2.3367  decode.d7.loss_cls: 0.5164  decode.d7.loss_mask: 2.1640  decode.d7.loss_dice: 2.3408  decode.d8.loss_cls: 0.6749  decode.d8.loss_mask: 2.0132  decode.d8.loss_dice: 2.3613
2025/03/29 12:02:23 - mmengine - INFO - Iter(train) [ 1650/20000]  base_lr: 9.2548e-05 lr: 9.2548e-05  eta: 2:48:02  time: 0.5346  data_time: 0.0208  memory: 7054  loss: 52.1796  decode.loss_cls: 0.6872  decode.loss_mask: 2.2890  decode.loss_dice: 2.1928  decode.d0.loss_cls: 1.5595  decode.d0.loss_mask: 2.1474  decode.d0.loss_dice: 2.2255  decode.d1.loss_cls: 0.8046  decode.d1.loss_mask: 2.1411  decode.d1.loss_dice: 2.1005  decode.d2.loss_cls: 0.8762  decode.d2.loss_mask: 2.1446  decode.d2.loss_dice: 2.1373  decode.d3.loss_cls: 0.8377  decode.d3.loss_mask: 2.2232  decode.d3.loss_dice: 2.1591  decode.d4.loss_cls: 0.8605  decode.d4.loss_mask: 2.1911  decode.d4.loss_dice: 2.1303  decode.d5.loss_cls: 0.8972  decode.d5.loss_mask: 2.1298  decode.d5.loss_dice: 2.0944  decode.d6.loss_cls: 0.8005  decode.d6.loss_mask: 2.2278  decode.d6.loss_dice: 2.1803  decode.d7.loss_cls: 0.7337  decode.d7.loss_mask: 2.2005  decode.d7.loss_dice: 2.1147  decode.d8.loss_cls: 0.7278  decode.d8.loss_mask: 2.2172  decode.d8.loss_dice: 2.1485
2025/03/29 12:02:50 - mmengine - INFO - Iter(train) [ 1700/20000]  base_lr: 9.2321e-05 lr: 9.2321e-05  eta: 2:47:30  time: 0.5294  data_time: 0.0208  memory: 7054  loss: 47.7124  decode.loss_cls: 0.5117  decode.loss_mask: 2.1710  decode.loss_dice: 1.9579  decode.d0.loss_cls: 1.4308  decode.d0.loss_mask: 2.0669  decode.d0.loss_dice: 2.0478  decode.d1.loss_cls: 0.5634  decode.d1.loss_mask: 2.1120  decode.d1.loss_dice: 2.0474  decode.d2.loss_cls: 0.5111  decode.d2.loss_mask: 2.1110  decode.d2.loss_dice: 2.0190  decode.d3.loss_cls: 0.5662  decode.d3.loss_mask: 2.1664  decode.d3.loss_dice: 2.0165  decode.d4.loss_cls: 0.4876  decode.d4.loss_mask: 2.2080  decode.d4.loss_dice: 1.9684  decode.d5.loss_cls: 0.5189  decode.d5.loss_mask: 2.2524  decode.d5.loss_dice: 1.9436  decode.d6.loss_cls: 0.6273  decode.d6.loss_mask: 2.1746  decode.d6.loss_dice: 1.9551  decode.d7.loss_cls: 0.5220  decode.d7.loss_mask: 2.1765  decode.d7.loss_dice: 1.9203  decode.d8.loss_cls: 0.4830  decode.d8.loss_mask: 2.2233  decode.d8.loss_dice: 1.9521
2025/03/29 12:03:18 - mmengine - INFO - Iter(train) [ 1750/20000]  base_lr: 9.2094e-05 lr: 9.2094e-05  eta: 2:47:00  time: 0.5312  data_time: 0.0218  memory: 7053  loss: 48.4798  decode.loss_cls: 0.7209  decode.loss_mask: 2.0548  decode.loss_dice: 1.9295  decode.d0.loss_cls: 1.3621  decode.d0.loss_mask: 2.1260  decode.d0.loss_dice: 2.0391  decode.d1.loss_cls: 0.5775  decode.d1.loss_mask: 2.1508  decode.d1.loss_dice: 2.0180  decode.d2.loss_cls: 0.5512  decode.d2.loss_mask: 2.2121  decode.d2.loss_dice: 2.0301  decode.d3.loss_cls: 0.5998  decode.d3.loss_mask: 2.1715  decode.d3.loss_dice: 2.0552  decode.d4.loss_cls: 0.6378  decode.d4.loss_mask: 2.1742  decode.d4.loss_dice: 1.9940  decode.d5.loss_cls: 0.7208  decode.d5.loss_mask: 2.0785  decode.d5.loss_dice: 1.9775  decode.d6.loss_cls: 0.7539  decode.d6.loss_mask: 2.0380  decode.d6.loss_dice: 1.9541  decode.d7.loss_cls: 0.6644  decode.d7.loss_mask: 2.1128  decode.d7.loss_dice: 1.9785  decode.d8.loss_cls: 0.7524  decode.d8.loss_mask: 2.0738  decode.d8.loss_dice: 1.9704
2025/03/29 12:03:45 - mmengine - INFO - Iter(train) [ 1800/20000]  base_lr: 9.1866e-05 lr: 9.1866e-05  eta: 2:46:28  time: 0.5555  data_time: 0.0259  memory: 7060  loss: 47.0222  decode.loss_cls: 0.5526  decode.loss_mask: 2.0737  decode.loss_dice: 2.0800  decode.d0.loss_cls: 1.3180  decode.d0.loss_mask: 1.9443  decode.d0.loss_dice: 2.1203  decode.d1.loss_cls: 0.5495  decode.d1.loss_mask: 1.9674  decode.d1.loss_dice: 2.0259  decode.d2.loss_cls: 0.5369  decode.d2.loss_mask: 1.9722  decode.d2.loss_dice: 2.0310  decode.d3.loss_cls: 0.5735  decode.d3.loss_mask: 1.9988  decode.d3.loss_dice: 1.9919  decode.d4.loss_cls: 0.5187  decode.d4.loss_mask: 2.0763  decode.d4.loss_dice: 2.0237  decode.d5.loss_cls: 0.5129  decode.d5.loss_mask: 2.1322  decode.d5.loss_dice: 2.0039  decode.d6.loss_cls: 0.4909  decode.d6.loss_mask: 2.1313  decode.d6.loss_dice: 2.0361  decode.d7.loss_cls: 0.5382  decode.d7.loss_mask: 2.0519  decode.d7.loss_dice: 2.0307  decode.d8.loss_cls: 0.5345  decode.d8.loss_mask: 2.1050  decode.d8.loss_dice: 2.1000
2025/03/29 12:04:12 - mmengine - INFO - Iter(train) [ 1850/20000]  base_lr: 9.1639e-05 lr: 9.1639e-05  eta: 2:45:57  time: 0.5267  data_time: 0.0206  memory: 7076  loss: 47.7059  decode.loss_cls: 0.4802  decode.loss_mask: 2.1768  decode.loss_dice: 2.0038  decode.d0.loss_cls: 1.2787  decode.d0.loss_mask: 2.0449  decode.d0.loss_dice: 2.0915  decode.d1.loss_cls: 0.5260  decode.d1.loss_mask: 2.1383  decode.d1.loss_dice: 2.0060  decode.d2.loss_cls: 0.5179  decode.d2.loss_mask: 2.1745  decode.d2.loss_dice: 2.0150  decode.d3.loss_cls: 0.5239  decode.d3.loss_mask: 2.1612  decode.d3.loss_dice: 1.9704  decode.d4.loss_cls: 0.5862  decode.d4.loss_mask: 2.1526  decode.d4.loss_dice: 1.9873  decode.d5.loss_cls: 0.5866  decode.d5.loss_mask: 2.1780  decode.d5.loss_dice: 2.0317  decode.d6.loss_cls: 0.5099  decode.d6.loss_mask: 2.1833  decode.d6.loss_dice: 2.0211  decode.d7.loss_cls: 0.4680  decode.d7.loss_mask: 2.1875  decode.d7.loss_dice: 2.0284  decode.d8.loss_cls: 0.5147  decode.d8.loss_mask: 2.1600  decode.d8.loss_dice: 2.0019
2025/03/29 12:04:38 - mmengine - INFO - Iter(train) [ 1900/20000]  base_lr: 9.1412e-05 lr: 9.1412e-05  eta: 2:45:23  time: 0.5513  data_time: 0.0240  memory: 7069  loss: 50.4432  decode.loss_cls: 0.7548  decode.loss_mask: 2.1375  decode.loss_dice: 2.1295  decode.d0.loss_cls: 1.3717  decode.d0.loss_mask: 2.0414  decode.d0.loss_dice: 2.1146  decode.d1.loss_cls: 0.7165  decode.d1.loss_mask: 2.0408  decode.d1.loss_dice: 2.1415  decode.d2.loss_cls: 0.7984  decode.d2.loss_mask: 2.0671  decode.d2.loss_dice: 2.1495  decode.d3.loss_cls: 0.7374  decode.d3.loss_mask: 2.0880  decode.d3.loss_dice: 2.1010  decode.d4.loss_cls: 0.7594  decode.d4.loss_mask: 2.0913  decode.d4.loss_dice: 2.1160  decode.d5.loss_cls: 0.8357  decode.d5.loss_mask: 2.0826  decode.d5.loss_dice: 2.0980  decode.d6.loss_cls: 0.7027  decode.d6.loss_mask: 2.1637  decode.d6.loss_dice: 2.1752  decode.d7.loss_cls: 0.7465  decode.d7.loss_mask: 2.1158  decode.d7.loss_dice: 2.1622  decode.d8.loss_cls: 0.7893  decode.d8.loss_mask: 2.0745  decode.d8.loss_dice: 2.1408
2025/03/29 12:05:05 - mmengine - INFO - Iter(train) [ 1950/20000]  base_lr: 9.1185e-05 lr: 9.1185e-05  eta: 2:44:52  time: 0.5288  data_time: 0.0205  memory: 7048  loss: 52.1276  decode.loss_cls: 0.7586  decode.loss_mask: 2.3681  decode.loss_dice: 2.1319  decode.d0.loss_cls: 1.2817  decode.d0.loss_mask: 2.2688  decode.d0.loss_dice: 2.0870  decode.d1.loss_cls: 0.6576  decode.d1.loss_mask: 2.2210  decode.d1.loss_dice: 2.0728  decode.d2.loss_cls: 0.7670  decode.d2.loss_mask: 2.2247  decode.d2.loss_dice: 2.1285  decode.d3.loss_cls: 0.8873  decode.d3.loss_mask: 2.2084  decode.d3.loss_dice: 2.1633  decode.d4.loss_cls: 0.7776  decode.d4.loss_mask: 2.3127  decode.d4.loss_dice: 2.0989  decode.d5.loss_cls: 0.7965  decode.d5.loss_mask: 2.2524  decode.d5.loss_dice: 2.0661  decode.d6.loss_cls: 0.8115  decode.d6.loss_mask: 2.3082  decode.d6.loss_dice: 2.1036  decode.d7.loss_cls: 0.8746  decode.d7.loss_mask: 2.2357  decode.d7.loss_dice: 2.0515  decode.d8.loss_cls: 0.7255  decode.d8.loss_mask: 2.3569  decode.d8.loss_dice: 2.1294
2025/03/29 12:05:32 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 12:05:32 - mmengine - INFO - Iter(train) [ 2000/20000]  base_lr: 9.0957e-05 lr: 9.0957e-05  eta: 2:44:16  time: 0.5317  data_time: 0.0207  memory: 7054  loss: 50.8736  decode.loss_cls: 0.6244  decode.loss_mask: 2.2112  decode.loss_dice: 2.1038  decode.d0.loss_cls: 1.2161  decode.d0.loss_mask: 2.2183  decode.d0.loss_dice: 2.1007  decode.d1.loss_cls: 0.5782  decode.d1.loss_mask: 2.2370  decode.d1.loss_dice: 2.0657  decode.d2.loss_cls: 0.6827  decode.d2.loss_mask: 2.3090  decode.d2.loss_dice: 2.2113  decode.d3.loss_cls: 0.6441  decode.d3.loss_mask: 2.2439  decode.d3.loss_dice: 2.0826  decode.d4.loss_cls: 0.7018  decode.d4.loss_mask: 2.2361  decode.d4.loss_dice: 2.1445  decode.d5.loss_cls: 0.7168  decode.d5.loss_mask: 2.2631  decode.d5.loss_dice: 2.1096  decode.d6.loss_cls: 0.6381  decode.d6.loss_mask: 2.2944  decode.d6.loss_dice: 2.1171  decode.d7.loss_cls: 0.6069  decode.d7.loss_mask: 2.3131  decode.d7.loss_dice: 2.1529  decode.d8.loss_cls: 0.6089  decode.d8.loss_mask: 2.2768  decode.d8.loss_dice: 2.1646
2025/03/29 12:05:32 - mmengine - INFO - Saving checkpoint at 2000 iterations
2025/03/29 12:05:38 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:06:26  time: 0.0908  data_time: 0.0018  memory: 3081  
2025/03/29 12:05:42 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:06:05  time: 0.0906  data_time: 0.0018  memory: 3080  
2025/03/29 12:05:47 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:56  time: 0.0905  data_time: 0.0017  memory: 3080  
2025/03/29 12:05:51 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:48  time: 0.0906  data_time: 0.0017  memory: 3080  
2025/03/29 12:05:56 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:42  time: 0.0907  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:01 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:37  time: 0.0905  data_time: 0.0017  memory: 3080  
2025/03/29 12:06:05 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:31  time: 0.0907  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:10 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:26  time: 0.0908  data_time: 0.0017  memory: 3080  
2025/03/29 12:06:14 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:21  time: 0.0908  data_time: 0.0019  memory: 3080  
2025/03/29 12:06:19 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:17  time: 0.0905  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:23 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:12  time: 0.0908  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:28 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:07  time: 0.0908  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:32 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:02  time: 0.0906  data_time: 0.0017  memory: 3080  
2025/03/29 12:06:37 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:58  time: 0.0908  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:41 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:53  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:46 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:49  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 12:06:51 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:44  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 12:06:55 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:40  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:00 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:35  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:04 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:31  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:09 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:26  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:13 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:21  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:18 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:17  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:23 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:12  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 12:07:27 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:08  time: 0.0923  data_time: 0.0019  memory: 3080  
2025/03/29 12:07:32 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:03  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:36 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:59  time: 0.0908  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:41 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:54  time: 0.0908  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:45 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:49  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:50 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:45  time: 0.0909  data_time: 0.0018  memory: 3080  
2025/03/29 12:07:55 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:40  time: 0.0939  data_time: 0.0019  memory: 3080  
2025/03/29 12:07:59 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:36  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:04 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:31  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 12:08:08 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:27  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 12:08:13 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:22  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:17 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:17  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:22 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:13  time: 0.0909  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:27 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:08  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 12:08:31 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:04  time: 0.0927  data_time: 0.0020  memory: 3080  
2025/03/29 12:08:36 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:02:59  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 12:08:40 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:45 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:50  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:50 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:54 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:41  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 12:08:59 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:36  time: 0.0910  data_time: 0.0017  memory: 3080  
2025/03/29 12:09:03 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:08 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:27  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 12:09:12 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0910  data_time: 0.0017  memory: 3080  
2025/03/29 12:09:17 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:18  time: 0.0914  data_time: 0.0019  memory: 3080  
2025/03/29 12:09:21 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:26 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:31 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:04  time: 0.0913  data_time: 0.0019  memory: 3080  
2025/03/29 12:09:35 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:40 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:55  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:44 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:49 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:46  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:53 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 12:09:58 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 12:10:03 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:32  time: 0.0913  data_time: 0.0019  memory: 3080  
2025/03/29 12:10:07 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0916  data_time: 0.0020  memory: 3080  
2025/03/29 12:10:12 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:23  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 12:10:16 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:10:21 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:14  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 12:10:25 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:10:30 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 12:10:35 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:00  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 12:10:39 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0914  data_time: 0.0019  memory: 3080  
2025/03/29 12:10:44 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:51  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:10:48 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0914  data_time: 0.0017  memory: 3080  
2025/03/29 12:10:53 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 12:10:57 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0912  data_time: 0.0017  memory: 3080  
2025/03/29 12:11:02 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0912  data_time: 0.0016  memory: 3080  
2025/03/29 12:11:07 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0912  data_time: 0.0017  memory: 3080  
2025/03/29 12:11:11 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 12:11:16 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:19  time: 0.0936  data_time: 0.0018  memory: 3080  
2025/03/29 12:11:20 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 12:11:25 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 12:11:30 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 12:11:34 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0915  data_time: 0.0019  memory: 3080  
2025/03/29 12:11:36 - mmengine - INFO - per class results:
2025/03/29 12:11:36 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 38.89 | 48.67 |
|   building   | 62.12 | 77.82 |
|     road     | 28.08 | 28.44 |
|    water     |  51.7 | 60.78 |
|    barren    |  3.02 |  36.1 |
|    forest    | 29.12 | 70.79 |
| agricultural |  4.7  |  4.88 |
+--------------+-------+-------+
2025/03/29 12:11:36 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 37.9200  mIoU: 31.0900  mAcc: 46.7800  data_time: 0.0018  time: 0.0913
2025/03/29 12:11:36 - mmengine - INFO - The best checkpoint with 31.0900 mIoU at 2000 iter is saved to best_mIoU_iter_2000.pth.
2025/03/29 12:12:04 - mmengine - INFO - Iter(train) [ 2050/20000]  base_lr: 9.0730e-05 lr: 9.0730e-05  eta: 2:43:58  time: 0.5312  data_time: 0.0201  memory: 7073  loss: 47.1999  decode.loss_cls: 0.4869  decode.loss_mask: 1.9690  decode.loss_dice: 2.1076  decode.d0.loss_cls: 1.2229  decode.d0.loss_mask: 1.8927  decode.d0.loss_dice: 2.1317  decode.d1.loss_cls: 0.5385  decode.d1.loss_mask: 2.0149  decode.d1.loss_dice: 2.1366  decode.d2.loss_cls: 0.5412  decode.d2.loss_mask: 1.9900  decode.d2.loss_dice: 2.0572  decode.d3.loss_cls: 0.5738  decode.d3.loss_mask: 2.0207  decode.d3.loss_dice: 2.1008  decode.d4.loss_cls: 0.5925  decode.d4.loss_mask: 1.9596  decode.d4.loss_dice: 2.0448  decode.d5.loss_cls: 0.5996  decode.d5.loss_mask: 1.9789  decode.d5.loss_dice: 2.0822  decode.d6.loss_cls: 0.5638  decode.d6.loss_mask: 2.0444  decode.d6.loss_dice: 2.1824  decode.d7.loss_cls: 0.5735  decode.d7.loss_mask: 2.0037  decode.d7.loss_dice: 2.1501  decode.d8.loss_cls: 0.6010  decode.d8.loss_mask: 1.9300  decode.d8.loss_dice: 2.1089
2025/03/29 12:12:31 - mmengine - INFO - Iter(train) [ 2100/20000]  base_lr: 9.0502e-05 lr: 9.0502e-05  eta: 2:43:25  time: 0.5360  data_time: 0.0211  memory: 7050  loss: 42.3149  decode.loss_cls: 0.3587  decode.loss_mask: 1.8420  decode.loss_dice: 1.8779  decode.d0.loss_cls: 1.0628  decode.d0.loss_mask: 1.7516  decode.d0.loss_dice: 1.9093  decode.d1.loss_cls: 0.4836  decode.d1.loss_mask: 1.8095  decode.d1.loss_dice: 1.9446  decode.d2.loss_cls: 0.4057  decode.d2.loss_mask: 1.7921  decode.d2.loss_dice: 1.9440  decode.d3.loss_cls: 0.4660  decode.d3.loss_mask: 1.7497  decode.d3.loss_dice: 1.9265  decode.d4.loss_cls: 0.4862  decode.d4.loss_mask: 1.7660  decode.d4.loss_dice: 1.9256  decode.d5.loss_cls: 0.3895  decode.d5.loss_mask: 1.8318  decode.d5.loss_dice: 1.9638  decode.d6.loss_cls: 0.5168  decode.d6.loss_mask: 1.7906  decode.d6.loss_dice: 1.9192  decode.d7.loss_cls: 0.4706  decode.d7.loss_mask: 1.8103  decode.d7.loss_dice: 1.9843  decode.d8.loss_cls: 0.4015  decode.d8.loss_mask: 1.8343  decode.d8.loss_dice: 1.9003
2025/03/29 12:12:59 - mmengine - INFO - Iter(train) [ 2150/20000]  base_lr: 9.0275e-05 lr: 9.0275e-05  eta: 2:43:02  time: 0.5580  data_time: 0.0247  memory: 7066  loss: 42.5939  decode.loss_cls: 0.5414  decode.loss_mask: 1.8093  decode.loss_dice: 1.7651  decode.d0.loss_cls: 1.0852  decode.d0.loss_mask: 1.8159  decode.d0.loss_dice: 1.8596  decode.d1.loss_cls: 0.4898  decode.d1.loss_mask: 1.8351  decode.d1.loss_dice: 1.8263  decode.d2.loss_cls: 0.5439  decode.d2.loss_mask: 1.8673  decode.d2.loss_dice: 1.7930  decode.d3.loss_cls: 0.5477  decode.d3.loss_mask: 1.7618  decode.d3.loss_dice: 1.7420  decode.d4.loss_cls: 0.5988  decode.d4.loss_mask: 1.8988  decode.d4.loss_dice: 1.7712  decode.d5.loss_cls: 0.6012  decode.d5.loss_mask: 1.8363  decode.d5.loss_dice: 1.7675  decode.d6.loss_cls: 0.5520  decode.d6.loss_mask: 1.9201  decode.d6.loss_dice: 1.7642  decode.d7.loss_cls: 0.5615  decode.d7.loss_mask: 1.8778  decode.d7.loss_dice: 1.8545  decode.d8.loss_cls: 0.5372  decode.d8.loss_mask: 1.9226  decode.d8.loss_dice: 1.8468
2025/03/29 12:13:27 - mmengine - INFO - Iter(train) [ 2200/20000]  base_lr: 9.0047e-05 lr: 9.0047e-05  eta: 2:42:39  time: 0.5565  data_time: 0.0248  memory: 7064  loss: 51.5918  decode.loss_cls: 0.6672  decode.loss_mask: 2.3538  decode.loss_dice: 2.1116  decode.d0.loss_cls: 1.1924  decode.d0.loss_mask: 2.1525  decode.d0.loss_dice: 2.0751  decode.d1.loss_cls: 0.7024  decode.d1.loss_mask: 2.3211  decode.d1.loss_dice: 2.0658  decode.d2.loss_cls: 0.6749  decode.d2.loss_mask: 2.3468  decode.d2.loss_dice: 2.1309  decode.d3.loss_cls: 0.6355  decode.d3.loss_mask: 2.3944  decode.d3.loss_dice: 2.1495  decode.d4.loss_cls: 0.6370  decode.d4.loss_mask: 2.3403  decode.d4.loss_dice: 2.0578  decode.d5.loss_cls: 0.6220  decode.d5.loss_mask: 2.4326  decode.d5.loss_dice: 2.1420  decode.d6.loss_cls: 0.6170  decode.d6.loss_mask: 2.4147  decode.d6.loss_dice: 2.0942  decode.d7.loss_cls: 0.7826  decode.d7.loss_mask: 2.2409  decode.d7.loss_dice: 2.0659  decode.d8.loss_cls: 0.6457  decode.d8.loss_mask: 2.4088  decode.d8.loss_dice: 2.1164
2025/03/29 12:13:56 - mmengine - INFO - Iter(train) [ 2250/20000]  base_lr: 8.9820e-05 lr: 8.9820e-05  eta: 2:42:24  time: 0.5913  data_time: 0.0309  memory: 7082  loss: 49.5782  decode.loss_cls: 0.6330  decode.loss_mask: 2.2029  decode.loss_dice: 2.1842  decode.d0.loss_cls: 1.0675  decode.d0.loss_mask: 2.1688  decode.d0.loss_dice: 2.1371  decode.d1.loss_cls: 0.5808  decode.d1.loss_mask: 2.1836  decode.d1.loss_dice: 2.0900  decode.d2.loss_cls: 0.5972  decode.d2.loss_mask: 2.2139  decode.d2.loss_dice: 2.0766  decode.d3.loss_cls: 0.6184  decode.d3.loss_mask: 2.2272  decode.d3.loss_dice: 2.1565  decode.d4.loss_cls: 0.6001  decode.d4.loss_mask: 2.2154  decode.d4.loss_dice: 2.1215  decode.d5.loss_cls: 0.5781  decode.d5.loss_mask: 2.2594  decode.d5.loss_dice: 2.1435  decode.d6.loss_cls: 0.5301  decode.d6.loss_mask: 2.2333  decode.d6.loss_dice: 2.1152  decode.d7.loss_cls: 0.5187  decode.d7.loss_mask: 2.2167  decode.d7.loss_dice: 2.1017  decode.d8.loss_cls: 0.5152  decode.d8.loss_mask: 2.1877  decode.d8.loss_dice: 2.1037
2025/03/29 12:14:26 - mmengine - INFO - Iter(train) [ 2300/20000]  base_lr: 8.9592e-05 lr: 8.9592e-05  eta: 2:42:16  time: 0.5943  data_time: 0.0293  memory: 7072  loss: 45.4498  decode.loss_cls: 0.6948  decode.loss_mask: 1.8298  decode.loss_dice: 1.9472  decode.d0.loss_cls: 1.1180  decode.d0.loss_mask: 1.8588  decode.d0.loss_dice: 2.0387  decode.d1.loss_cls: 0.5931  decode.d1.loss_mask: 1.8738  decode.d1.loss_dice: 2.0246  decode.d2.loss_cls: 0.6452  decode.d2.loss_mask: 1.8597  decode.d2.loss_dice: 2.0287  decode.d3.loss_cls: 0.6882  decode.d3.loss_mask: 1.8353  decode.d3.loss_dice: 1.9517  decode.d4.loss_cls: 0.5827  decode.d4.loss_mask: 1.9025  decode.d4.loss_dice: 1.9478  decode.d5.loss_cls: 0.6873  decode.d5.loss_mask: 1.8962  decode.d5.loss_dice: 1.9415  decode.d6.loss_cls: 0.7793  decode.d6.loss_mask: 1.7699  decode.d6.loss_dice: 1.9024  decode.d7.loss_cls: 0.6987  decode.d7.loss_mask: 1.8662  decode.d7.loss_dice: 1.9630  decode.d8.loss_cls: 0.5998  decode.d8.loss_mask: 1.9156  decode.d8.loss_dice: 2.0094
2025/03/29 12:14:53 - mmengine - INFO - Iter(train) [ 2350/20000]  base_lr: 8.9364e-05 lr: 8.9364e-05  eta: 2:41:46  time: 0.5595  data_time: 0.0243  memory: 7057  loss: 46.7986  decode.loss_cls: 0.5465  decode.loss_mask: 2.1202  decode.loss_dice: 1.9753  decode.d0.loss_cls: 1.0703  decode.d0.loss_mask: 2.0920  decode.d0.loss_dice: 1.9825  decode.d1.loss_cls: 0.5857  decode.d1.loss_mask: 2.1157  decode.d1.loss_dice: 1.9404  decode.d2.loss_cls: 0.5443  decode.d2.loss_mask: 2.1284  decode.d2.loss_dice: 1.9800  decode.d3.loss_cls: 0.6444  decode.d3.loss_mask: 2.1220  decode.d3.loss_dice: 1.9530  decode.d4.loss_cls: 0.5684  decode.d4.loss_mask: 2.0675  decode.d4.loss_dice: 1.9802  decode.d5.loss_cls: 0.5310  decode.d5.loss_mask: 2.0728  decode.d5.loss_dice: 1.9674  decode.d6.loss_cls: 0.6468  decode.d6.loss_mask: 2.0938  decode.d6.loss_dice: 1.8944  decode.d7.loss_cls: 0.5677  decode.d7.loss_mask: 2.1315  decode.d7.loss_dice: 1.9339  decode.d8.loss_cls: 0.5353  decode.d8.loss_mask: 2.0871  decode.d8.loss_dice: 1.9202
2025/03/29 12:15:21 - mmengine - INFO - Iter(train) [ 2400/20000]  base_lr: 8.9136e-05 lr: 8.9136e-05  eta: 2:41:23  time: 0.5594  data_time: 0.0240  memory: 7070  loss: 45.0664  decode.loss_cls: 0.6131  decode.loss_mask: 1.9209  decode.loss_dice: 1.9360  decode.d0.loss_cls: 0.8889  decode.d0.loss_mask: 1.9527  decode.d0.loss_dice: 2.0281  decode.d1.loss_cls: 0.4188  decode.d1.loss_mask: 1.9910  decode.d1.loss_dice: 2.0182  decode.d2.loss_cls: 0.4319  decode.d2.loss_mask: 1.9697  decode.d2.loss_dice: 2.0530  decode.d3.loss_cls: 0.4915  decode.d3.loss_mask: 1.9713  decode.d3.loss_dice: 1.9866  decode.d4.loss_cls: 0.5715  decode.d4.loss_mask: 1.9543  decode.d4.loss_dice: 1.9431  decode.d5.loss_cls: 0.5757  decode.d5.loss_mask: 1.9336  decode.d5.loss_dice: 1.9516  decode.d6.loss_cls: 0.6029  decode.d6.loss_mask: 1.9180  decode.d6.loss_dice: 1.9342  decode.d7.loss_cls: 0.6054  decode.d7.loss_mask: 1.9897  decode.d7.loss_dice: 1.9518  decode.d8.loss_cls: 0.6496  decode.d8.loss_mask: 1.8941  decode.d8.loss_dice: 1.9193
2025/03/29 12:15:48 - mmengine - INFO - Iter(train) [ 2450/20000]  base_lr: 8.8908e-05 lr: 8.8908e-05  eta: 2:40:51  time: 0.5313  data_time: 0.0205  memory: 7051  loss: 48.8490  decode.loss_cls: 0.6470  decode.loss_mask: 2.2061  decode.loss_dice: 2.0432  decode.d0.loss_cls: 0.9700  decode.d0.loss_mask: 2.1653  decode.d0.loss_dice: 2.0611  decode.d1.loss_cls: 0.6054  decode.d1.loss_mask: 2.1780  decode.d1.loss_dice: 2.0919  decode.d2.loss_cls: 0.6634  decode.d2.loss_mask: 2.2034  decode.d2.loss_dice: 2.0577  decode.d3.loss_cls: 0.5713  decode.d3.loss_mask: 2.1786  decode.d3.loss_dice: 2.0212  decode.d4.loss_cls: 0.6324  decode.d4.loss_mask: 2.2399  decode.d4.loss_dice: 2.0124  decode.d5.loss_cls: 0.6185  decode.d5.loss_mask: 2.1878  decode.d5.loss_dice: 2.0092  decode.d6.loss_cls: 0.6437  decode.d6.loss_mask: 2.1949  decode.d6.loss_dice: 2.0096  decode.d7.loss_cls: 0.6493  decode.d7.loss_mask: 2.1674  decode.d7.loss_dice: 2.0063  decode.d8.loss_cls: 0.6384  decode.d8.loss_mask: 2.1584  decode.d8.loss_dice: 2.0173
2025/03/29 12:16:15 - mmengine - INFO - Iter(train) [ 2500/20000]  base_lr: 8.8680e-05 lr: 8.8680e-05  eta: 2:40:23  time: 0.5553  data_time: 0.0241  memory: 7072  loss: 47.4769  decode.loss_cls: 0.6566  decode.loss_mask: 1.9723  decode.loss_dice: 2.0949  decode.d0.loss_cls: 0.9621  decode.d0.loss_mask: 1.8855  decode.d0.loss_dice: 2.1306  decode.d1.loss_cls: 0.5624  decode.d1.loss_mask: 1.9627  decode.d1.loss_dice: 2.1162  decode.d2.loss_cls: 0.5542  decode.d2.loss_mask: 1.9349  decode.d2.loss_dice: 2.1212  decode.d3.loss_cls: 0.5722  decode.d3.loss_mask: 1.9756  decode.d3.loss_dice: 2.1121  decode.d4.loss_cls: 0.7288  decode.d4.loss_mask: 1.9091  decode.d4.loss_dice: 2.0536  decode.d5.loss_cls: 0.6891  decode.d5.loss_mask: 2.0001  decode.d5.loss_dice: 2.0916  decode.d6.loss_cls: 0.7420  decode.d6.loss_mask: 1.9992  decode.d6.loss_dice: 2.0841  decode.d7.loss_cls: 0.6725  decode.d7.loss_mask: 1.9773  decode.d7.loss_dice: 2.1018  decode.d8.loss_cls: 0.7223  decode.d8.loss_mask: 1.9756  decode.d8.loss_dice: 2.1167
2025/03/29 12:16:43 - mmengine - INFO - Iter(train) [ 2550/20000]  base_lr: 8.8452e-05 lr: 8.8452e-05  eta: 2:39:58  time: 0.5547  data_time: 0.0239  memory: 7074  loss: 49.4082  decode.loss_cls: 0.5471  decode.loss_mask: 2.3555  decode.loss_dice: 2.1266  decode.d0.loss_cls: 0.9228  decode.d0.loss_mask: 2.2749  decode.d0.loss_dice: 2.0513  decode.d1.loss_cls: 0.5793  decode.d1.loss_mask: 2.2326  decode.d1.loss_dice: 2.0393  decode.d2.loss_cls: 0.5332  decode.d2.loss_mask: 2.2753  decode.d2.loss_dice: 2.0908  decode.d3.loss_cls: 0.4938  decode.d3.loss_mask: 2.2828  decode.d3.loss_dice: 2.0856  decode.d4.loss_cls: 0.4928  decode.d4.loss_mask: 2.2938  decode.d4.loss_dice: 2.0256  decode.d5.loss_cls: 0.5169  decode.d5.loss_mask: 2.2712  decode.d5.loss_dice: 2.1091  decode.d6.loss_cls: 0.6617  decode.d6.loss_mask: 2.2460  decode.d6.loss_dice: 2.0445  decode.d7.loss_cls: 0.5306  decode.d7.loss_mask: 2.3061  decode.d7.loss_dice: 2.1077  decode.d8.loss_cls: 0.5550  decode.d8.loss_mask: 2.2990  decode.d8.loss_dice: 2.0573
2025/03/29 12:17:11 - mmengine - INFO - Iter(train) [ 2600/20000]  base_lr: 8.8224e-05 lr: 8.8224e-05  eta: 2:39:29  time: 0.5536  data_time: 0.0241  memory: 7062  loss: 46.1226  decode.loss_cls: 0.4653  decode.loss_mask: 2.1525  decode.loss_dice: 2.0082  decode.d0.loss_cls: 0.9033  decode.d0.loss_mask: 1.9975  decode.d0.loss_dice: 2.0456  decode.d1.loss_cls: 0.4632  decode.d1.loss_mask: 2.0904  decode.d1.loss_dice: 1.9822  decode.d2.loss_cls: 0.4525  decode.d2.loss_mask: 2.0508  decode.d2.loss_dice: 1.9853  decode.d3.loss_cls: 0.4640  decode.d3.loss_mask: 2.0816  decode.d3.loss_dice: 2.0040  decode.d4.loss_cls: 0.4926  decode.d4.loss_mask: 2.0665  decode.d4.loss_dice: 1.9874  decode.d5.loss_cls: 0.5305  decode.d5.loss_mask: 2.1169  decode.d5.loss_dice: 2.0097  decode.d6.loss_cls: 0.4989  decode.d6.loss_mask: 2.0725  decode.d6.loss_dice: 1.9712  decode.d7.loss_cls: 0.5489  decode.d7.loss_mask: 2.1306  decode.d7.loss_dice: 1.9569  decode.d8.loss_cls: 0.4723  decode.d8.loss_mask: 2.1087  decode.d8.loss_dice: 2.0124
2025/03/29 12:17:37 - mmengine - INFO - Iter(train) [ 2650/20000]  base_lr: 8.7996e-05 lr: 8.7996e-05  eta: 2:38:55  time: 0.5281  data_time: 0.0202  memory: 7075  loss: 45.9560  decode.loss_cls: 0.5649  decode.loss_mask: 2.1265  decode.loss_dice: 1.8094  decode.d0.loss_cls: 0.9075  decode.d0.loss_mask: 2.1181  decode.d0.loss_dice: 1.8589  decode.d1.loss_cls: 0.5054  decode.d1.loss_mask: 2.1865  decode.d1.loss_dice: 1.8435  decode.d2.loss_cls: 0.4681  decode.d2.loss_mask: 2.2716  decode.d2.loss_dice: 1.8360  decode.d3.loss_cls: 0.5234  decode.d3.loss_mask: 2.2318  decode.d3.loss_dice: 1.9408  decode.d4.loss_cls: 0.4730  decode.d4.loss_mask: 2.2814  decode.d4.loss_dice: 1.8327  decode.d5.loss_cls: 0.6110  decode.d5.loss_mask: 2.1263  decode.d5.loss_dice: 1.8181  decode.d6.loss_cls: 0.6008  decode.d6.loss_mask: 2.1805  decode.d6.loss_dice: 1.7703  decode.d7.loss_cls: 0.6370  decode.d7.loss_mask: 2.1041  decode.d7.loss_dice: 1.8047  decode.d8.loss_cls: 0.6066  decode.d8.loss_mask: 2.1279  decode.d8.loss_dice: 1.7894
2025/03/29 12:18:04 - mmengine - INFO - Iter(train) [ 2700/20000]  base_lr: 8.7768e-05 lr: 8.7768e-05  eta: 2:38:22  time: 0.5285  data_time: 0.0205  memory: 7053  loss: 44.7124  decode.loss_cls: 0.4206  decode.loss_mask: 2.0039  decode.loss_dice: 1.9819  decode.d0.loss_cls: 0.8560  decode.d0.loss_mask: 1.9650  decode.d0.loss_dice: 1.9930  decode.d1.loss_cls: 0.4549  decode.d1.loss_mask: 1.9497  decode.d1.loss_dice: 1.9654  decode.d2.loss_cls: 0.4489  decode.d2.loss_mask: 1.9580  decode.d2.loss_dice: 1.9604  decode.d3.loss_cls: 0.4193  decode.d3.loss_mask: 1.9536  decode.d3.loss_dice: 1.9522  decode.d4.loss_cls: 0.4334  decode.d4.loss_mask: 2.0505  decode.d4.loss_dice: 2.0027  decode.d5.loss_cls: 0.4777  decode.d5.loss_mask: 2.0441  decode.d5.loss_dice: 2.0090  decode.d6.loss_cls: 0.4866  decode.d6.loss_mask: 1.9954  decode.d6.loss_dice: 1.9969  decode.d7.loss_cls: 0.4783  decode.d7.loss_mask: 1.9883  decode.d7.loss_dice: 2.0031  decode.d8.loss_cls: 0.4175  decode.d8.loss_mask: 2.0254  decode.d8.loss_dice: 2.0210
2025/03/29 12:18:21 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 12:18:31 - mmengine - INFO - Iter(train) [ 2750/20000]  base_lr: 8.7539e-05 lr: 8.7539e-05  eta: 2:37:52  time: 0.5625  data_time: 0.0267  memory: 7062  loss: 43.8109  decode.loss_cls: 0.4447  decode.loss_mask: 1.9833  decode.loss_dice: 1.9159  decode.d0.loss_cls: 0.8455  decode.d0.loss_mask: 2.0005  decode.d0.loss_dice: 1.9354  decode.d1.loss_cls: 0.3791  decode.d1.loss_mask: 2.0111  decode.d1.loss_dice: 1.9293  decode.d2.loss_cls: 0.3953  decode.d2.loss_mask: 2.0368  decode.d2.loss_dice: 1.8898  decode.d3.loss_cls: 0.4276  decode.d3.loss_mask: 1.9929  decode.d3.loss_dice: 1.8621  decode.d4.loss_cls: 0.5069  decode.d4.loss_mask: 1.9885  decode.d4.loss_dice: 1.9075  decode.d5.loss_cls: 0.4822  decode.d5.loss_mask: 1.9859  decode.d5.loss_dice: 1.9114  decode.d6.loss_cls: 0.4408  decode.d6.loss_mask: 2.0207  decode.d6.loss_dice: 1.8879  decode.d7.loss_cls: 0.4637  decode.d7.loss_mask: 1.9830  decode.d7.loss_dice: 1.8539  decode.d8.loss_cls: 0.4479  decode.d8.loss_mask: 2.0124  decode.d8.loss_dice: 1.8687
2025/03/29 12:18:59 - mmengine - INFO - Iter(train) [ 2800/20000]  base_lr: 8.7311e-05 lr: 8.7311e-05  eta: 2:37:32  time: 0.5629  data_time: 0.0259  memory: 7058  loss: 45.5571  decode.loss_cls: 0.5024  decode.loss_mask: 1.9618  decode.loss_dice: 2.0831  decode.d0.loss_cls: 0.8557  decode.d0.loss_mask: 1.8206  decode.d0.loss_dice: 2.0507  decode.d1.loss_cls: 0.4756  decode.d1.loss_mask: 1.8139  decode.d1.loss_dice: 2.0160  decode.d2.loss_cls: 0.5338  decode.d2.loss_mask: 1.8722  decode.d2.loss_dice: 2.0685  decode.d3.loss_cls: 0.5000  decode.d3.loss_mask: 1.8276  decode.d3.loss_dice: 2.0882  decode.d4.loss_cls: 0.6204  decode.d4.loss_mask: 1.8613  decode.d4.loss_dice: 2.0570  decode.d5.loss_cls: 0.6802  decode.d5.loss_mask: 2.0022  decode.d5.loss_dice: 2.1143  decode.d6.loss_cls: 0.6213  decode.d6.loss_mask: 1.9133  decode.d6.loss_dice: 2.1073  decode.d7.loss_cls: 0.6218  decode.d7.loss_mask: 1.9645  decode.d7.loss_dice: 2.0043  decode.d8.loss_cls: 0.5575  decode.d8.loss_mask: 1.9193  decode.d8.loss_dice: 2.0424
2025/03/29 12:19:26 - mmengine - INFO - Iter(train) [ 2850/20000]  base_lr: 8.7082e-05 lr: 8.7082e-05  eta: 2:37:00  time: 0.5332  data_time: 0.0203  memory: 7056  loss: 42.5851  decode.loss_cls: 0.3973  decode.loss_mask: 1.8916  decode.loss_dice: 1.9108  decode.d0.loss_cls: 0.8346  decode.d0.loss_mask: 1.8669  decode.d0.loss_dice: 1.9402  decode.d1.loss_cls: 0.4510  decode.d1.loss_mask: 1.8843  decode.d1.loss_dice: 1.9239  decode.d2.loss_cls: 0.4510  decode.d2.loss_mask: 1.8773  decode.d2.loss_dice: 1.9080  decode.d3.loss_cls: 0.4674  decode.d3.loss_mask: 1.8590  decode.d3.loss_dice: 1.8948  decode.d4.loss_cls: 0.4706  decode.d4.loss_mask: 1.8505  decode.d4.loss_dice: 1.8766  decode.d5.loss_cls: 0.5328  decode.d5.loss_mask: 1.7832  decode.d5.loss_dice: 1.8737  decode.d6.loss_cls: 0.3973  decode.d6.loss_mask: 1.8408  decode.d6.loss_dice: 1.8759  decode.d7.loss_cls: 0.4402  decode.d7.loss_mask: 1.9234  decode.d7.loss_dice: 1.9391  decode.d8.loss_cls: 0.4569  decode.d8.loss_mask: 1.8592  decode.d8.loss_dice: 1.9066
2025/03/29 12:19:53 - mmengine - INFO - Iter(train) [ 2900/20000]  base_lr: 8.6854e-05 lr: 8.6854e-05  eta: 2:36:31  time: 0.5296  data_time: 0.0207  memory: 7051  loss: 45.0305  decode.loss_cls: 0.5934  decode.loss_mask: 2.0717  decode.loss_dice: 1.8235  decode.d0.loss_cls: 0.8919  decode.d0.loss_mask: 2.0682  decode.d0.loss_dice: 1.7980  decode.d1.loss_cls: 0.5052  decode.d1.loss_mask: 2.0797  decode.d1.loss_dice: 1.7793  decode.d2.loss_cls: 0.4518  decode.d2.loss_mask: 2.0964  decode.d2.loss_dice: 1.8529  decode.d3.loss_cls: 0.4712  decode.d3.loss_mask: 2.1437  decode.d3.loss_dice: 1.9000  decode.d4.loss_cls: 0.4869  decode.d4.loss_mask: 2.1429  decode.d4.loss_dice: 1.8654  decode.d5.loss_cls: 0.6305  decode.d5.loss_mask: 2.0325  decode.d5.loss_dice: 1.8381  decode.d6.loss_cls: 0.5479  decode.d6.loss_mask: 2.1290  decode.d6.loss_dice: 1.8041  decode.d7.loss_cls: 0.5053  decode.d7.loss_mask: 2.1831  decode.d7.loss_dice: 1.8764  decode.d8.loss_cls: 0.5704  decode.d8.loss_mask: 2.1015  decode.d8.loss_dice: 1.7896
2025/03/29 12:20:20 - mmengine - INFO - Iter(train) [ 2950/20000]  base_lr: 8.6625e-05 lr: 8.6625e-05  eta: 2:35:59  time: 0.5340  data_time: 0.0210  memory: 7074  loss: 49.8922  decode.loss_cls: 0.5339  decode.loss_mask: 2.4849  decode.loss_dice: 2.0367  decode.d0.loss_cls: 0.8393  decode.d0.loss_mask: 2.4559  decode.d0.loss_dice: 2.0259  decode.d1.loss_cls: 0.4492  decode.d1.loss_mask: 2.4474  decode.d1.loss_dice: 2.0440  decode.d2.loss_cls: 0.5156  decode.d2.loss_mask: 2.4056  decode.d2.loss_dice: 2.0237  decode.d3.loss_cls: 0.4695  decode.d3.loss_mask: 2.4398  decode.d3.loss_dice: 2.0607  decode.d4.loss_cls: 0.4708  decode.d4.loss_mask: 2.4264  decode.d4.loss_dice: 2.0575  decode.d5.loss_cls: 0.5287  decode.d5.loss_mask: 2.3802  decode.d5.loss_dice: 2.0049  decode.d6.loss_cls: 0.5314  decode.d6.loss_mask: 2.4323  decode.d6.loss_dice: 1.9773  decode.d7.loss_cls: 0.4924  decode.d7.loss_mask: 2.4157  decode.d7.loss_dice: 2.0103  decode.d8.loss_cls: 0.4872  decode.d8.loss_mask: 2.4224  decode.d8.loss_dice: 2.0225
2025/03/29 12:20:48 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 12:20:48 - mmengine - INFO - Iter(train) [ 3000/20000]  base_lr: 8.6397e-05 lr: 8.6397e-05  eta: 2:35:32  time: 0.5617  data_time: 0.0254  memory: 7052  loss: 48.1835  decode.loss_cls: 0.5626  decode.loss_mask: 2.2155  decode.loss_dice: 2.0693  decode.d0.loss_cls: 0.8218  decode.d0.loss_mask: 2.0936  decode.d0.loss_dice: 2.0570  decode.d1.loss_cls: 0.6052  decode.d1.loss_mask: 2.1446  decode.d1.loss_dice: 1.9562  decode.d2.loss_cls: 0.6986  decode.d2.loss_mask: 2.1388  decode.d2.loss_dice: 2.0330  decode.d3.loss_cls: 0.7117  decode.d3.loss_mask: 2.1529  decode.d3.loss_dice: 2.0471  decode.d4.loss_cls: 0.6215  decode.d4.loss_mask: 2.1688  decode.d4.loss_dice: 2.0504  decode.d5.loss_cls: 0.5933  decode.d5.loss_mask: 2.1725  decode.d5.loss_dice: 2.0079  decode.d6.loss_cls: 0.5919  decode.d6.loss_mask: 2.1041  decode.d6.loss_dice: 1.9985  decode.d7.loss_cls: 0.6159  decode.d7.loss_mask: 2.1232  decode.d7.loss_dice: 1.9882  decode.d8.loss_cls: 0.6358  decode.d8.loss_mask: 2.1787  decode.d8.loss_dice: 2.0250
2025/03/29 12:21:15 - mmengine - INFO - Iter(train) [ 3050/20000]  base_lr: 8.6168e-05 lr: 8.6168e-05  eta: 2:35:03  time: 0.5316  data_time: 0.0201  memory: 7063  loss: 40.2093  decode.loss_cls: 0.3740  decode.loss_mask: 1.9256  decode.loss_dice: 1.6283  decode.d0.loss_cls: 0.7368  decode.d0.loss_mask: 1.9283  decode.d0.loss_dice: 1.7044  decode.d1.loss_cls: 0.3388  decode.d1.loss_mask: 1.9179  decode.d1.loss_dice: 1.6568  decode.d2.loss_cls: 0.2676  decode.d2.loss_mask: 2.0033  decode.d2.loss_dice: 1.7097  decode.d3.loss_cls: 0.3964  decode.d3.loss_mask: 1.9650  decode.d3.loss_dice: 1.6774  decode.d4.loss_cls: 0.2998  decode.d4.loss_mask: 1.9393  decode.d4.loss_dice: 1.6667  decode.d5.loss_cls: 0.3907  decode.d5.loss_mask: 1.9855  decode.d5.loss_dice: 1.7477  decode.d6.loss_cls: 0.3679  decode.d6.loss_mask: 1.9703  decode.d6.loss_dice: 1.7066  decode.d7.loss_cls: 0.4102  decode.d7.loss_mask: 1.9267  decode.d7.loss_dice: 1.6457  decode.d8.loss_cls: 0.3433  decode.d8.loss_mask: 1.9353  decode.d8.loss_dice: 1.6433
2025/03/29 12:21:42 - mmengine - INFO - Iter(train) [ 3100/20000]  base_lr: 8.5939e-05 lr: 8.5939e-05  eta: 2:34:35  time: 0.5565  data_time: 0.0253  memory: 7069  loss: 43.9522  decode.loss_cls: 0.6632  decode.loss_mask: 1.7210  decode.loss_dice: 1.9562  decode.d0.loss_cls: 0.8383  decode.d0.loss_mask: 1.7111  decode.d0.loss_dice: 1.9978  decode.d1.loss_cls: 0.6779  decode.d1.loss_mask: 1.7108  decode.d1.loss_dice: 1.9882  decode.d2.loss_cls: 0.5858  decode.d2.loss_mask: 1.7005  decode.d2.loss_dice: 2.0093  decode.d3.loss_cls: 0.6541  decode.d3.loss_mask: 1.7363  decode.d3.loss_dice: 1.9659  decode.d4.loss_cls: 0.6728  decode.d4.loss_mask: 1.7381  decode.d4.loss_dice: 2.0297  decode.d5.loss_cls: 0.7314  decode.d5.loss_mask: 1.7444  decode.d5.loss_dice: 2.0059  decode.d6.loss_cls: 0.7253  decode.d6.loss_mask: 1.7373  decode.d6.loss_dice: 1.9665  decode.d7.loss_cls: 0.6963  decode.d7.loss_mask: 1.7114  decode.d7.loss_dice: 1.9754  decode.d8.loss_cls: 0.6712  decode.d8.loss_mask: 1.7019  decode.d8.loss_dice: 1.9283
2025/03/29 12:22:09 - mmengine - INFO - Iter(train) [ 3150/20000]  base_lr: 8.5710e-05 lr: 8.5710e-05  eta: 2:34:04  time: 0.5299  data_time: 0.0206  memory: 7066  loss: 52.3134  decode.loss_cls: 0.5736  decode.loss_mask: 2.2498  decode.loss_dice: 2.2952  decode.d0.loss_cls: 0.8982  decode.d0.loss_mask: 2.2554  decode.d0.loss_dice: 2.3658  decode.d1.loss_cls: 0.6527  decode.d1.loss_mask: 2.2188  decode.d1.loss_dice: 2.2931  decode.d2.loss_cls: 0.6420  decode.d2.loss_mask: 2.1892  decode.d2.loss_dice: 2.2435  decode.d3.loss_cls: 0.6920  decode.d3.loss_mask: 2.2424  decode.d3.loss_dice: 2.2871  decode.d4.loss_cls: 0.6460  decode.d4.loss_mask: 2.2519  decode.d4.loss_dice: 2.2614  decode.d5.loss_cls: 0.8233  decode.d5.loss_mask: 2.2994  decode.d5.loss_dice: 2.2942  decode.d6.loss_cls: 0.7077  decode.d6.loss_mask: 2.2602  decode.d6.loss_dice: 2.2959  decode.d7.loss_cls: 0.6611  decode.d7.loss_mask: 2.2944  decode.d7.loss_dice: 2.2828  decode.d8.loss_cls: 0.6814  decode.d8.loss_mask: 2.2126  decode.d8.loss_dice: 2.2422
2025/03/29 12:22:36 - mmengine - INFO - Iter(train) [ 3200/20000]  base_lr: 8.5481e-05 lr: 8.5481e-05  eta: 2:33:34  time: 0.5432  data_time: 0.0219  memory: 7068  loss: 47.0559  decode.loss_cls: 0.6250  decode.loss_mask: 1.9698  decode.loss_dice: 2.0436  decode.d0.loss_cls: 0.8446  decode.d0.loss_mask: 2.0137  decode.d0.loss_dice: 2.1119  decode.d1.loss_cls: 0.6169  decode.d1.loss_mask: 1.9950  decode.d1.loss_dice: 2.1007  decode.d2.loss_cls: 0.5664  decode.d2.loss_mask: 1.9869  decode.d2.loss_dice: 2.1344  decode.d3.loss_cls: 0.5228  decode.d3.loss_mask: 1.9603  decode.d3.loss_dice: 2.0589  decode.d4.loss_cls: 0.6317  decode.d4.loss_mask: 2.0062  decode.d4.loss_dice: 2.1181  decode.d5.loss_cls: 0.7421  decode.d5.loss_mask: 1.9987  decode.d5.loss_dice: 2.0736  decode.d6.loss_cls: 0.6654  decode.d6.loss_mask: 1.9410  decode.d6.loss_dice: 2.0425  decode.d7.loss_cls: 0.6381  decode.d7.loss_mask: 1.9677  decode.d7.loss_dice: 2.0849  decode.d8.loss_cls: 0.6053  decode.d8.loss_mask: 1.9270  decode.d8.loss_dice: 2.0625
2025/03/29 12:23:04 - mmengine - INFO - Iter(train) [ 3250/20000]  base_lr: 8.5252e-05 lr: 8.5252e-05  eta: 2:33:10  time: 0.5608  data_time: 0.0263  memory: 7074  loss: 44.1588  decode.loss_cls: 0.4542  decode.loss_mask: 2.0070  decode.loss_dice: 1.9056  decode.d0.loss_cls: 0.7240  decode.d0.loss_mask: 1.9807  decode.d0.loss_dice: 1.9371  decode.d1.loss_cls: 0.3861  decode.d1.loss_mask: 2.0491  decode.d1.loss_dice: 1.9036  decode.d2.loss_cls: 0.3821  decode.d2.loss_mask: 2.0393  decode.d2.loss_dice: 1.8840  decode.d3.loss_cls: 0.3723  decode.d3.loss_mask: 2.0839  decode.d3.loss_dice: 1.9279  decode.d4.loss_cls: 0.4931  decode.d4.loss_mask: 1.9682  decode.d4.loss_dice: 1.8751  decode.d5.loss_cls: 0.5587  decode.d5.loss_mask: 1.9852  decode.d5.loss_dice: 1.9608  decode.d6.loss_cls: 0.5109  decode.d6.loss_mask: 1.9340  decode.d6.loss_dice: 1.8898  decode.d7.loss_cls: 0.4869  decode.d7.loss_mask: 2.0223  decode.d7.loss_dice: 1.8994  decode.d8.loss_cls: 0.4879  decode.d8.loss_mask: 2.0828  decode.d8.loss_dice: 1.9668
2025/03/29 12:23:32 - mmengine - INFO - Iter(train) [ 3300/20000]  base_lr: 8.5023e-05 lr: 8.5023e-05  eta: 2:32:47  time: 0.5823  data_time: 0.0286  memory: 7062  loss: 48.3631  decode.loss_cls: 0.4584  decode.loss_mask: 2.1689  decode.loss_dice: 2.0992  decode.d0.loss_cls: 0.7400  decode.d0.loss_mask: 2.1372  decode.d0.loss_dice: 2.1466  decode.d1.loss_cls: 0.5011  decode.d1.loss_mask: 2.2101  decode.d1.loss_dice: 2.1659  decode.d2.loss_cls: 0.6016  decode.d2.loss_mask: 2.1387  decode.d2.loss_dice: 2.0778  decode.d3.loss_cls: 0.5265  decode.d3.loss_mask: 2.1405  decode.d3.loss_dice: 2.0653  decode.d4.loss_cls: 0.5188  decode.d4.loss_mask: 2.1850  decode.d4.loss_dice: 2.0858  decode.d5.loss_cls: 0.6141  decode.d5.loss_mask: 2.1339  decode.d5.loss_dice: 2.1084  decode.d6.loss_cls: 0.5887  decode.d6.loss_mask: 2.1607  decode.d6.loss_dice: 2.0831  decode.d7.loss_cls: 0.5714  decode.d7.loss_mask: 2.1934  decode.d7.loss_dice: 2.1175  decode.d8.loss_cls: 0.5497  decode.d8.loss_mask: 2.1627  decode.d8.loss_dice: 2.1118
2025/03/29 12:24:00 - mmengine - INFO - Iter(train) [ 3350/20000]  base_lr: 8.4794e-05 lr: 8.4794e-05  eta: 2:32:23  time: 0.5639  data_time: 0.0270  memory: 7069  loss: 45.4684  decode.loss_cls: 0.4036  decode.loss_mask: 2.0736  decode.loss_dice: 2.0803  decode.d0.loss_cls: 0.5388  decode.d0.loss_mask: 2.1223  decode.d0.loss_dice: 2.0999  decode.d1.loss_cls: 0.2717  decode.d1.loss_mask: 2.1327  decode.d1.loss_dice: 2.0527  decode.d2.loss_cls: 0.3705  decode.d2.loss_mask: 2.1295  decode.d2.loss_dice: 2.0673  decode.d3.loss_cls: 0.3719  decode.d3.loss_mask: 2.1006  decode.d3.loss_dice: 2.0742  decode.d4.loss_cls: 0.3837  decode.d4.loss_mask: 2.0760  decode.d4.loss_dice: 2.0523  decode.d5.loss_cls: 0.3885  decode.d5.loss_mask: 2.1351  decode.d5.loss_dice: 2.0616  decode.d6.loss_cls: 0.3993  decode.d6.loss_mask: 2.0373  decode.d6.loss_dice: 2.0348  decode.d7.loss_cls: 0.3486  decode.d7.loss_mask: 2.0374  decode.d7.loss_dice: 2.0702  decode.d8.loss_cls: 0.4474  decode.d8.loss_mask: 2.0705  decode.d8.loss_dice: 2.0361
2025/03/29 12:24:30 - mmengine - INFO - Iter(train) [ 3400/20000]  base_lr: 8.4565e-05 lr: 8.4565e-05  eta: 2:32:04  time: 0.5952  data_time: 0.0296  memory: 7068  loss: 46.3578  decode.loss_cls: 0.7053  decode.loss_mask: 2.0023  decode.loss_dice: 1.9132  decode.d0.loss_cls: 0.7860  decode.d0.loss_mask: 2.0115  decode.d0.loss_dice: 2.0128  decode.d1.loss_cls: 0.7070  decode.d1.loss_mask: 2.0201  decode.d1.loss_dice: 1.9503  decode.d2.loss_cls: 0.6312  decode.d2.loss_mask: 2.0097  decode.d2.loss_dice: 1.9117  decode.d3.loss_cls: 0.6702  decode.d3.loss_mask: 2.0568  decode.d3.loss_dice: 1.9412  decode.d4.loss_cls: 0.6527  decode.d4.loss_mask: 2.0250  decode.d4.loss_dice: 1.9246  decode.d5.loss_cls: 0.5878  decode.d5.loss_mask: 2.0796  decode.d5.loss_dice: 1.9694  decode.d6.loss_cls: 0.6683  decode.d6.loss_mask: 2.0262  decode.d6.loss_dice: 1.9730  decode.d7.loss_cls: 0.5518  decode.d7.loss_mask: 2.0742  decode.d7.loss_dice: 1.9599  decode.d8.loss_cls: 0.4935  decode.d8.loss_mask: 2.0955  decode.d8.loss_dice: 1.9467
2025/03/29 12:24:58 - mmengine - INFO - Iter(train) [ 3450/20000]  base_lr: 8.4336e-05 lr: 8.4336e-05  eta: 2:31:43  time: 0.6175  data_time: 0.0289  memory: 7053  loss: 48.7140  decode.loss_cls: 0.6140  decode.loss_mask: 2.2312  decode.loss_dice: 1.9821  decode.d0.loss_cls: 0.7747  decode.d0.loss_mask: 2.2322  decode.d0.loss_dice: 1.9975  decode.d1.loss_cls: 0.6713  decode.d1.loss_mask: 2.2250  decode.d1.loss_dice: 1.9784  decode.d2.loss_cls: 0.5153  decode.d2.loss_mask: 2.2855  decode.d2.loss_dice: 1.9767  decode.d3.loss_cls: 0.6203  decode.d3.loss_mask: 2.2948  decode.d3.loss_dice: 2.0018  decode.d4.loss_cls: 0.6526  decode.d4.loss_mask: 2.2737  decode.d4.loss_dice: 1.9380  decode.d5.loss_cls: 0.5046  decode.d5.loss_mask: 2.2792  decode.d5.loss_dice: 1.9951  decode.d6.loss_cls: 0.5526  decode.d6.loss_mask: 2.2794  decode.d6.loss_dice: 2.0607  decode.d7.loss_cls: 0.5562  decode.d7.loss_mask: 2.2826  decode.d7.loss_dice: 2.0341  decode.d8.loss_cls: 0.5772  decode.d8.loss_mask: 2.3137  decode.d8.loss_dice: 2.0136
2025/03/29 12:25:28 - mmengine - INFO - Iter(train) [ 3500/20000]  base_lr: 8.4106e-05 lr: 8.4106e-05  eta: 2:31:26  time: 0.6003  data_time: 0.0301  memory: 7079  loss: 50.4291  decode.loss_cls: 0.6197  decode.loss_mask: 2.0532  decode.loss_dice: 2.4351  decode.d0.loss_cls: 0.7464  decode.d0.loss_mask: 2.0253  decode.d0.loss_dice: 2.3563  decode.d1.loss_cls: 0.5287  decode.d1.loss_mask: 2.0307  decode.d1.loss_dice: 2.3371  decode.d2.loss_cls: 0.5394  decode.d2.loss_mask: 2.0739  decode.d2.loss_dice: 2.3998  decode.d3.loss_cls: 0.6023  decode.d3.loss_mask: 2.0250  decode.d3.loss_dice: 2.3882  decode.d4.loss_cls: 0.5163  decode.d4.loss_mask: 2.1011  decode.d4.loss_dice: 2.4387  decode.d5.loss_cls: 0.5790  decode.d5.loss_mask: 2.0732  decode.d5.loss_dice: 2.4227  decode.d6.loss_cls: 0.6020  decode.d6.loss_mask: 2.0476  decode.d6.loss_dice: 2.4232  decode.d7.loss_cls: 0.5379  decode.d7.loss_mask: 2.0282  decode.d7.loss_dice: 2.4388  decode.d8.loss_cls: 0.5422  decode.d8.loss_mask: 2.0935  decode.d8.loss_dice: 2.4234
2025/03/29 12:25:56 - mmengine - INFO - Iter(train) [ 3550/20000]  base_lr: 8.3877e-05 lr: 8.3877e-05  eta: 2:31:00  time: 0.5889  data_time: 0.0281  memory: 7070  loss: 46.5262  decode.loss_cls: 0.5681  decode.loss_mask: 2.0575  decode.loss_dice: 2.0730  decode.d0.loss_cls: 0.6607  decode.d0.loss_mask: 2.0986  decode.d0.loss_dice: 2.0436  decode.d1.loss_cls: 0.4944  decode.d1.loss_mask: 2.0855  decode.d1.loss_dice: 2.0590  decode.d2.loss_cls: 0.4577  decode.d2.loss_mask: 2.1537  decode.d2.loss_dice: 2.0496  decode.d3.loss_cls: 0.5273  decode.d3.loss_mask: 2.0023  decode.d3.loss_dice: 2.0012  decode.d4.loss_cls: 0.4292  decode.d4.loss_mask: 2.1539  decode.d4.loss_dice: 2.0359  decode.d5.loss_cls: 0.3863  decode.d5.loss_mask: 2.1413  decode.d5.loss_dice: 2.1027  decode.d6.loss_cls: 0.4624  decode.d6.loss_mask: 2.0997  decode.d6.loss_dice: 2.0420  decode.d7.loss_cls: 0.4498  decode.d7.loss_mask: 2.1042  decode.d7.loss_dice: 2.0475  decode.d8.loss_cls: 0.5063  decode.d8.loss_mask: 2.1500  decode.d8.loss_dice: 2.0828
2025/03/29 12:26:23 - mmengine - INFO - Iter(train) [ 3600/20000]  base_lr: 8.3647e-05 lr: 8.3647e-05  eta: 2:30:30  time: 0.5366  data_time: 0.0219  memory: 7071  loss: 43.6789  decode.loss_cls: 0.4481  decode.loss_mask: 1.9505  decode.loss_dice: 1.9795  decode.d0.loss_cls: 0.6081  decode.d0.loss_mask: 1.8706  decode.d0.loss_dice: 2.0208  decode.d1.loss_cls: 0.5057  decode.d1.loss_mask: 1.8586  decode.d1.loss_dice: 1.9726  decode.d2.loss_cls: 0.4773  decode.d2.loss_mask: 1.8772  decode.d2.loss_dice: 1.9083  decode.d3.loss_cls: 0.4789  decode.d3.loss_mask: 1.8581  decode.d3.loss_dice: 1.9415  decode.d4.loss_cls: 0.4279  decode.d4.loss_mask: 1.9085  decode.d4.loss_dice: 1.9742  decode.d5.loss_cls: 0.4250  decode.d5.loss_mask: 1.9598  decode.d5.loss_dice: 1.9746  decode.d6.loss_cls: 0.5258  decode.d6.loss_mask: 1.9121  decode.d6.loss_dice: 1.9677  decode.d7.loss_cls: 0.4350  decode.d7.loss_mask: 1.9490  decode.d7.loss_dice: 2.0352  decode.d8.loss_cls: 0.5029  decode.d8.loss_mask: 1.9595  decode.d8.loss_dice: 1.9660
2025/03/29 12:26:50 - mmengine - INFO - Iter(train) [ 3650/20000]  base_lr: 8.3418e-05 lr: 8.3418e-05  eta: 2:30:02  time: 0.5271  data_time: 0.0208  memory: 7069  loss: 41.8396  decode.loss_cls: 0.4271  decode.loss_mask: 1.8800  decode.loss_dice: 1.7906  decode.d0.loss_cls: 0.5690  decode.d0.loss_mask: 2.0060  decode.d0.loss_dice: 1.9002  decode.d1.loss_cls: 0.3123  decode.d1.loss_mask: 1.9208  decode.d1.loss_dice: 1.8888  decode.d2.loss_cls: 0.4037  decode.d2.loss_mask: 1.9320  decode.d2.loss_dice: 1.8713  decode.d3.loss_cls: 0.4089  decode.d3.loss_mask: 1.9387  decode.d3.loss_dice: 1.8550  decode.d4.loss_cls: 0.3907  decode.d4.loss_mask: 1.8991  decode.d4.loss_dice: 1.8397  decode.d5.loss_cls: 0.4396  decode.d5.loss_mask: 1.8941  decode.d5.loss_dice: 1.8378  decode.d6.loss_cls: 0.4364  decode.d6.loss_mask: 1.8854  decode.d6.loss_dice: 1.8234  decode.d7.loss_cls: 0.4400  decode.d7.loss_mask: 1.8766  decode.d7.loss_dice: 1.8179  decode.d8.loss_cls: 0.4232  decode.d8.loss_mask: 1.8781  decode.d8.loss_dice: 1.8531
2025/03/29 12:27:17 - mmengine - INFO - Iter(train) [ 3700/20000]  base_lr: 8.3188e-05 lr: 8.3188e-05  eta: 2:29:29  time: 0.5310  data_time: 0.0205  memory: 7051  loss: 44.9997  decode.loss_cls: 0.4014  decode.loss_mask: 2.1531  decode.loss_dice: 2.0450  decode.d0.loss_cls: 0.5303  decode.d0.loss_mask: 2.0831  decode.d0.loss_dice: 2.0151  decode.d1.loss_cls: 0.4237  decode.d1.loss_mask: 2.1173  decode.d1.loss_dice: 2.0187  decode.d2.loss_cls: 0.3695  decode.d2.loss_mask: 2.0867  decode.d2.loss_dice: 1.9532  decode.d3.loss_cls: 0.3972  decode.d3.loss_mask: 2.0762  decode.d3.loss_dice: 1.9507  decode.d4.loss_cls: 0.3688  decode.d4.loss_mask: 2.0851  decode.d4.loss_dice: 1.9718  decode.d5.loss_cls: 0.3996  decode.d5.loss_mask: 2.1187  decode.d5.loss_dice: 1.9811  decode.d6.loss_cls: 0.3871  decode.d6.loss_mask: 2.0843  decode.d6.loss_dice: 1.9630  decode.d7.loss_cls: 0.3407  decode.d7.loss_mask: 2.1002  decode.d7.loss_dice: 1.9957  decode.d8.loss_cls: 0.3872  decode.d8.loss_mask: 2.1518  decode.d8.loss_dice: 2.0433
2025/03/29 12:27:44 - mmengine - INFO - Iter(train) [ 3750/20000]  base_lr: 8.2958e-05 lr: 8.2958e-05  eta: 2:28:59  time: 0.5318  data_time: 0.0205  memory: 7059  loss: 44.0360  decode.loss_cls: 0.3399  decode.loss_mask: 1.9517  decode.loss_dice: 2.1020  decode.d0.loss_cls: 0.5526  decode.d0.loss_mask: 1.9624  decode.d0.loss_dice: 2.0729  decode.d1.loss_cls: 0.3026  decode.d1.loss_mask: 1.9240  decode.d1.loss_dice: 2.1324  decode.d2.loss_cls: 0.3108  decode.d2.loss_mask: 1.9613  decode.d2.loss_dice: 2.1209  decode.d3.loss_cls: 0.3355  decode.d3.loss_mask: 1.9250  decode.d3.loss_dice: 2.0744  decode.d4.loss_cls: 0.3305  decode.d4.loss_mask: 1.9314  decode.d4.loss_dice: 2.0829  decode.d5.loss_cls: 0.2872  decode.d5.loss_mask: 1.9871  decode.d5.loss_dice: 2.1292  decode.d6.loss_cls: 0.3171  decode.d6.loss_mask: 1.9720  decode.d6.loss_dice: 2.1375  decode.d7.loss_cls: 0.3476  decode.d7.loss_mask: 1.9345  decode.d7.loss_dice: 2.1263  decode.d8.loss_cls: 0.3227  decode.d8.loss_mask: 1.9566  decode.d8.loss_dice: 2.1050
2025/03/29 12:28:11 - mmengine - INFO - Iter(train) [ 3800/20000]  base_lr: 8.2729e-05 lr: 8.2729e-05  eta: 2:28:33  time: 0.5566  data_time: 0.0241  memory: 7053  loss: 45.9922  decode.loss_cls: 0.4666  decode.loss_mask: 1.9668  decode.loss_dice: 2.1191  decode.d0.loss_cls: 0.6899  decode.d0.loss_mask: 1.9458  decode.d0.loss_dice: 2.0450  decode.d1.loss_cls: 0.5012  decode.d1.loss_mask: 1.9934  decode.d1.loss_dice: 2.1148  decode.d2.loss_cls: 0.4431  decode.d2.loss_mask: 1.9733  decode.d2.loss_dice: 2.0959  decode.d3.loss_cls: 0.4787  decode.d3.loss_mask: 1.9975  decode.d3.loss_dice: 2.1042  decode.d4.loss_cls: 0.5103  decode.d4.loss_mask: 1.9853  decode.d4.loss_dice: 2.1010  decode.d5.loss_cls: 0.5317  decode.d5.loss_mask: 1.9930  decode.d5.loss_dice: 2.0821  decode.d6.loss_cls: 0.4680  decode.d6.loss_mask: 2.0556  decode.d6.loss_dice: 2.1049  decode.d7.loss_cls: 0.5199  decode.d7.loss_mask: 2.0091  decode.d7.loss_dice: 2.1191  decode.d8.loss_cls: 0.4604  decode.d8.loss_mask: 1.9733  decode.d8.loss_dice: 2.1432
2025/03/29 12:28:39 - mmengine - INFO - Iter(train) [ 3850/20000]  base_lr: 8.2499e-05 lr: 8.2499e-05  eta: 2:28:04  time: 0.5631  data_time: 0.0259  memory: 7073  loss: 46.4608  decode.loss_cls: 0.6121  decode.loss_mask: 2.0539  decode.loss_dice: 2.0055  decode.d0.loss_cls: 0.7630  decode.d0.loss_mask: 2.0237  decode.d0.loss_dice: 2.0097  decode.d1.loss_cls: 0.5775  decode.d1.loss_mask: 2.0078  decode.d1.loss_dice: 1.9819  decode.d2.loss_cls: 0.4932  decode.d2.loss_mask: 2.0317  decode.d2.loss_dice: 1.9376  decode.d3.loss_cls: 0.5040  decode.d3.loss_mask: 2.0908  decode.d3.loss_dice: 1.9736  decode.d4.loss_cls: 0.5212  decode.d4.loss_mask: 2.0747  decode.d4.loss_dice: 1.9930  decode.d5.loss_cls: 0.5535  decode.d5.loss_mask: 2.1167  decode.d5.loss_dice: 2.0836  decode.d6.loss_cls: 0.4760  decode.d6.loss_mask: 2.1601  decode.d6.loss_dice: 2.0856  decode.d7.loss_cls: 0.5780  decode.d7.loss_mask: 2.0406  decode.d7.loss_dice: 2.0486  decode.d8.loss_cls: 0.6130  decode.d8.loss_mask: 2.0295  decode.d8.loss_dice: 2.0208
2025/03/29 12:29:06 - mmengine - INFO - Iter(train) [ 3900/20000]  base_lr: 8.2269e-05 lr: 8.2269e-05  eta: 2:27:36  time: 0.5635  data_time: 0.0256  memory: 7069  loss: 39.2578  decode.loss_cls: 0.4343  decode.loss_mask: 1.9172  decode.loss_dice: 1.6569  decode.d0.loss_cls: 0.6700  decode.d0.loss_mask: 1.8458  decode.d0.loss_dice: 1.5620  decode.d1.loss_cls: 0.3519  decode.d1.loss_mask: 1.8738  decode.d1.loss_dice: 1.6138  decode.d2.loss_cls: 0.3879  decode.d2.loss_mask: 1.8722  decode.d2.loss_dice: 1.5994  decode.d3.loss_cls: 0.3822  decode.d3.loss_mask: 1.8469  decode.d3.loss_dice: 1.6027  decode.d4.loss_cls: 0.4324  decode.d4.loss_mask: 1.9147  decode.d4.loss_dice: 1.6447  decode.d5.loss_cls: 0.3788  decode.d5.loss_mask: 1.8927  decode.d5.loss_dice: 1.6024  decode.d6.loss_cls: 0.4189  decode.d6.loss_mask: 1.8743  decode.d6.loss_dice: 1.6233  decode.d7.loss_cls: 0.4125  decode.d7.loss_mask: 1.8825  decode.d7.loss_dice: 1.6271  decode.d8.loss_cls: 0.4177  decode.d8.loss_mask: 1.9264  decode.d8.loss_dice: 1.5922
2025/03/29 12:29:33 - mmengine - INFO - Iter(train) [ 3950/20000]  base_lr: 8.2039e-05 lr: 8.2039e-05  eta: 2:27:08  time: 0.5383  data_time: 0.0227  memory: 7053  loss: 47.6311  decode.loss_cls: 0.5216  decode.loss_mask: 2.2234  decode.loss_dice: 1.9469  decode.d0.loss_cls: 0.6811  decode.d0.loss_mask: 2.2582  decode.d0.loss_dice: 1.9419  decode.d1.loss_cls: 0.4790  decode.d1.loss_mask: 2.2326  decode.d1.loss_dice: 1.9878  decode.d2.loss_cls: 0.5443  decode.d2.loss_mask: 2.2622  decode.d2.loss_dice: 1.9777  decode.d3.loss_cls: 0.6638  decode.d3.loss_mask: 2.1867  decode.d3.loss_dice: 1.9404  decode.d4.loss_cls: 0.6116  decode.d4.loss_mask: 2.1991  decode.d4.loss_dice: 1.9447  decode.d5.loss_cls: 0.5635  decode.d5.loss_mask: 2.2003  decode.d5.loss_dice: 2.0110  decode.d6.loss_cls: 0.5430  decode.d6.loss_mask: 2.2650  decode.d6.loss_dice: 1.9801  decode.d7.loss_cls: 0.5313  decode.d7.loss_mask: 2.2113  decode.d7.loss_dice: 2.0078  decode.d8.loss_cls: 0.4862  decode.d8.loss_mask: 2.2111  decode.d8.loss_dice: 2.0172
2025/03/29 12:30:01 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 12:30:01 - mmengine - INFO - Iter(train) [ 4000/20000]  base_lr: 8.1809e-05 lr: 8.1809e-05  eta: 2:26:40  time: 0.5583  data_time: 0.0240  memory: 7074  loss: 43.8013  decode.loss_cls: 0.6669  decode.loss_mask: 1.9147  decode.loss_dice: 1.9525  decode.d0.loss_cls: 0.6000  decode.d0.loss_mask: 1.8796  decode.d0.loss_dice: 1.9191  decode.d1.loss_cls: 0.4773  decode.d1.loss_mask: 1.9013  decode.d1.loss_dice: 1.8834  decode.d2.loss_cls: 0.4848  decode.d2.loss_mask: 1.9545  decode.d2.loss_dice: 1.9155  decode.d3.loss_cls: 0.6463  decode.d3.loss_mask: 1.9505  decode.d3.loss_dice: 1.8361  decode.d4.loss_cls: 0.5863  decode.d4.loss_mask: 1.9666  decode.d4.loss_dice: 1.8794  decode.d5.loss_cls: 0.6371  decode.d5.loss_mask: 1.8763  decode.d5.loss_dice: 1.8458  decode.d6.loss_cls: 0.6814  decode.d6.loss_mask: 1.8538  decode.d6.loss_dice: 1.8061  decode.d7.loss_cls: 0.5851  decode.d7.loss_mask: 1.9194  decode.d7.loss_dice: 1.8287  decode.d8.loss_cls: 0.4928  decode.d8.loss_mask: 1.9468  decode.d8.loss_dice: 1.9129
2025/03/29 12:30:01 - mmengine - INFO - Saving checkpoint at 4000 iterations
2025/03/29 12:30:07 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:05:58  time: 0.0909  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:11 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:53  time: 0.0925  data_time: 0.0020  memory: 3086  
2025/03/29 12:30:16 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:50  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:20 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:46  time: 0.0938  data_time: 0.0021  memory: 3086  
2025/03/29 12:30:25 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:42  time: 0.0939  data_time: 0.0019  memory: 3086  
2025/03/29 12:30:30 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:37  time: 0.0913  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:34 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:32  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:39 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:27  time: 0.0909  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:43 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:22  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:48 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:17  time: 0.0942  data_time: 0.0021  memory: 3086  
2025/03/29 12:30:52 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:13  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:30:57 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:08  time: 0.0909  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:02 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:03  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:06 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:59  time: 0.0928  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:11 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:54  time: 0.0912  data_time: 0.0017  memory: 3086  
2025/03/29 12:31:15 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:50  time: 0.0913  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:20 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:45  time: 0.0916  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:24 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:40  time: 0.0933  data_time: 0.0019  memory: 3086  
2025/03/29 12:31:29 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:36  time: 0.0911  data_time: 0.0017  memory: 3086  
2025/03/29 12:31:34 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:31  time: 0.0914  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:38 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:27  time: 0.0911  data_time: 0.0017  memory: 3086  
2025/03/29 12:31:43 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:22  time: 0.0912  data_time: 0.0017  memory: 3086  
2025/03/29 12:31:47 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:17  time: 0.0913  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:52 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:13  time: 0.0931  data_time: 0.0018  memory: 3086  
2025/03/29 12:31:56 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:08  time: 0.0911  data_time: 0.0019  memory: 3086  
2025/03/29 12:32:01 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:04  time: 0.0918  data_time: 0.0019  memory: 3086  
2025/03/29 12:32:06 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:59  time: 0.0913  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:10 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:55  time: 0.0914  data_time: 0.0019  memory: 3086  
2025/03/29 12:32:15 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0914  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:19 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:45  time: 0.0939  data_time: 0.0021  memory: 3086  
2025/03/29 12:32:24 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0911  data_time: 0.0019  memory: 3086  
2025/03/29 12:32:29 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:36  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:33 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0913  data_time: 0.0019  memory: 3086  
2025/03/29 12:32:38 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:27  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:42 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:23  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:47 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:51 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:13  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:32:56 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0911  data_time: 0.0019  memory: 3086  
2025/03/29 12:33:01 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:04  time: 0.0911  data_time: 0.0019  memory: 3086  
2025/03/29 12:33:05 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:10 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:14 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:50  time: 0.0909  data_time: 0.0017  memory: 3086  
2025/03/29 12:33:19 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:23 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:41  time: 0.0910  data_time: 0.0019  memory: 3086  
2025/03/29 12:33:28 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:32 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:37 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:27  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:42 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0910  data_time: 0.0017  memory: 3086  
2025/03/29 12:33:46 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:18  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:33:51 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0910  data_time: 0.0017  memory: 3086  
2025/03/29 12:33:55 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0909  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:00 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:04 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0931  data_time: 0.0019  memory: 3086  
2025/03/29 12:34:09 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:55  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:14 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0911  data_time: 0.0017  memory: 3086  
2025/03/29 12:34:18 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:46  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:23 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:27 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:32 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:36 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:41 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:23  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:46 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0926  data_time: 0.0019  memory: 3086  
2025/03/29 12:34:50 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:14  time: 0.0913  data_time: 0.0017  memory: 3086  
2025/03/29 12:34:55 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:34:59 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:04 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:08 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:13 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:51  time: 0.0914  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:18 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0912  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:22 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0913  data_time: 0.0017  memory: 3086  
2025/03/29 12:35:27 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0915  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:31 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0911  data_time: 0.0019  memory: 3086  
2025/03/29 12:35:36 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0911  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:40 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0910  data_time: 0.0018  memory: 3086  
2025/03/29 12:35:45 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:19  time: 0.0910  data_time: 0.0016  memory: 3086  
2025/03/29 12:35:50 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0911  data_time: 0.0017  memory: 3086  
2025/03/29 12:35:54 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0909  data_time: 0.0017  memory: 3086  
2025/03/29 12:35:59 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0923  data_time: 0.0019  memory: 3086  
2025/03/29 12:36:03 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0909  data_time: 0.0017  memory: 3086  
2025/03/29 12:36:05 - mmengine - INFO - per class results:
2025/03/29 12:36:05 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 48.29 | 73.92 |
|   building   | 44.75 |  59.3 |
|     road     | 39.23 |  42.1 |
|    water     | 56.61 |  72.6 |
|    barren    |  9.08 | 26.59 |
|    forest    | 30.06 | 46.48 |
| agricultural | 42.62 | 45.62 |
+--------------+-------+-------+
2025/03/29 12:36:05 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 60.6600  mIoU: 38.6600  mAcc: 52.3700  data_time: 0.0018  time: 0.0913
2025/03/29 12:36:05 - mmengine - INFO - The previous best checkpoint /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0/best_mIoU_iter_2000.pth is removed
2025/03/29 12:36:06 - mmengine - INFO - The best checkpoint with 38.6600 mIoU at 4000 iter is saved to best_mIoU_iter_4000.pth.
2025/03/29 12:37:04 - mmengine - INFO - Iter(train) [ 4050/20000]  base_lr: 8.1579e-05 lr: 8.1579e-05  eta: 2:28:16  time: 1.1490  data_time: 0.0232  memory: 11207  loss: 69.5353  decode.loss_cls: 0.5714  decode.loss_mask: 2.1063  decode.loss_dice: 1.7067  decode.d0.loss_cls: 0.6452  decode.d0.loss_mask: 2.0328  decode.d0.loss_dice: 1.7182  decode.d1.loss_cls: 0.3958  decode.d1.loss_mask: 1.9888  decode.d1.loss_dice: 1.7097  decode.d2.loss_cls: 0.5629  decode.d2.loss_mask: 1.9734  decode.d2.loss_dice: 1.6688  decode.d3.loss_cls: 0.4510  decode.d3.loss_mask: 1.9836  decode.d3.loss_dice: 1.6653  decode.d4.loss_cls: 0.5719  decode.d4.loss_mask: 1.9514  decode.d4.loss_dice: 1.6673  decode.d5.loss_cls: 0.5652  decode.d5.loss_mask: 2.0060  decode.d5.loss_dice: 1.7039  decode.d6.loss_cls: 0.5936  decode.d6.loss_mask: 1.9575  decode.d6.loss_dice: 1.6847  decode.d7.loss_cls: 0.6020  decode.d7.loss_mask: 1.9844  decode.d7.loss_dice: 1.6541  decode.d8.loss_cls: 0.5810  decode.d8.loss_mask: 1.9813  decode.d8.loss_dice: 1.6851  mix_decode.loss_cls: 0.5276  mix_decode.loss_mask: 1.0646  mix_decode.loss_dice: 1.1795  mix_decode.d0.loss_cls: 0.5421  mix_decode.d0.loss_mask: 1.0595  mix_decode.d0.loss_dice: 1.2188  mix_decode.d1.loss_cls: 0.4571  mix_decode.d1.loss_mask: 1.0412  mix_decode.d1.loss_dice: 1.1910  mix_decode.d2.loss_cls: 0.4515  mix_decode.d2.loss_mask: 0.9795  mix_decode.d2.loss_dice: 1.1669  mix_decode.d3.loss_cls: 0.5062  mix_decode.d3.loss_mask: 1.0178  mix_decode.d3.loss_dice: 1.1330  mix_decode.d4.loss_cls: 0.5285  mix_decode.d4.loss_mask: 1.0144  mix_decode.d4.loss_dice: 1.1401  mix_decode.d5.loss_cls: 0.5364  mix_decode.d5.loss_mask: 0.9941  mix_decode.d5.loss_dice: 1.1625  mix_decode.d6.loss_cls: 0.5264  mix_decode.d6.loss_mask: 1.0157  mix_decode.d6.loss_dice: 1.1849  mix_decode.d7.loss_cls: 0.5291  mix_decode.d7.loss_mask: 1.0760  mix_decode.d7.loss_dice: 1.1823  mix_decode.d8.loss_cls: 0.5302  mix_decode.d8.loss_mask: 1.0446  mix_decode.d8.loss_dice: 1.1645
2025/03/29 12:38:01 - mmengine - INFO - Iter(train) [ 4100/20000]  base_lr: 8.1349e-05 lr: 8.1349e-05  eta: 2:29:42  time: 1.1504  data_time: 0.0230  memory: 11202  loss: 73.8112  decode.loss_cls: 0.4856  decode.loss_mask: 2.1006  decode.loss_dice: 2.1296  decode.d0.loss_cls: 0.5727  decode.d0.loss_mask: 2.0750  decode.d0.loss_dice: 2.1538  decode.d1.loss_cls: 0.3774  decode.d1.loss_mask: 2.0869  decode.d1.loss_dice: 2.1104  decode.d2.loss_cls: 0.4746  decode.d2.loss_mask: 2.0967  decode.d2.loss_dice: 2.0973  decode.d3.loss_cls: 0.4663  decode.d3.loss_mask: 2.1118  decode.d3.loss_dice: 2.0901  decode.d4.loss_cls: 0.4856  decode.d4.loss_mask: 2.0784  decode.d4.loss_dice: 2.1428  decode.d5.loss_cls: 0.4176  decode.d5.loss_mask: 2.0782  decode.d5.loss_dice: 2.1582  decode.d6.loss_cls: 0.4739  decode.d6.loss_mask: 2.0644  decode.d6.loss_dice: 2.1393  decode.d7.loss_cls: 0.4412  decode.d7.loss_mask: 2.0710  decode.d7.loss_dice: 2.1431  decode.d8.loss_cls: 0.5172  decode.d8.loss_mask: 2.0597  decode.d8.loss_dice: 2.1334  mix_decode.loss_cls: 0.5428  mix_decode.loss_mask: 1.0558  mix_decode.loss_dice: 1.1226  mix_decode.d0.loss_cls: 0.5449  mix_decode.d0.loss_mask: 1.0296  mix_decode.d0.loss_dice: 1.1749  mix_decode.d1.loss_cls: 0.5374  mix_decode.d1.loss_mask: 0.9722  mix_decode.d1.loss_dice: 1.1021  mix_decode.d2.loss_cls: 0.6004  mix_decode.d2.loss_mask: 0.9602  mix_decode.d2.loss_dice: 1.0769  mix_decode.d3.loss_cls: 0.5279  mix_decode.d3.loss_mask: 1.0142  mix_decode.d3.loss_dice: 1.0651  mix_decode.d4.loss_cls: 0.5523  mix_decode.d4.loss_mask: 1.0125  mix_decode.d4.loss_dice: 1.1270  mix_decode.d5.loss_cls: 0.6418  mix_decode.d5.loss_mask: 0.9657  mix_decode.d5.loss_dice: 1.1094  mix_decode.d6.loss_cls: 0.5707  mix_decode.d6.loss_mask: 0.9948  mix_decode.d6.loss_dice: 1.1106  mix_decode.d7.loss_cls: 0.6598  mix_decode.d7.loss_mask: 0.9958  mix_decode.d7.loss_dice: 1.1228  mix_decode.d8.loss_cls: 0.6391  mix_decode.d8.loss_mask: 0.9919  mix_decode.d8.loss_dice: 1.1572
2025/03/29 12:38:58 - mmengine - INFO - Iter(train) [ 4150/20000]  base_lr: 8.1118e-05 lr: 8.1118e-05  eta: 2:31:05  time: 1.1462  data_time: 0.0221  memory: 11213  loss: 66.4276  decode.loss_cls: 0.5581  decode.loss_mask: 1.8917  decode.loss_dice: 2.0293  decode.d0.loss_cls: 0.6778  decode.d0.loss_mask: 1.7587  decode.d0.loss_dice: 1.9851  decode.d1.loss_cls: 0.4610  decode.d1.loss_mask: 1.8402  decode.d1.loss_dice: 1.9717  decode.d2.loss_cls: 0.6173  decode.d2.loss_mask: 1.8035  decode.d2.loss_dice: 1.9036  decode.d3.loss_cls: 0.5533  decode.d3.loss_mask: 1.8455  decode.d3.loss_dice: 1.9509  decode.d4.loss_cls: 0.5854  decode.d4.loss_mask: 1.7773  decode.d4.loss_dice: 1.9365  decode.d5.loss_cls: 0.5930  decode.d5.loss_mask: 1.7771  decode.d5.loss_dice: 1.9456  decode.d6.loss_cls: 0.6190  decode.d6.loss_mask: 1.7357  decode.d6.loss_dice: 1.9482  decode.d7.loss_cls: 0.6096  decode.d7.loss_mask: 1.7312  decode.d7.loss_dice: 1.9211  decode.d8.loss_cls: 0.5525  decode.d8.loss_mask: 1.8636  decode.d8.loss_dice: 1.9914  mix_decode.loss_cls: 0.4830  mix_decode.loss_mask: 0.7842  mix_decode.loss_dice: 1.0325  mix_decode.d0.loss_cls: 0.4970  mix_decode.d0.loss_mask: 0.7882  mix_decode.d0.loss_dice: 1.0700  mix_decode.d1.loss_cls: 0.4213  mix_decode.d1.loss_mask: 0.8027  mix_decode.d1.loss_dice: 1.0324  mix_decode.d2.loss_cls: 0.4468  mix_decode.d2.loss_mask: 0.8023  mix_decode.d2.loss_dice: 0.9983  mix_decode.d3.loss_cls: 0.4143  mix_decode.d3.loss_mask: 0.7970  mix_decode.d3.loss_dice: 1.0253  mix_decode.d4.loss_cls: 0.4771  mix_decode.d4.loss_mask: 0.7796  mix_decode.d4.loss_dice: 1.0369  mix_decode.d5.loss_cls: 0.4888  mix_decode.d5.loss_mask: 0.7688  mix_decode.d5.loss_dice: 1.0387  mix_decode.d6.loss_cls: 0.5654  mix_decode.d6.loss_mask: 0.7613  mix_decode.d6.loss_dice: 1.0237  mix_decode.d7.loss_cls: 0.5358  mix_decode.d7.loss_mask: 0.7764  mix_decode.d7.loss_dice: 1.0188  mix_decode.d8.loss_cls: 0.4761  mix_decode.d8.loss_mask: 0.8070  mix_decode.d8.loss_dice: 1.0430
2025/03/29 12:39:55 - mmengine - INFO - Iter(train) [ 4200/20000]  base_lr: 8.0888e-05 lr: 8.0888e-05  eta: 2:32:24  time: 1.1427  data_time: 0.0225  memory: 11227  loss: 74.3923  decode.loss_cls: 0.3535  decode.loss_mask: 2.4010  decode.loss_dice: 2.1556  decode.d0.loss_cls: 0.5495  decode.d0.loss_mask: 2.2675  decode.d0.loss_dice: 2.0809  decode.d1.loss_cls: 0.2966  decode.d1.loss_mask: 2.3215  decode.d1.loss_dice: 2.1192  decode.d2.loss_cls: 0.3596  decode.d2.loss_mask: 2.3343  decode.d2.loss_dice: 2.1212  decode.d3.loss_cls: 0.3337  decode.d3.loss_mask: 2.3222  decode.d3.loss_dice: 2.0887  decode.d4.loss_cls: 0.3452  decode.d4.loss_mask: 2.4077  decode.d4.loss_dice: 2.1388  decode.d5.loss_cls: 0.4048  decode.d5.loss_mask: 2.2962  decode.d5.loss_dice: 2.0521  decode.d6.loss_cls: 0.5132  decode.d6.loss_mask: 2.2558  decode.d6.loss_dice: 1.9929  decode.d7.loss_cls: 0.3466  decode.d7.loss_mask: 2.3254  decode.d7.loss_dice: 2.1392  decode.d8.loss_cls: 0.3162  decode.d8.loss_mask: 2.3701  decode.d8.loss_dice: 2.1494  mix_decode.loss_cls: 0.5275  mix_decode.loss_mask: 0.8738  mix_decode.loss_dice: 1.2469  mix_decode.d0.loss_cls: 0.5428  mix_decode.d0.loss_mask: 0.8327  mix_decode.d0.loss_dice: 1.2522  mix_decode.d1.loss_cls: 0.4968  mix_decode.d1.loss_mask: 0.8312  mix_decode.d1.loss_dice: 1.2204  mix_decode.d2.loss_cls: 0.5415  mix_decode.d2.loss_mask: 0.8517  mix_decode.d2.loss_dice: 1.1984  mix_decode.d3.loss_cls: 0.5325  mix_decode.d3.loss_mask: 0.8497  mix_decode.d3.loss_dice: 1.1889  mix_decode.d4.loss_cls: 0.5361  mix_decode.d4.loss_mask: 0.8369  mix_decode.d4.loss_dice: 1.2106  mix_decode.d5.loss_cls: 0.6322  mix_decode.d5.loss_mask: 0.8896  mix_decode.d5.loss_dice: 1.1959  mix_decode.d6.loss_cls: 0.6645  mix_decode.d6.loss_mask: 0.8362  mix_decode.d6.loss_dice: 1.1259  mix_decode.d7.loss_cls: 0.5492  mix_decode.d7.loss_mask: 0.9053  mix_decode.d7.loss_dice: 1.2117  mix_decode.d8.loss_cls: 0.5771  mix_decode.d8.loss_mask: 0.8759  mix_decode.d8.loss_dice: 1.1992
2025/03/29 12:40:53 - mmengine - INFO - Iter(train) [ 4250/20000]  base_lr: 8.0658e-05 lr: 8.0658e-05  eta: 2:33:40  time: 1.1449  data_time: 0.0221  memory: 11220  loss: 64.0566  decode.loss_cls: 0.4273  decode.loss_mask: 2.0037  decode.loss_dice: 1.8088  decode.d0.loss_cls: 0.5781  decode.d0.loss_mask: 2.0691  decode.d0.loss_dice: 1.9007  decode.d1.loss_cls: 0.4960  decode.d1.loss_mask: 1.9741  decode.d1.loss_dice: 1.8311  decode.d2.loss_cls: 0.4480  decode.d2.loss_mask: 2.0079  decode.d2.loss_dice: 1.8000  decode.d3.loss_cls: 0.4505  decode.d3.loss_mask: 1.9742  decode.d3.loss_dice: 1.8309  decode.d4.loss_cls: 0.4524  decode.d4.loss_mask: 1.9788  decode.d4.loss_dice: 1.8119  decode.d5.loss_cls: 0.4948  decode.d5.loss_mask: 1.9643  decode.d5.loss_dice: 1.7890  decode.d6.loss_cls: 0.4946  decode.d6.loss_mask: 2.0035  decode.d6.loss_dice: 1.7953  decode.d7.loss_cls: 0.4551  decode.d7.loss_mask: 2.0067  decode.d7.loss_dice: 1.8340  decode.d8.loss_cls: 0.4206  decode.d8.loss_mask: 1.9846  decode.d8.loss_dice: 1.8393  mix_decode.loss_cls: 0.3364  mix_decode.loss_mask: 0.7818  mix_decode.loss_dice: 0.9580  mix_decode.d0.loss_cls: 0.4352  mix_decode.d0.loss_mask: 0.7969  mix_decode.d0.loss_dice: 1.0085  mix_decode.d1.loss_cls: 0.3147  mix_decode.d1.loss_mask: 0.7807  mix_decode.d1.loss_dice: 0.9651  mix_decode.d2.loss_cls: 0.3549  mix_decode.d2.loss_mask: 0.7687  mix_decode.d2.loss_dice: 0.9403  mix_decode.d3.loss_cls: 0.3277  mix_decode.d3.loss_mask: 0.7812  mix_decode.d3.loss_dice: 0.9629  mix_decode.d4.loss_cls: 0.3824  mix_decode.d4.loss_mask: 0.7532  mix_decode.d4.loss_dice: 0.9496  mix_decode.d5.loss_cls: 0.3739  mix_decode.d5.loss_mask: 0.8274  mix_decode.d5.loss_dice: 0.9871  mix_decode.d6.loss_cls: 0.3854  mix_decode.d6.loss_mask: 0.7976  mix_decode.d6.loss_dice: 0.9757  mix_decode.d7.loss_cls: 0.3696  mix_decode.d7.loss_mask: 0.7837  mix_decode.d7.loss_dice: 0.9512  mix_decode.d8.loss_cls: 0.3370  mix_decode.d8.loss_mask: 0.7883  mix_decode.d8.loss_dice: 0.9562
2025/03/29 12:41:50 - mmengine - INFO - Iter(train) [ 4300/20000]  base_lr: 8.0427e-05 lr: 8.0427e-05  eta: 2:34:53  time: 1.1490  data_time: 0.0234  memory: 11217  loss: 70.7329  decode.loss_cls: 0.7236  decode.loss_mask: 1.8794  decode.loss_dice: 2.0396  decode.d0.loss_cls: 0.6976  decode.d0.loss_mask: 1.9415  decode.d0.loss_dice: 2.1333  decode.d1.loss_cls: 0.6314  decode.d1.loss_mask: 1.8788  decode.d1.loss_dice: 2.0670  decode.d2.loss_cls: 0.5992  decode.d2.loss_mask: 1.9113  decode.d2.loss_dice: 2.0161  decode.d3.loss_cls: 0.6170  decode.d3.loss_mask: 1.8753  decode.d3.loss_dice: 2.0218  decode.d4.loss_cls: 0.6647  decode.d4.loss_mask: 1.9274  decode.d4.loss_dice: 1.9670  decode.d5.loss_cls: 0.6761  decode.d5.loss_mask: 1.9444  decode.d5.loss_dice: 2.0190  decode.d6.loss_cls: 0.7288  decode.d6.loss_mask: 1.8785  decode.d6.loss_dice: 2.0621  decode.d7.loss_cls: 0.6579  decode.d7.loss_mask: 1.9723  decode.d7.loss_dice: 2.0705  decode.d8.loss_cls: 0.6909  decode.d8.loss_mask: 1.9157  decode.d8.loss_dice: 2.0045  mix_decode.loss_cls: 0.5299  mix_decode.loss_mask: 0.7270  mix_decode.loss_dice: 1.1747  mix_decode.d0.loss_cls: 0.5325  mix_decode.d0.loss_mask: 0.7824  mix_decode.d0.loss_dice: 1.2631  mix_decode.d1.loss_cls: 0.4602  mix_decode.d1.loss_mask: 0.7310  mix_decode.d1.loss_dice: 1.1707  mix_decode.d2.loss_cls: 0.5483  mix_decode.d2.loss_mask: 0.7177  mix_decode.d2.loss_dice: 1.1428  mix_decode.d3.loss_cls: 0.5171  mix_decode.d3.loss_mask: 0.7318  mix_decode.d3.loss_dice: 1.1691  mix_decode.d4.loss_cls: 0.5847  mix_decode.d4.loss_mask: 0.7251  mix_decode.d4.loss_dice: 1.1447  mix_decode.d5.loss_cls: 0.6177  mix_decode.d5.loss_mask: 0.7145  mix_decode.d5.loss_dice: 1.1444  mix_decode.d6.loss_cls: 0.6057  mix_decode.d6.loss_mask: 0.7125  mix_decode.d6.loss_dice: 1.1812  mix_decode.d7.loss_cls: 0.5883  mix_decode.d7.loss_mask: 0.7245  mix_decode.d7.loss_dice: 1.1563  mix_decode.d8.loss_cls: 0.5539  mix_decode.d8.loss_mask: 0.7266  mix_decode.d8.loss_dice: 1.1416
2025/03/29 12:42:47 - mmengine - INFO - Iter(train) [ 4350/20000]  base_lr: 8.0197e-05 lr: 8.0197e-05  eta: 2:36:03  time: 1.1434  data_time: 0.0222  memory: 11221  loss: 68.7786  decode.loss_cls: 0.6238  decode.loss_mask: 2.0260  decode.loss_dice: 2.0207  decode.d0.loss_cls: 0.6919  decode.d0.loss_mask: 1.9640  decode.d0.loss_dice: 1.9921  decode.d1.loss_cls: 0.5346  decode.d1.loss_mask: 2.0238  decode.d1.loss_dice: 2.0212  decode.d2.loss_cls: 0.4892  decode.d2.loss_mask: 2.0267  decode.d2.loss_dice: 2.0187  decode.d3.loss_cls: 0.4445  decode.d3.loss_mask: 2.0706  decode.d3.loss_dice: 2.0283  decode.d4.loss_cls: 0.5225  decode.d4.loss_mask: 2.0098  decode.d4.loss_dice: 1.9887  decode.d5.loss_cls: 0.5377  decode.d5.loss_mask: 2.0331  decode.d5.loss_dice: 1.9999  decode.d6.loss_cls: 0.5066  decode.d6.loss_mask: 1.9877  decode.d6.loss_dice: 2.0216  decode.d7.loss_cls: 0.5249  decode.d7.loss_mask: 1.9984  decode.d7.loss_dice: 2.0128  decode.d8.loss_cls: 0.5508  decode.d8.loss_mask: 2.0589  decode.d8.loss_dice: 2.0093  mix_decode.loss_cls: 0.4584  mix_decode.loss_mask: 0.8621  mix_decode.loss_dice: 1.0623  mix_decode.d0.loss_cls: 0.4629  mix_decode.d0.loss_mask: 0.8083  mix_decode.d0.loss_dice: 1.0953  mix_decode.d1.loss_cls: 0.3889  mix_decode.d1.loss_mask: 0.8050  mix_decode.d1.loss_dice: 1.0401  mix_decode.d2.loss_cls: 0.4560  mix_decode.d2.loss_mask: 0.7760  mix_decode.d2.loss_dice: 1.0416  mix_decode.d3.loss_cls: 0.4038  mix_decode.d3.loss_mask: 0.7786  mix_decode.d3.loss_dice: 1.0303  mix_decode.d4.loss_cls: 0.4381  mix_decode.d4.loss_mask: 0.7756  mix_decode.d4.loss_dice: 1.0298  mix_decode.d5.loss_cls: 0.4863  mix_decode.d5.loss_mask: 0.7808  mix_decode.d5.loss_dice: 1.0397  mix_decode.d6.loss_cls: 0.5247  mix_decode.d6.loss_mask: 0.7620  mix_decode.d6.loss_dice: 1.0342  mix_decode.d7.loss_cls: 0.4175  mix_decode.d7.loss_mask: 0.8172  mix_decode.d7.loss_dice: 1.1021  mix_decode.d8.loss_cls: 0.4866  mix_decode.d8.loss_mask: 0.8205  mix_decode.d8.loss_dice: 1.0549
2025/03/29 12:43:45 - mmengine - INFO - Iter(train) [ 4400/20000]  base_lr: 7.9966e-05 lr: 7.9966e-05  eta: 2:37:11  time: 1.1484  data_time: 0.0220  memory: 11221  loss: 67.8900  decode.loss_cls: 0.5307  decode.loss_mask: 2.1707  decode.loss_dice: 2.1114  decode.d0.loss_cls: 0.5495  decode.d0.loss_mask: 1.9355  decode.d0.loss_dice: 2.0526  decode.d1.loss_cls: 0.3535  decode.d1.loss_mask: 1.9622  decode.d1.loss_dice: 2.0098  decode.d2.loss_cls: 0.4196  decode.d2.loss_mask: 1.9426  decode.d2.loss_dice: 1.9949  decode.d3.loss_cls: 0.5250  decode.d3.loss_mask: 1.8834  decode.d3.loss_dice: 1.9316  decode.d4.loss_cls: 0.5073  decode.d4.loss_mask: 1.9160  decode.d4.loss_dice: 1.9393  decode.d5.loss_cls: 0.5178  decode.d5.loss_mask: 1.9438  decode.d5.loss_dice: 1.9525  decode.d6.loss_cls: 0.5840  decode.d6.loss_mask: 1.8473  decode.d6.loss_dice: 1.9658  decode.d7.loss_cls: 0.5195  decode.d7.loss_mask: 1.9090  decode.d7.loss_dice: 2.0008  decode.d8.loss_cls: 0.4777  decode.d8.loss_mask: 1.9748  decode.d8.loss_dice: 1.9820  mix_decode.loss_cls: 0.3792  mix_decode.loss_mask: 0.9585  mix_decode.loss_dice: 1.0619  mix_decode.d0.loss_cls: 0.4881  mix_decode.d0.loss_mask: 0.8456  mix_decode.d0.loss_dice: 1.0029  mix_decode.d1.loss_cls: 0.4160  mix_decode.d1.loss_mask: 0.9152  mix_decode.d1.loss_dice: 1.0125  mix_decode.d2.loss_cls: 0.4247  mix_decode.d2.loss_mask: 0.9287  mix_decode.d2.loss_dice: 0.9996  mix_decode.d3.loss_cls: 0.4346  mix_decode.d3.loss_mask: 0.8835  mix_decode.d3.loss_dice: 0.9853  mix_decode.d4.loss_cls: 0.4736  mix_decode.d4.loss_mask: 0.8972  mix_decode.d4.loss_dice: 0.9818  mix_decode.d5.loss_cls: 0.5048  mix_decode.d5.loss_mask: 0.8622  mix_decode.d5.loss_dice: 0.9833  mix_decode.d6.loss_cls: 0.4834  mix_decode.d6.loss_mask: 0.8668  mix_decode.d6.loss_dice: 0.9955  mix_decode.d7.loss_cls: 0.4672  mix_decode.d7.loss_mask: 0.8747  mix_decode.d7.loss_dice: 1.0151  mix_decode.d8.loss_cls: 0.4518  mix_decode.d8.loss_mask: 0.8966  mix_decode.d8.loss_dice: 0.9891
2025/03/29 12:44:42 - mmengine - INFO - Iter(train) [ 4450/20000]  base_lr: 7.9735e-05 lr: 7.9735e-05  eta: 2:38:16  time: 1.1473  data_time: 0.0223  memory: 11211  loss: 67.4930  decode.loss_cls: 0.3797  decode.loss_mask: 1.8376  decode.loss_dice: 2.0908  decode.d0.loss_cls: 0.5446  decode.d0.loss_mask: 1.8219  decode.d0.loss_dice: 2.0999  decode.d1.loss_cls: 0.3404  decode.d1.loss_mask: 1.8346  decode.d1.loss_dice: 2.1408  decode.d2.loss_cls: 0.4132  decode.d2.loss_mask: 1.8103  decode.d2.loss_dice: 2.0728  decode.d3.loss_cls: 0.3982  decode.d3.loss_mask: 1.8361  decode.d3.loss_dice: 2.1089  decode.d4.loss_cls: 0.3712  decode.d4.loss_mask: 1.8353  decode.d4.loss_dice: 2.1006  decode.d5.loss_cls: 0.3825  decode.d5.loss_mask: 1.8369  decode.d5.loss_dice: 2.1277  decode.d6.loss_cls: 0.3823  decode.d6.loss_mask: 1.8323  decode.d6.loss_dice: 2.1068  decode.d7.loss_cls: 0.3828  decode.d7.loss_mask: 1.8469  decode.d7.loss_dice: 2.1071  decode.d8.loss_cls: 0.3964  decode.d8.loss_mask: 1.8408  decode.d8.loss_dice: 2.1082  mix_decode.loss_cls: 0.3877  mix_decode.loss_mask: 0.8733  mix_decode.loss_dice: 1.1337  mix_decode.d0.loss_cls: 0.4625  mix_decode.d0.loss_mask: 0.8776  mix_decode.d0.loss_dice: 1.1499  mix_decode.d1.loss_cls: 0.3812  mix_decode.d1.loss_mask: 0.8300  mix_decode.d1.loss_dice: 1.1246  mix_decode.d2.loss_cls: 0.3941  mix_decode.d2.loss_mask: 0.8625  mix_decode.d2.loss_dice: 1.1162  mix_decode.d3.loss_cls: 0.4146  mix_decode.d3.loss_mask: 0.8899  mix_decode.d3.loss_dice: 1.1437  mix_decode.d4.loss_cls: 0.4109  mix_decode.d4.loss_mask: 0.8470  mix_decode.d4.loss_dice: 1.1021  mix_decode.d5.loss_cls: 0.3910  mix_decode.d5.loss_mask: 0.9030  mix_decode.d5.loss_dice: 1.1563  mix_decode.d6.loss_cls: 0.3978  mix_decode.d6.loss_mask: 0.8570  mix_decode.d6.loss_dice: 1.1291  mix_decode.d7.loss_cls: 0.5281  mix_decode.d7.loss_mask: 0.7992  mix_decode.d7.loss_dice: 1.0719  mix_decode.d8.loss_cls: 0.4405  mix_decode.d8.loss_mask: 0.8884  mix_decode.d8.loss_dice: 1.1418
2025/03/29 12:45:40 - mmengine - INFO - Iter(train) [ 4500/20000]  base_lr: 7.9504e-05 lr: 7.9504e-05  eta: 2:39:18  time: 1.1506  data_time: 0.0227  memory: 11214  loss: 68.3622  decode.loss_cls: 0.4621  decode.loss_mask: 2.0445  decode.loss_dice: 1.9177  decode.d0.loss_cls: 0.6260  decode.d0.loss_mask: 2.0680  decode.d0.loss_dice: 1.9156  decode.d1.loss_cls: 0.4505  decode.d1.loss_mask: 2.1002  decode.d1.loss_dice: 1.8413  decode.d2.loss_cls: 0.4685  decode.d2.loss_mask: 2.0043  decode.d2.loss_dice: 1.8269  decode.d3.loss_cls: 0.4184  decode.d3.loss_mask: 2.0507  decode.d3.loss_dice: 1.8545  decode.d4.loss_cls: 0.4925  decode.d4.loss_mask: 2.0280  decode.d4.loss_dice: 1.8644  decode.d5.loss_cls: 0.5025  decode.d5.loss_mask: 2.0109  decode.d5.loss_dice: 1.8879  decode.d6.loss_cls: 0.4982  decode.d6.loss_mask: 1.9752  decode.d6.loss_dice: 1.8379  decode.d7.loss_cls: 0.5088  decode.d7.loss_mask: 2.0403  decode.d7.loss_dice: 1.8326  decode.d8.loss_cls: 0.5392  decode.d8.loss_mask: 2.0122  decode.d8.loss_dice: 1.8506  mix_decode.loss_cls: 0.5046  mix_decode.loss_mask: 0.8946  mix_decode.loss_dice: 1.0556  mix_decode.d0.loss_cls: 0.5104  mix_decode.d0.loss_mask: 0.8912  mix_decode.d0.loss_dice: 1.0628  mix_decode.d1.loss_cls: 0.4757  mix_decode.d1.loss_mask: 0.8856  mix_decode.d1.loss_dice: 1.0358  mix_decode.d2.loss_cls: 0.5052  mix_decode.d2.loss_mask: 0.8516  mix_decode.d2.loss_dice: 1.0585  mix_decode.d3.loss_cls: 0.4896  mix_decode.d3.loss_mask: 0.9227  mix_decode.d3.loss_dice: 1.0485  mix_decode.d4.loss_cls: 0.5332  mix_decode.d4.loss_mask: 0.8670  mix_decode.d4.loss_dice: 1.0497  mix_decode.d5.loss_cls: 0.5184  mix_decode.d5.loss_mask: 0.8810  mix_decode.d5.loss_dice: 1.0411  mix_decode.d6.loss_cls: 0.5623  mix_decode.d6.loss_mask: 0.8518  mix_decode.d6.loss_dice: 1.0457  mix_decode.d7.loss_cls: 0.5481  mix_decode.d7.loss_mask: 0.8883  mix_decode.d7.loss_dice: 1.0377  mix_decode.d8.loss_cls: 0.4652  mix_decode.d8.loss_mask: 0.8989  mix_decode.d8.loss_dice: 1.0510
2025/03/29 12:46:37 - mmengine - INFO - Iter(train) [ 4550/20000]  base_lr: 7.9274e-05 lr: 7.9274e-05  eta: 2:40:18  time: 1.1458  data_time: 0.0225  memory: 11202  loss: 65.8061  decode.loss_cls: 0.5082  decode.loss_mask: 1.7441  decode.loss_dice: 2.0235  decode.d0.loss_cls: 0.5108  decode.d0.loss_mask: 1.7229  decode.d0.loss_dice: 2.0097  decode.d1.loss_cls: 0.3408  decode.d1.loss_mask: 1.7658  decode.d1.loss_dice: 2.0799  decode.d2.loss_cls: 0.4481  decode.d2.loss_mask: 1.7182  decode.d2.loss_dice: 1.9700  decode.d3.loss_cls: 0.4007  decode.d3.loss_mask: 1.7802  decode.d3.loss_dice: 1.9868  decode.d4.loss_cls: 0.5296  decode.d4.loss_mask: 1.7138  decode.d4.loss_dice: 2.0580  decode.d5.loss_cls: 0.6023  decode.d5.loss_mask: 1.7118  decode.d5.loss_dice: 1.9546  decode.d6.loss_cls: 0.5085  decode.d6.loss_mask: 1.7760  decode.d6.loss_dice: 2.0513  decode.d7.loss_cls: 0.4331  decode.d7.loss_mask: 1.7914  decode.d7.loss_dice: 1.9806  decode.d8.loss_cls: 0.4012  decode.d8.loss_mask: 1.7458  decode.d8.loss_dice: 2.0437  mix_decode.loss_cls: 0.3862  mix_decode.loss_mask: 0.8617  mix_decode.loss_dice: 1.0939  mix_decode.d0.loss_cls: 0.4443  mix_decode.d0.loss_mask: 0.8769  mix_decode.d0.loss_dice: 1.1084  mix_decode.d1.loss_cls: 0.2932  mix_decode.d1.loss_mask: 0.9257  mix_decode.d1.loss_dice: 1.0894  mix_decode.d2.loss_cls: 0.3539  mix_decode.d2.loss_mask: 0.9189  mix_decode.d2.loss_dice: 1.0822  mix_decode.d3.loss_cls: 0.4149  mix_decode.d3.loss_mask: 0.8638  mix_decode.d3.loss_dice: 1.0566  mix_decode.d4.loss_cls: 0.4346  mix_decode.d4.loss_mask: 0.8385  mix_decode.d4.loss_dice: 1.0699  mix_decode.d5.loss_cls: 0.4265  mix_decode.d5.loss_mask: 0.8607  mix_decode.d5.loss_dice: 1.0635  mix_decode.d6.loss_cls: 0.4300  mix_decode.d6.loss_mask: 0.8909  mix_decode.d6.loss_dice: 1.0746  mix_decode.d7.loss_cls: 0.3599  mix_decode.d7.loss_mask: 0.8541  mix_decode.d7.loss_dice: 1.0879  mix_decode.d8.loss_cls: 0.3744  mix_decode.d8.loss_mask: 0.8784  mix_decode.d8.loss_dice: 1.0803
2025/03/29 12:47:35 - mmengine - INFO - Iter(train) [ 4600/20000]  base_lr: 7.9043e-05 lr: 7.9043e-05  eta: 2:41:15  time: 1.1479  data_time: 0.0230  memory: 11212  loss: 62.3319  decode.loss_cls: 0.4106  decode.loss_mask: 1.9467  decode.loss_dice: 1.6061  decode.d0.loss_cls: 0.4512  decode.d0.loss_mask: 2.0168  decode.d0.loss_dice: 1.6505  decode.d1.loss_cls: 0.3350  decode.d1.loss_mask: 1.9067  decode.d1.loss_dice: 1.6230  decode.d2.loss_cls: 0.3468  decode.d2.loss_mask: 1.9462  decode.d2.loss_dice: 1.5952  decode.d3.loss_cls: 0.3986  decode.d3.loss_mask: 1.9606  decode.d3.loss_dice: 1.6141  decode.d4.loss_cls: 0.3847  decode.d4.loss_mask: 1.9662  decode.d4.loss_dice: 1.6272  decode.d5.loss_cls: 0.3896  decode.d5.loss_mask: 1.9562  decode.d5.loss_dice: 1.6189  decode.d6.loss_cls: 0.3820  decode.d6.loss_mask: 1.9646  decode.d6.loss_dice: 1.6288  decode.d7.loss_cls: 0.3635  decode.d7.loss_mask: 1.9928  decode.d7.loss_dice: 1.6166  decode.d8.loss_cls: 0.3609  decode.d8.loss_mask: 1.9555  decode.d8.loss_dice: 1.5972  mix_decode.loss_cls: 0.4448  mix_decode.loss_mask: 0.8505  mix_decode.loss_dice: 0.9321  mix_decode.d0.loss_cls: 0.4964  mix_decode.d0.loss_mask: 0.9065  mix_decode.d0.loss_dice: 0.9948  mix_decode.d1.loss_cls: 0.4411  mix_decode.d1.loss_mask: 0.8273  mix_decode.d1.loss_dice: 0.9389  mix_decode.d2.loss_cls: 0.4803  mix_decode.d2.loss_mask: 0.8428  mix_decode.d2.loss_dice: 0.9317  mix_decode.d3.loss_cls: 0.4756  mix_decode.d3.loss_mask: 0.8696  mix_decode.d3.loss_dice: 0.9558  mix_decode.d4.loss_cls: 0.4729  mix_decode.d4.loss_mask: 0.8480  mix_decode.d4.loss_dice: 0.9436  mix_decode.d5.loss_cls: 0.4794  mix_decode.d5.loss_mask: 0.8783  mix_decode.d5.loss_dice: 0.9368  mix_decode.d6.loss_cls: 0.4365  mix_decode.d6.loss_mask: 0.8755  mix_decode.d6.loss_dice: 0.9765  mix_decode.d7.loss_cls: 0.4882  mix_decode.d7.loss_mask: 0.8270  mix_decode.d7.loss_dice: 0.9363  mix_decode.d8.loss_cls: 0.4217  mix_decode.d8.loss_mask: 0.8571  mix_decode.d8.loss_dice: 0.9530
2025/03/29 12:48:32 - mmengine - INFO - Iter(train) [ 4650/20000]  base_lr: 7.8812e-05 lr: 7.8812e-05  eta: 2:42:09  time: 1.1469  data_time: 0.0230  memory: 11217  loss: 69.4818  decode.loss_cls: 0.4463  decode.loss_mask: 2.0897  decode.loss_dice: 2.0278  decode.d0.loss_cls: 0.6065  decode.d0.loss_mask: 2.0395  decode.d0.loss_dice: 2.0558  decode.d1.loss_cls: 0.4497  decode.d1.loss_mask: 2.0457  decode.d1.loss_dice: 2.0180  decode.d2.loss_cls: 0.4354  decode.d2.loss_mask: 2.0793  decode.d2.loss_dice: 2.0215  decode.d3.loss_cls: 0.4032  decode.d3.loss_mask: 2.0607  decode.d3.loss_dice: 1.9966  decode.d4.loss_cls: 0.3869  decode.d4.loss_mask: 2.1565  decode.d4.loss_dice: 2.0782  decode.d5.loss_cls: 0.4332  decode.d5.loss_mask: 2.1405  decode.d5.loss_dice: 2.0939  decode.d6.loss_cls: 0.3744  decode.d6.loss_mask: 2.0925  decode.d6.loss_dice: 2.0456  decode.d7.loss_cls: 0.4311  decode.d7.loss_mask: 2.0740  decode.d7.loss_dice: 2.0501  decode.d8.loss_cls: 0.4046  decode.d8.loss_mask: 2.2169  decode.d8.loss_dice: 2.1157  mix_decode.loss_cls: 0.4285  mix_decode.loss_mask: 0.8583  mix_decode.loss_dice: 1.0684  mix_decode.d0.loss_cls: 0.4386  mix_decode.d0.loss_mask: 0.8643  mix_decode.d0.loss_dice: 1.1393  mix_decode.d1.loss_cls: 0.3229  mix_decode.d1.loss_mask: 0.9004  mix_decode.d1.loss_dice: 1.0846  mix_decode.d2.loss_cls: 0.3730  mix_decode.d2.loss_mask: 0.8699  mix_decode.d2.loss_dice: 1.0676  mix_decode.d3.loss_cls: 0.3583  mix_decode.d3.loss_mask: 0.8882  mix_decode.d3.loss_dice: 1.0912  mix_decode.d4.loss_cls: 0.3985  mix_decode.d4.loss_mask: 0.8934  mix_decode.d4.loss_dice: 1.0811  mix_decode.d5.loss_cls: 0.4604  mix_decode.d5.loss_mask: 0.8736  mix_decode.d5.loss_dice: 1.0751  mix_decode.d6.loss_cls: 0.4102  mix_decode.d6.loss_mask: 0.8980  mix_decode.d6.loss_dice: 1.0912  mix_decode.d7.loss_cls: 0.4474  mix_decode.d7.loss_mask: 0.8391  mix_decode.d7.loss_dice: 1.0472  mix_decode.d8.loss_cls: 0.4063  mix_decode.d8.loss_mask: 0.8528  mix_decode.d8.loss_dice: 1.0841
2025/03/29 12:49:30 - mmengine - INFO - Iter(train) [ 4700/20000]  base_lr: 7.8581e-05 lr: 7.8581e-05  eta: 2:43:01  time: 1.1560  data_time: 0.0229  memory: 11202  loss: 70.1152  decode.loss_cls: 0.6024  decode.loss_mask: 2.0097  decode.loss_dice: 2.0757  decode.d0.loss_cls: 0.7334  decode.d0.loss_mask: 2.0196  decode.d0.loss_dice: 2.0977  decode.d1.loss_cls: 0.5756  decode.d1.loss_mask: 2.1260  decode.d1.loss_dice: 2.1457  decode.d2.loss_cls: 0.5396  decode.d2.loss_mask: 2.0561  decode.d2.loss_dice: 2.0746  decode.d3.loss_cls: 0.5129  decode.d3.loss_mask: 2.0972  decode.d3.loss_dice: 2.1155  decode.d4.loss_cls: 0.5089  decode.d4.loss_mask: 2.1033  decode.d4.loss_dice: 2.1305  decode.d5.loss_cls: 0.6020  decode.d5.loss_mask: 2.0545  decode.d5.loss_dice: 2.0725  decode.d6.loss_cls: 0.5696  decode.d6.loss_mask: 2.0712  decode.d6.loss_dice: 2.1036  decode.d7.loss_cls: 0.5462  decode.d7.loss_mask: 2.0456  decode.d7.loss_dice: 2.1408  decode.d8.loss_cls: 0.5426  decode.d8.loss_mask: 2.0993  decode.d8.loss_dice: 2.1151  mix_decode.loss_cls: 0.4392  mix_decode.loss_mask: 0.8400  mix_decode.loss_dice: 0.9943  mix_decode.d0.loss_cls: 0.4482  mix_decode.d0.loss_mask: 0.8457  mix_decode.d0.loss_dice: 0.9987  mix_decode.d1.loss_cls: 0.3336  mix_decode.d1.loss_mask: 0.8707  mix_decode.d1.loss_dice: 0.9908  mix_decode.d2.loss_cls: 0.3832  mix_decode.d2.loss_mask: 0.8600  mix_decode.d2.loss_dice: 1.0024  mix_decode.d3.loss_cls: 0.3944  mix_decode.d3.loss_mask: 0.8680  mix_decode.d3.loss_dice: 0.9882  mix_decode.d4.loss_cls: 0.3769  mix_decode.d4.loss_mask: 0.8718  mix_decode.d4.loss_dice: 1.0107  mix_decode.d5.loss_cls: 0.4486  mix_decode.d5.loss_mask: 0.8598  mix_decode.d5.loss_dice: 1.0050  mix_decode.d6.loss_cls: 0.4444  mix_decode.d6.loss_mask: 0.8398  mix_decode.d6.loss_dice: 0.9716  mix_decode.d7.loss_cls: 0.4460  mix_decode.d7.loss_mask: 0.8579  mix_decode.d7.loss_dice: 0.9805  mix_decode.d8.loss_cls: 0.4028  mix_decode.d8.loss_mask: 0.8671  mix_decode.d8.loss_dice: 0.9875
2025/03/29 12:50:27 - mmengine - INFO - Iter(train) [ 4750/20000]  base_lr: 7.8349e-05 lr: 7.8349e-05  eta: 2:43:51  time: 1.1473  data_time: 0.0223  memory: 11218  loss: 67.6350  decode.loss_cls: 0.3244  decode.loss_mask: 2.3520  decode.loss_dice: 1.9432  decode.d0.loss_cls: 0.6121  decode.d0.loss_mask: 2.3241  decode.d0.loss_dice: 1.9193  decode.d1.loss_cls: 0.3808  decode.d1.loss_mask: 2.2816  decode.d1.loss_dice: 1.9495  decode.d2.loss_cls: 0.4030  decode.d2.loss_mask: 2.2447  decode.d2.loss_dice: 1.8985  decode.d3.loss_cls: 0.3393  decode.d3.loss_mask: 2.3077  decode.d3.loss_dice: 1.9580  decode.d4.loss_cls: 0.4036  decode.d4.loss_mask: 2.2963  decode.d4.loss_dice: 1.9390  decode.d5.loss_cls: 0.3632  decode.d5.loss_mask: 2.3471  decode.d5.loss_dice: 1.9359  decode.d6.loss_cls: 0.4190  decode.d6.loss_mask: 2.3267  decode.d6.loss_dice: 1.9072  decode.d7.loss_cls: 0.3434  decode.d7.loss_mask: 2.2933  decode.d7.loss_dice: 1.9416  decode.d8.loss_cls: 0.3744  decode.d8.loss_mask: 2.2958  decode.d8.loss_dice: 1.9305  mix_decode.loss_cls: 0.4487  mix_decode.loss_mask: 0.7532  mix_decode.loss_dice: 0.9092  mix_decode.d0.loss_cls: 0.4791  mix_decode.d0.loss_mask: 0.7646  mix_decode.d0.loss_dice: 0.9695  mix_decode.d1.loss_cls: 0.3679  mix_decode.d1.loss_mask: 0.7740  mix_decode.d1.loss_dice: 0.9772  mix_decode.d2.loss_cls: 0.4755  mix_decode.d2.loss_mask: 0.7332  mix_decode.d2.loss_dice: 0.8998  mix_decode.d3.loss_cls: 0.4747  mix_decode.d3.loss_mask: 0.7815  mix_decode.d3.loss_dice: 0.9481  mix_decode.d4.loss_cls: 0.3940  mix_decode.d4.loss_mask: 0.7884  mix_decode.d4.loss_dice: 0.9339  mix_decode.d5.loss_cls: 0.4103  mix_decode.d5.loss_mask: 0.7439  mix_decode.d5.loss_dice: 0.9032  mix_decode.d6.loss_cls: 0.5047  mix_decode.d6.loss_mask: 0.7127  mix_decode.d6.loss_dice: 0.8862  mix_decode.d7.loss_cls: 0.4979  mix_decode.d7.loss_mask: 0.7436  mix_decode.d7.loss_dice: 0.8858  mix_decode.d8.loss_cls: 0.4874  mix_decode.d8.loss_mask: 0.7443  mix_decode.d8.loss_dice: 0.8876
2025/03/29 12:51:24 - mmengine - INFO - Iter(train) [ 4800/20000]  base_lr: 7.8118e-05 lr: 7.8118e-05  eta: 2:44:39  time: 1.1483  data_time: 0.0224  memory: 11208  loss: 61.6146  decode.loss_cls: 0.4458  decode.loss_mask: 1.7119  decode.loss_dice: 1.8955  decode.d0.loss_cls: 0.5038  decode.d0.loss_mask: 1.6993  decode.d0.loss_dice: 1.9900  decode.d1.loss_cls: 0.4663  decode.d1.loss_mask: 1.6510  decode.d1.loss_dice: 1.8997  decode.d2.loss_cls: 0.4943  decode.d2.loss_mask: 1.6578  decode.d2.loss_dice: 1.9286  decode.d3.loss_cls: 0.4736  decode.d3.loss_mask: 1.6904  decode.d3.loss_dice: 1.9250  decode.d4.loss_cls: 0.4618  decode.d4.loss_mask: 1.7191  decode.d4.loss_dice: 1.8965  decode.d5.loss_cls: 0.4903  decode.d5.loss_mask: 1.7090  decode.d5.loss_dice: 1.9076  decode.d6.loss_cls: 0.4743  decode.d6.loss_mask: 1.7258  decode.d6.loss_dice: 1.8727  decode.d7.loss_cls: 0.4313  decode.d7.loss_mask: 1.6919  decode.d7.loss_dice: 1.8808  decode.d8.loss_cls: 0.4439  decode.d8.loss_mask: 1.7036  decode.d8.loss_dice: 1.8771  mix_decode.loss_cls: 0.3653  mix_decode.loss_mask: 0.6672  mix_decode.loss_dice: 0.9619  mix_decode.d0.loss_cls: 0.4580  mix_decode.d0.loss_mask: 0.6934  mix_decode.d0.loss_dice: 1.0483  mix_decode.d1.loss_cls: 0.3384  mix_decode.d1.loss_mask: 0.6629  mix_decode.d1.loss_dice: 0.9844  mix_decode.d2.loss_cls: 0.4558  mix_decode.d2.loss_mask: 0.6753  mix_decode.d2.loss_dice: 0.9827  mix_decode.d3.loss_cls: 0.4328  mix_decode.d3.loss_mask: 0.6942  mix_decode.d3.loss_dice: 1.0002  mix_decode.d4.loss_cls: 0.3907  mix_decode.d4.loss_mask: 0.6893  mix_decode.d4.loss_dice: 1.0045  mix_decode.d5.loss_cls: 0.3592  mix_decode.d5.loss_mask: 0.6799  mix_decode.d5.loss_dice: 0.9752  mix_decode.d6.loss_cls: 0.4750  mix_decode.d6.loss_mask: 0.7024  mix_decode.d6.loss_dice: 0.9850  mix_decode.d7.loss_cls: 0.4687  mix_decode.d7.loss_mask: 0.6817  mix_decode.d7.loss_dice: 1.0009  mix_decode.d8.loss_cls: 0.3936  mix_decode.d8.loss_mask: 0.6994  mix_decode.d8.loss_dice: 0.9695
2025/03/29 12:52:22 - mmengine - INFO - Iter(train) [ 4850/20000]  base_lr: 7.7887e-05 lr: 7.7887e-05  eta: 2:45:24  time: 1.1520  data_time: 0.0227  memory: 11219  loss: 66.3851  decode.loss_cls: 0.6364  decode.loss_mask: 2.0484  decode.loss_dice: 1.9574  decode.d0.loss_cls: 0.7290  decode.d0.loss_mask: 1.9132  decode.d0.loss_dice: 1.9864  decode.d1.loss_cls: 0.6623  decode.d1.loss_mask: 1.9708  decode.d1.loss_dice: 1.9447  decode.d2.loss_cls: 0.7142  decode.d2.loss_mask: 1.9802  decode.d2.loss_dice: 1.8628  decode.d3.loss_cls: 0.7018  decode.d3.loss_mask: 1.9829  decode.d3.loss_dice: 1.8954  decode.d4.loss_cls: 0.6438  decode.d4.loss_mask: 2.0318  decode.d4.loss_dice: 1.9236  decode.d5.loss_cls: 0.6986  decode.d5.loss_mask: 2.0384  decode.d5.loss_dice: 1.9321  decode.d6.loss_cls: 0.7685  decode.d6.loss_mask: 2.0303  decode.d6.loss_dice: 1.9353  decode.d7.loss_cls: 0.7384  decode.d7.loss_mask: 1.9987  decode.d7.loss_dice: 1.9440  decode.d8.loss_cls: 0.7270  decode.d8.loss_mask: 1.9845  decode.d8.loss_dice: 1.9213  mix_decode.loss_cls: 0.3637  mix_decode.loss_mask: 0.7730  mix_decode.loss_dice: 0.9114  mix_decode.d0.loss_cls: 0.3922  mix_decode.d0.loss_mask: 0.7631  mix_decode.d0.loss_dice: 0.9000  mix_decode.d1.loss_cls: 0.3246  mix_decode.d1.loss_mask: 0.7678  mix_decode.d1.loss_dice: 0.8658  mix_decode.d2.loss_cls: 0.3251  mix_decode.d2.loss_mask: 0.7747  mix_decode.d2.loss_dice: 0.8652  mix_decode.d3.loss_cls: 0.3717  mix_decode.d3.loss_mask: 0.7580  mix_decode.d3.loss_dice: 0.8477  mix_decode.d4.loss_cls: 0.3799  mix_decode.d4.loss_mask: 0.7494  mix_decode.d4.loss_dice: 0.8427  mix_decode.d5.loss_cls: 0.3761  mix_decode.d5.loss_mask: 0.7612  mix_decode.d5.loss_dice: 0.8554  mix_decode.d6.loss_cls: 0.4140  mix_decode.d6.loss_mask: 0.7623  mix_decode.d6.loss_dice: 0.8573  mix_decode.d7.loss_cls: 0.4775  mix_decode.d7.loss_mask: 0.7296  mix_decode.d7.loss_dice: 0.8611  mix_decode.d8.loss_cls: 0.4241  mix_decode.d8.loss_mask: 0.7387  mix_decode.d8.loss_dice: 0.8494
2025/03/29 12:53:19 - mmengine - INFO - Iter(train) [ 4900/20000]  base_lr: 7.7655e-05 lr: 7.7655e-05  eta: 2:46:07  time: 1.1445  data_time: 0.0223  memory: 11212  loss: 62.8773  decode.loss_cls: 0.4772  decode.loss_mask: 1.9387  decode.loss_dice: 1.8107  decode.d0.loss_cls: 0.6470  decode.d0.loss_mask: 1.9223  decode.d0.loss_dice: 1.8364  decode.d1.loss_cls: 0.4347  decode.d1.loss_mask: 1.9436  decode.d1.loss_dice: 1.7939  decode.d2.loss_cls: 0.5389  decode.d2.loss_mask: 1.9732  decode.d2.loss_dice: 1.7369  decode.d3.loss_cls: 0.5167  decode.d3.loss_mask: 1.8928  decode.d3.loss_dice: 1.7034  decode.d4.loss_cls: 0.4735  decode.d4.loss_mask: 1.9386  decode.d4.loss_dice: 1.7939  decode.d5.loss_cls: 0.4411  decode.d5.loss_mask: 2.0108  decode.d5.loss_dice: 1.7758  decode.d6.loss_cls: 0.5255  decode.d6.loss_mask: 1.9815  decode.d6.loss_dice: 1.8162  decode.d7.loss_cls: 0.5456  decode.d7.loss_mask: 1.9924  decode.d7.loss_dice: 1.8206  decode.d8.loss_cls: 0.4779  decode.d8.loss_mask: 1.9899  decode.d8.loss_dice: 1.7686  mix_decode.loss_cls: 0.4464  mix_decode.loss_mask: 0.7220  mix_decode.loss_dice: 0.8914  mix_decode.d0.loss_cls: 0.4035  mix_decode.d0.loss_mask: 0.7301  mix_decode.d0.loss_dice: 0.9229  mix_decode.d1.loss_cls: 0.3566  mix_decode.d1.loss_mask: 0.7481  mix_decode.d1.loss_dice: 0.8673  mix_decode.d2.loss_cls: 0.4536  mix_decode.d2.loss_mask: 0.7286  mix_decode.d2.loss_dice: 0.8705  mix_decode.d3.loss_cls: 0.3532  mix_decode.d3.loss_mask: 0.7519  mix_decode.d3.loss_dice: 0.8665  mix_decode.d4.loss_cls: 0.4221  mix_decode.d4.loss_mask: 0.7586  mix_decode.d4.loss_dice: 0.8924  mix_decode.d5.loss_cls: 0.4859  mix_decode.d5.loss_mask: 0.7596  mix_decode.d5.loss_dice: 0.8683  mix_decode.d6.loss_cls: 0.4507  mix_decode.d6.loss_mask: 0.7187  mix_decode.d6.loss_dice: 0.8593  mix_decode.d7.loss_cls: 0.3947  mix_decode.d7.loss_mask: 0.7574  mix_decode.d7.loss_dice: 0.8654  mix_decode.d8.loss_cls: 0.3834  mix_decode.d8.loss_mask: 0.7438  mix_decode.d8.loss_dice: 0.8861
2025/03/29 12:54:17 - mmengine - INFO - Iter(train) [ 4950/20000]  base_lr: 7.7424e-05 lr: 7.7424e-05  eta: 2:46:48  time: 1.1452  data_time: 0.0222  memory: 11208  loss: 64.0887  decode.loss_cls: 0.3558  decode.loss_mask: 1.9885  decode.loss_dice: 1.8886  decode.d0.loss_cls: 0.4184  decode.d0.loss_mask: 2.0070  decode.d0.loss_dice: 1.9530  decode.d1.loss_cls: 0.3479  decode.d1.loss_mask: 1.9704  decode.d1.loss_dice: 1.9067  decode.d2.loss_cls: 0.3975  decode.d2.loss_mask: 1.9817  decode.d2.loss_dice: 1.8769  decode.d3.loss_cls: 0.4127  decode.d3.loss_mask: 1.9985  decode.d3.loss_dice: 1.9005  decode.d4.loss_cls: 0.3453  decode.d4.loss_mask: 2.0225  decode.d4.loss_dice: 1.9364  decode.d5.loss_cls: 0.3373  decode.d5.loss_mask: 1.9929  decode.d5.loss_dice: 1.9307  decode.d6.loss_cls: 0.3295  decode.d6.loss_mask: 2.0014  decode.d6.loss_dice: 1.9167  decode.d7.loss_cls: 0.3420  decode.d7.loss_mask: 2.0138  decode.d7.loss_dice: 1.9106  decode.d8.loss_cls: 0.3235  decode.d8.loss_mask: 2.0000  decode.d8.loss_dice: 1.9118  mix_decode.loss_cls: 0.2925  mix_decode.loss_mask: 0.7489  mix_decode.loss_dice: 1.0140  mix_decode.d0.loss_cls: 0.4416  mix_decode.d0.loss_mask: 0.7526  mix_decode.d0.loss_dice: 1.0461  mix_decode.d1.loss_cls: 0.3008  mix_decode.d1.loss_mask: 0.7571  mix_decode.d1.loss_dice: 1.0549  mix_decode.d2.loss_cls: 0.3371  mix_decode.d2.loss_mask: 0.7528  mix_decode.d2.loss_dice: 1.0076  mix_decode.d3.loss_cls: 0.3122  mix_decode.d3.loss_mask: 0.7609  mix_decode.d3.loss_dice: 1.0223  mix_decode.d4.loss_cls: 0.3757  mix_decode.d4.loss_mask: 0.7689  mix_decode.d4.loss_dice: 1.0131  mix_decode.d5.loss_cls: 0.3646  mix_decode.d5.loss_mask: 0.7121  mix_decode.d5.loss_dice: 1.0160  mix_decode.d6.loss_cls: 0.4049  mix_decode.d6.loss_mask: 0.7487  mix_decode.d6.loss_dice: 1.0254  mix_decode.d7.loss_cls: 0.4607  mix_decode.d7.loss_mask: 0.7503  mix_decode.d7.loss_dice: 0.9939  mix_decode.d8.loss_cls: 0.3727  mix_decode.d8.loss_mask: 0.7579  mix_decode.d8.loss_dice: 1.0039
2025/03/29 12:55:14 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 12:55:14 - mmengine - INFO - Iter(train) [ 5000/20000]  base_lr: 7.7192e-05 lr: 7.7192e-05  eta: 2:47:28  time: 1.1493  data_time: 0.0223  memory: 11210  loss: 66.6123  decode.loss_cls: 0.6426  decode.loss_mask: 1.9186  decode.loss_dice: 2.0731  decode.d0.loss_cls: 0.6556  decode.d0.loss_mask: 1.9669  decode.d0.loss_dice: 2.0284  decode.d1.loss_cls: 0.5048  decode.d1.loss_mask: 1.9978  decode.d1.loss_dice: 2.0865  decode.d2.loss_cls: 0.5369  decode.d2.loss_mask: 2.0461  decode.d2.loss_dice: 2.0164  decode.d3.loss_cls: 0.6068  decode.d3.loss_mask: 1.9476  decode.d3.loss_dice: 1.9781  decode.d4.loss_cls: 0.5102  decode.d4.loss_mask: 1.9892  decode.d4.loss_dice: 2.0425  decode.d5.loss_cls: 0.6584  decode.d5.loss_mask: 1.9288  decode.d5.loss_dice: 2.0186  decode.d6.loss_cls: 0.7061  decode.d6.loss_mask: 1.9712  decode.d6.loss_dice: 2.0686  decode.d7.loss_cls: 0.5909  decode.d7.loss_mask: 2.0147  decode.d7.loss_dice: 2.0759  decode.d8.loss_cls: 0.6374  decode.d8.loss_mask: 1.9781  decode.d8.loss_dice: 2.0630  mix_decode.loss_cls: 0.3653  mix_decode.loss_mask: 0.7216  mix_decode.loss_dice: 0.9572  mix_decode.d0.loss_cls: 0.3352  mix_decode.d0.loss_mask: 0.7338  mix_decode.d0.loss_dice: 1.0235  mix_decode.d1.loss_cls: 0.2771  mix_decode.d1.loss_mask: 0.7289  mix_decode.d1.loss_dice: 0.9849  mix_decode.d2.loss_cls: 0.3264  mix_decode.d2.loss_mask: 0.7322  mix_decode.d2.loss_dice: 0.9368  mix_decode.d3.loss_cls: 0.3218  mix_decode.d3.loss_mask: 0.7179  mix_decode.d3.loss_dice: 0.9509  mix_decode.d4.loss_cls: 0.3142  mix_decode.d4.loss_mask: 0.7431  mix_decode.d4.loss_dice: 0.9949  mix_decode.d5.loss_cls: 0.4110  mix_decode.d5.loss_mask: 0.7179  mix_decode.d5.loss_dice: 0.9430  mix_decode.d6.loss_cls: 0.3465  mix_decode.d6.loss_mask: 0.7263  mix_decode.d6.loss_dice: 0.9816  mix_decode.d7.loss_cls: 0.3426  mix_decode.d7.loss_mask: 0.7389  mix_decode.d7.loss_dice: 0.9635  mix_decode.d8.loss_cls: 0.3317  mix_decode.d8.loss_mask: 0.7248  mix_decode.d8.loss_dice: 0.9586
2025/03/29 12:56:11 - mmengine - INFO - Iter(train) [ 5050/20000]  base_lr: 7.6961e-05 lr: 7.6961e-05  eta: 2:48:05  time: 1.1493  data_time: 0.0228  memory: 11213  loss: 67.1382  decode.loss_cls: 0.6533  decode.loss_mask: 2.1770  decode.loss_dice: 1.8495  decode.d0.loss_cls: 0.6990  decode.d0.loss_mask: 2.1842  decode.d0.loss_dice: 1.8603  decode.d1.loss_cls: 0.6248  decode.d1.loss_mask: 2.2273  decode.d1.loss_dice: 1.8370  decode.d2.loss_cls: 0.6620  decode.d2.loss_mask: 2.2728  decode.d2.loss_dice: 1.8629  decode.d3.loss_cls: 0.6533  decode.d3.loss_mask: 2.1891  decode.d3.loss_dice: 1.8174  decode.d4.loss_cls: 0.6691  decode.d4.loss_mask: 2.1212  decode.d4.loss_dice: 1.8614  decode.d5.loss_cls: 0.7142  decode.d5.loss_mask: 2.1504  decode.d5.loss_dice: 1.8784  decode.d6.loss_cls: 0.6980  decode.d6.loss_mask: 2.1634  decode.d6.loss_dice: 1.8516  decode.d7.loss_cls: 0.5931  decode.d7.loss_mask: 2.1721  decode.d7.loss_dice: 1.7997  decode.d8.loss_cls: 0.5774  decode.d8.loss_mask: 2.2016  decode.d8.loss_dice: 1.7777  mix_decode.loss_cls: 0.3593  mix_decode.loss_mask: 0.7424  mix_decode.loss_dice: 0.9383  mix_decode.d0.loss_cls: 0.4065  mix_decode.d0.loss_mask: 0.7301  mix_decode.d0.loss_dice: 0.9365  mix_decode.d1.loss_cls: 0.2722  mix_decode.d1.loss_mask: 0.7622  mix_decode.d1.loss_dice: 0.9068  mix_decode.d2.loss_cls: 0.3372  mix_decode.d2.loss_mask: 0.7670  mix_decode.d2.loss_dice: 0.9166  mix_decode.d3.loss_cls: 0.3166  mix_decode.d3.loss_mask: 0.7784  mix_decode.d3.loss_dice: 0.9390  mix_decode.d4.loss_cls: 0.3373  mix_decode.d4.loss_mask: 0.7574  mix_decode.d4.loss_dice: 0.9227  mix_decode.d5.loss_cls: 0.3525  mix_decode.d5.loss_mask: 0.7330  mix_decode.d5.loss_dice: 0.8976  mix_decode.d6.loss_cls: 0.3663  mix_decode.d6.loss_mask: 0.7941  mix_decode.d6.loss_dice: 0.9452  mix_decode.d7.loss_cls: 0.3994  mix_decode.d7.loss_mask: 0.7396  mix_decode.d7.loss_dice: 0.9059  mix_decode.d8.loss_cls: 0.3747  mix_decode.d8.loss_mask: 0.7634  mix_decode.d8.loss_dice: 0.9405
2025/03/29 12:57:09 - mmengine - INFO - Iter(train) [ 5100/20000]  base_lr: 7.6729e-05 lr: 7.6729e-05  eta: 2:48:41  time: 1.1532  data_time: 0.0230  memory: 11214  loss: 63.0432  decode.loss_cls: 0.3932  decode.loss_mask: 2.0753  decode.loss_dice: 1.8710  decode.d0.loss_cls: 0.5882  decode.d0.loss_mask: 1.9680  decode.d0.loss_dice: 1.8294  decode.d1.loss_cls: 0.4377  decode.d1.loss_mask: 2.0278  decode.d1.loss_dice: 1.8541  decode.d2.loss_cls: 0.5255  decode.d2.loss_mask: 1.9829  decode.d2.loss_dice: 1.7899  decode.d3.loss_cls: 0.4439  decode.d3.loss_mask: 2.0493  decode.d3.loss_dice: 1.8604  decode.d4.loss_cls: 0.4685  decode.d4.loss_mask: 2.0889  decode.d4.loss_dice: 1.8526  decode.d5.loss_cls: 0.5302  decode.d5.loss_mask: 2.0191  decode.d5.loss_dice: 1.8207  decode.d6.loss_cls: 0.5121  decode.d6.loss_mask: 2.0204  decode.d6.loss_dice: 1.8177  decode.d7.loss_cls: 0.5001  decode.d7.loss_mask: 2.0101  decode.d7.loss_dice: 1.8543  decode.d8.loss_cls: 0.3928  decode.d8.loss_mask: 2.0616  decode.d8.loss_dice: 1.8614  mix_decode.loss_cls: 0.3507  mix_decode.loss_mask: 0.7582  mix_decode.loss_dice: 0.8247  mix_decode.d0.loss_cls: 0.3395  mix_decode.d0.loss_mask: 0.7199  mix_decode.d0.loss_dice: 0.8732  mix_decode.d1.loss_cls: 0.3085  mix_decode.d1.loss_mask: 0.7193  mix_decode.d1.loss_dice: 0.8444  mix_decode.d2.loss_cls: 0.3599  mix_decode.d2.loss_mask: 0.7281  mix_decode.d2.loss_dice: 0.8412  mix_decode.d3.loss_cls: 0.3989  mix_decode.d3.loss_mask: 0.7134  mix_decode.d3.loss_dice: 0.8251  mix_decode.d4.loss_cls: 0.3864  mix_decode.d4.loss_mask: 0.7195  mix_decode.d4.loss_dice: 0.8229  mix_decode.d5.loss_cls: 0.4230  mix_decode.d5.loss_mask: 0.7688  mix_decode.d5.loss_dice: 0.8193  mix_decode.d6.loss_cls: 0.4541  mix_decode.d6.loss_mask: 0.7400  mix_decode.d6.loss_dice: 0.8474  mix_decode.d7.loss_cls: 0.4175  mix_decode.d7.loss_mask: 0.7245  mix_decode.d7.loss_dice: 0.8247  mix_decode.d8.loss_cls: 0.3896  mix_decode.d8.loss_mask: 0.7722  mix_decode.d8.loss_dice: 0.8215
2025/03/29 12:58:06 - mmengine - INFO - Iter(train) [ 5150/20000]  base_lr: 7.6497e-05 lr: 7.6497e-05  eta: 2:49:14  time: 1.1524  data_time: 0.0232  memory: 11206  loss: 65.9286  decode.loss_cls: 0.4583  decode.loss_mask: 1.9172  decode.loss_dice: 2.0305  decode.d0.loss_cls: 0.4368  decode.d0.loss_mask: 2.0074  decode.d0.loss_dice: 2.0930  decode.d1.loss_cls: 0.3148  decode.d1.loss_mask: 1.9678  decode.d1.loss_dice: 2.0944  decode.d2.loss_cls: 0.3669  decode.d2.loss_mask: 2.0067  decode.d2.loss_dice: 2.0861  decode.d3.loss_cls: 0.3878  decode.d3.loss_mask: 1.9386  decode.d3.loss_dice: 2.0578  decode.d4.loss_cls: 0.3853  decode.d4.loss_mask: 1.9395  decode.d4.loss_dice: 2.0646  decode.d5.loss_cls: 0.4637  decode.d5.loss_mask: 1.9094  decode.d5.loss_dice: 2.0544  decode.d6.loss_cls: 0.5023  decode.d6.loss_mask: 1.9436  decode.d6.loss_dice: 2.0808  decode.d7.loss_cls: 0.4768  decode.d7.loss_mask: 1.9385  decode.d7.loss_dice: 2.0948  decode.d8.loss_cls: 0.4909  decode.d8.loss_mask: 1.9396  decode.d8.loss_dice: 2.0761  mix_decode.loss_cls: 0.3350  mix_decode.loss_mask: 0.7442  mix_decode.loss_dice: 1.0418  mix_decode.d0.loss_cls: 0.3525  mix_decode.d0.loss_mask: 0.7354  mix_decode.d0.loss_dice: 1.0726  mix_decode.d1.loss_cls: 0.3076  mix_decode.d1.loss_mask: 0.7286  mix_decode.d1.loss_dice: 1.0123  mix_decode.d2.loss_cls: 0.3900  mix_decode.d2.loss_mask: 0.7294  mix_decode.d2.loss_dice: 0.9946  mix_decode.d3.loss_cls: 0.3952  mix_decode.d3.loss_mask: 0.7206  mix_decode.d3.loss_dice: 0.9902  mix_decode.d4.loss_cls: 0.4079  mix_decode.d4.loss_mask: 0.7403  mix_decode.d4.loss_dice: 0.9962  mix_decode.d5.loss_cls: 0.3700  mix_decode.d5.loss_mask: 0.7413  mix_decode.d5.loss_dice: 1.0235  mix_decode.d6.loss_cls: 0.4562  mix_decode.d6.loss_mask: 0.7397  mix_decode.d6.loss_dice: 1.0146  mix_decode.d7.loss_cls: 0.4388  mix_decode.d7.loss_mask: 0.7283  mix_decode.d7.loss_dice: 1.0416  mix_decode.d8.loss_cls: 0.3785  mix_decode.d8.loss_mask: 0.7485  mix_decode.d8.loss_dice: 1.0290
2025/03/29 12:59:04 - mmengine - INFO - Iter(train) [ 5200/20000]  base_lr: 7.6265e-05 lr: 7.6265e-05  eta: 2:49:46  time: 1.1461  data_time: 0.0223  memory: 11213  loss: 69.4836  decode.loss_cls: 0.5621  decode.loss_mask: 1.8825  decode.loss_dice: 2.2855  decode.d0.loss_cls: 0.6404  decode.d0.loss_mask: 1.8422  decode.d0.loss_dice: 2.2500  decode.d1.loss_cls: 0.5354  decode.d1.loss_mask: 1.8399  decode.d1.loss_dice: 2.2596  decode.d2.loss_cls: 0.6823  decode.d2.loss_mask: 1.8446  decode.d2.loss_dice: 2.2466  decode.d3.loss_cls: 0.5435  decode.d3.loss_mask: 1.8963  decode.d3.loss_dice: 2.2458  decode.d4.loss_cls: 0.5101  decode.d4.loss_mask: 1.8754  decode.d4.loss_dice: 2.2555  decode.d5.loss_cls: 0.5899  decode.d5.loss_mask: 1.8472  decode.d5.loss_dice: 2.2610  decode.d6.loss_cls: 0.5557  decode.d6.loss_mask: 1.8474  decode.d6.loss_dice: 2.2832  decode.d7.loss_cls: 0.4599  decode.d7.loss_mask: 1.8940  decode.d7.loss_dice: 2.2549  decode.d8.loss_cls: 0.5736  decode.d8.loss_mask: 1.8713  decode.d8.loss_dice: 2.2770  mix_decode.loss_cls: 0.4817  mix_decode.loss_mask: 0.6852  mix_decode.loss_dice: 1.0807  mix_decode.d0.loss_cls: 0.3965  mix_decode.d0.loss_mask: 0.6816  mix_decode.d0.loss_dice: 1.1441  mix_decode.d1.loss_cls: 0.3837  mix_decode.d1.loss_mask: 0.7037  mix_decode.d1.loss_dice: 1.0924  mix_decode.d2.loss_cls: 0.5169  mix_decode.d2.loss_mask: 0.6815  mix_decode.d2.loss_dice: 1.0971  mix_decode.d3.loss_cls: 0.5292  mix_decode.d3.loss_mask: 0.6839  mix_decode.d3.loss_dice: 1.0554  mix_decode.d4.loss_cls: 0.5354  mix_decode.d4.loss_mask: 0.6691  mix_decode.d4.loss_dice: 1.0567  mix_decode.d5.loss_cls: 0.5459  mix_decode.d5.loss_mask: 0.6666  mix_decode.d5.loss_dice: 1.0773  mix_decode.d6.loss_cls: 0.5589  mix_decode.d6.loss_mask: 0.6581  mix_decode.d6.loss_dice: 1.0574  mix_decode.d7.loss_cls: 0.4757  mix_decode.d7.loss_mask: 0.6960  mix_decode.d7.loss_dice: 1.0646  mix_decode.d8.loss_cls: 0.4953  mix_decode.d8.loss_mask: 0.7026  mix_decode.d8.loss_dice: 1.0975
2025/03/29 13:00:01 - mmengine - INFO - Iter(train) [ 5250/20000]  base_lr: 7.6034e-05 lr: 7.6034e-05  eta: 2:50:16  time: 1.1491  data_time: 0.0223  memory: 11213  loss: 63.1876  decode.loss_cls: 0.4553  decode.loss_mask: 1.8664  decode.loss_dice: 1.7666  decode.d0.loss_cls: 0.4896  decode.d0.loss_mask: 1.8588  decode.d0.loss_dice: 1.7452  decode.d1.loss_cls: 0.4176  decode.d1.loss_mask: 1.8821  decode.d1.loss_dice: 1.7663  decode.d2.loss_cls: 0.4046  decode.d2.loss_mask: 1.9122  decode.d2.loss_dice: 1.7660  decode.d3.loss_cls: 0.4280  decode.d3.loss_mask: 1.9463  decode.d3.loss_dice: 1.7779  decode.d4.loss_cls: 0.3979  decode.d4.loss_mask: 1.8461  decode.d4.loss_dice: 1.7895  decode.d5.loss_cls: 0.4314  decode.d5.loss_mask: 1.8682  decode.d5.loss_dice: 1.7068  decode.d6.loss_cls: 0.4658  decode.d6.loss_mask: 1.9263  decode.d6.loss_dice: 1.7564  decode.d7.loss_cls: 0.4300  decode.d7.loss_mask: 1.8886  decode.d7.loss_dice: 1.7303  decode.d8.loss_cls: 0.4825  decode.d8.loss_mask: 1.8736  decode.d8.loss_dice: 1.7254  mix_decode.loss_cls: 0.4201  mix_decode.loss_mask: 0.7273  mix_decode.loss_dice: 1.0765  mix_decode.d0.loss_cls: 0.3314  mix_decode.d0.loss_mask: 0.7503  mix_decode.d0.loss_dice: 1.1341  mix_decode.d1.loss_cls: 0.3484  mix_decode.d1.loss_mask: 0.7710  mix_decode.d1.loss_dice: 1.1616  mix_decode.d2.loss_cls: 0.4567  mix_decode.d2.loss_mask: 0.7290  mix_decode.d2.loss_dice: 1.0573  mix_decode.d3.loss_cls: 0.4521  mix_decode.d3.loss_mask: 0.6931  mix_decode.d3.loss_dice: 1.0307  mix_decode.d4.loss_cls: 0.4305  mix_decode.d4.loss_mask: 0.7333  mix_decode.d4.loss_dice: 1.0944  mix_decode.d5.loss_cls: 0.4294  mix_decode.d5.loss_mask: 0.7175  mix_decode.d5.loss_dice: 1.0794  mix_decode.d6.loss_cls: 0.4116  mix_decode.d6.loss_mask: 0.7035  mix_decode.d6.loss_dice: 1.0585  mix_decode.d7.loss_cls: 0.4880  mix_decode.d7.loss_mask: 0.7653  mix_decode.d7.loss_dice: 1.1164  mix_decode.d8.loss_cls: 0.3816  mix_decode.d8.loss_mask: 0.7528  mix_decode.d8.loss_dice: 1.0840
2025/03/29 13:00:59 - mmengine - INFO - Iter(train) [ 5300/20000]  base_lr: 7.5802e-05 lr: 7.5802e-05  eta: 2:50:45  time: 1.1480  data_time: 0.0225  memory: 11211  loss: 60.8809  decode.loss_cls: 0.3028  decode.loss_mask: 1.8722  decode.loss_dice: 1.8088  decode.d0.loss_cls: 0.4645  decode.d0.loss_mask: 1.8843  decode.d0.loss_dice: 1.7800  decode.d1.loss_cls: 0.2717  decode.d1.loss_mask: 1.9132  decode.d1.loss_dice: 1.8583  decode.d2.loss_cls: 0.3843  decode.d2.loss_mask: 1.8179  decode.d2.loss_dice: 1.8301  decode.d3.loss_cls: 0.3559  decode.d3.loss_mask: 1.8118  decode.d3.loss_dice: 1.8101  decode.d4.loss_cls: 0.3219  decode.d4.loss_mask: 1.8685  decode.d4.loss_dice: 1.8790  decode.d5.loss_cls: 0.3215  decode.d5.loss_mask: 1.9327  decode.d5.loss_dice: 1.8463  decode.d6.loss_cls: 0.3116  decode.d6.loss_mask: 1.9324  decode.d6.loss_dice: 1.8407  decode.d7.loss_cls: 0.3230  decode.d7.loss_mask: 1.8538  decode.d7.loss_dice: 1.8487  decode.d8.loss_cls: 0.3980  decode.d8.loss_mask: 1.8563  decode.d8.loss_dice: 1.8298  mix_decode.loss_cls: 0.3915  mix_decode.loss_mask: 0.6363  mix_decode.loss_dice: 1.0468  mix_decode.d0.loss_cls: 0.3249  mix_decode.d0.loss_mask: 0.6522  mix_decode.d0.loss_dice: 1.0683  mix_decode.d1.loss_cls: 0.3129  mix_decode.d1.loss_mask: 0.6213  mix_decode.d1.loss_dice: 1.0161  mix_decode.d2.loss_cls: 0.3234  mix_decode.d2.loss_mask: 0.6409  mix_decode.d2.loss_dice: 1.0334  mix_decode.d3.loss_cls: 0.3789  mix_decode.d3.loss_mask: 0.6136  mix_decode.d3.loss_dice: 1.0182  mix_decode.d4.loss_cls: 0.4070  mix_decode.d4.loss_mask: 0.5957  mix_decode.d4.loss_dice: 1.0103  mix_decode.d5.loss_cls: 0.4202  mix_decode.d5.loss_mask: 0.5887  mix_decode.d5.loss_dice: 1.0085  mix_decode.d6.loss_cls: 0.4560  mix_decode.d6.loss_mask: 0.6303  mix_decode.d6.loss_dice: 0.9971  mix_decode.d7.loss_cls: 0.4541  mix_decode.d7.loss_mask: 0.6079  mix_decode.d7.loss_dice: 0.9915  mix_decode.d8.loss_cls: 0.4235  mix_decode.d8.loss_mask: 0.6360  mix_decode.d8.loss_dice: 1.0450
2025/03/29 13:01:56 - mmengine - INFO - Iter(train) [ 5350/20000]  base_lr: 7.5569e-05 lr: 7.5569e-05  eta: 2:51:12  time: 1.1468  data_time: 0.0227  memory: 11222  loss: 60.5101  decode.loss_cls: 0.4895  decode.loss_mask: 1.7334  decode.loss_dice: 1.9154  decode.d0.loss_cls: 0.5206  decode.d0.loss_mask: 1.7699  decode.d0.loss_dice: 1.9720  decode.d1.loss_cls: 0.4494  decode.d1.loss_mask: 1.7339  decode.d1.loss_dice: 1.9083  decode.d2.loss_cls: 0.4070  decode.d2.loss_mask: 1.7138  decode.d2.loss_dice: 1.8836  decode.d3.loss_cls: 0.4195  decode.d3.loss_mask: 1.6951  decode.d3.loss_dice: 1.9074  decode.d4.loss_cls: 0.3973  decode.d4.loss_mask: 1.7179  decode.d4.loss_dice: 1.9052  decode.d5.loss_cls: 0.3900  decode.d5.loss_mask: 1.7392  decode.d5.loss_dice: 1.9071  decode.d6.loss_cls: 0.4863  decode.d6.loss_mask: 1.7882  decode.d6.loss_dice: 1.8752  decode.d7.loss_cls: 0.4521  decode.d7.loss_mask: 1.7571  decode.d7.loss_dice: 1.8801  decode.d8.loss_cls: 0.4327  decode.d8.loss_mask: 1.7229  decode.d8.loss_dice: 1.9030  mix_decode.loss_cls: 0.3389  mix_decode.loss_mask: 0.7624  mix_decode.loss_dice: 0.8963  mix_decode.d0.loss_cls: 0.3601  mix_decode.d0.loss_mask: 0.7376  mix_decode.d0.loss_dice: 0.9222  mix_decode.d1.loss_cls: 0.2447  mix_decode.d1.loss_mask: 0.7824  mix_decode.d1.loss_dice: 0.9421  mix_decode.d2.loss_cls: 0.2866  mix_decode.d2.loss_mask: 0.7347  mix_decode.d2.loss_dice: 0.8998  mix_decode.d3.loss_cls: 0.2868  mix_decode.d3.loss_mask: 0.7549  mix_decode.d3.loss_dice: 0.9212  mix_decode.d4.loss_cls: 0.2389  mix_decode.d4.loss_mask: 0.7373  mix_decode.d4.loss_dice: 0.9288  mix_decode.d5.loss_cls: 0.3090  mix_decode.d5.loss_mask: 0.7354  mix_decode.d5.loss_dice: 0.8994  mix_decode.d6.loss_cls: 0.3188  mix_decode.d6.loss_mask: 0.7428  mix_decode.d6.loss_dice: 0.8688  mix_decode.d7.loss_cls: 0.3762  mix_decode.d7.loss_mask: 0.7375  mix_decode.d7.loss_dice: 0.8857  mix_decode.d8.loss_cls: 0.3503  mix_decode.d8.loss_mask: 0.7485  mix_decode.d8.loss_dice: 0.8889
2025/03/29 13:02:53 - mmengine - INFO - Iter(train) [ 5400/20000]  base_lr: 7.5337e-05 lr: 7.5337e-05  eta: 2:51:37  time: 1.1475  data_time: 0.0221  memory: 11217  loss: 67.4703  decode.loss_cls: 0.3184  decode.loss_mask: 2.1588  decode.loss_dice: 2.0475  decode.d0.loss_cls: 0.4192  decode.d0.loss_mask: 2.1126  decode.d0.loss_dice: 2.0529  decode.d1.loss_cls: 0.3163  decode.d1.loss_mask: 2.1480  decode.d1.loss_dice: 2.0129  decode.d2.loss_cls: 0.2975  decode.d2.loss_mask: 2.1323  decode.d2.loss_dice: 2.0426  decode.d3.loss_cls: 0.2900  decode.d3.loss_mask: 2.1826  decode.d3.loss_dice: 2.0747  decode.d4.loss_cls: 0.3370  decode.d4.loss_mask: 2.1152  decode.d4.loss_dice: 2.0422  decode.d5.loss_cls: 0.3164  decode.d5.loss_mask: 2.0908  decode.d5.loss_dice: 2.0463  decode.d6.loss_cls: 0.3509  decode.d6.loss_mask: 2.1200  decode.d6.loss_dice: 2.0547  decode.d7.loss_cls: 0.3178  decode.d7.loss_mask: 2.1275  decode.d7.loss_dice: 2.0581  decode.d8.loss_cls: 0.3341  decode.d8.loss_mask: 2.1092  decode.d8.loss_dice: 2.0494  mix_decode.loss_cls: 0.3621  mix_decode.loss_mask: 0.7688  mix_decode.loss_dice: 1.1254  mix_decode.d0.loss_cls: 0.2941  mix_decode.d0.loss_mask: 0.8110  mix_decode.d0.loss_dice: 1.1854  mix_decode.d1.loss_cls: 0.2599  mix_decode.d1.loss_mask: 0.8055  mix_decode.d1.loss_dice: 1.0967  mix_decode.d2.loss_cls: 0.2819  mix_decode.d2.loss_mask: 0.7781  mix_decode.d2.loss_dice: 1.1273  mix_decode.d3.loss_cls: 0.2658  mix_decode.d3.loss_mask: 0.8119  mix_decode.d3.loss_dice: 1.1531  mix_decode.d4.loss_cls: 0.2896  mix_decode.d4.loss_mask: 0.7732  mix_decode.d4.loss_dice: 1.1324  mix_decode.d5.loss_cls: 0.2935  mix_decode.d5.loss_mask: 0.7752  mix_decode.d5.loss_dice: 1.1603  mix_decode.d6.loss_cls: 0.3482  mix_decode.d6.loss_mask: 0.7830  mix_decode.d6.loss_dice: 1.1644  mix_decode.d7.loss_cls: 0.3721  mix_decode.d7.loss_mask: 0.7769  mix_decode.d7.loss_dice: 1.1178  mix_decode.d8.loss_cls: 0.3714  mix_decode.d8.loss_mask: 0.7689  mix_decode.d8.loss_dice: 1.1405
2025/03/29 13:03:51 - mmengine - INFO - Iter(train) [ 5450/20000]  base_lr: 7.5105e-05 lr: 7.5105e-05  eta: 2:52:01  time: 1.1471  data_time: 0.0225  memory: 11209  loss: 67.1685  decode.loss_cls: 0.6197  decode.loss_mask: 2.2591  decode.loss_dice: 1.9580  decode.d0.loss_cls: 0.5404  decode.d0.loss_mask: 2.1719  decode.d0.loss_dice: 2.0166  decode.d1.loss_cls: 0.5769  decode.d1.loss_mask: 2.1773  decode.d1.loss_dice: 2.0076  decode.d2.loss_cls: 0.4793  decode.d2.loss_mask: 2.1484  decode.d2.loss_dice: 1.9393  decode.d3.loss_cls: 0.5690  decode.d3.loss_mask: 2.1580  decode.d3.loss_dice: 1.9670  decode.d4.loss_cls: 0.5026  decode.d4.loss_mask: 2.1610  decode.d4.loss_dice: 2.0039  decode.d5.loss_cls: 0.5841  decode.d5.loss_mask: 2.2570  decode.d5.loss_dice: 1.9359  decode.d6.loss_cls: 0.5663  decode.d6.loss_mask: 2.2703  decode.d6.loss_dice: 1.9000  decode.d7.loss_cls: 0.4640  decode.d7.loss_mask: 2.2307  decode.d7.loss_dice: 2.0337  decode.d8.loss_cls: 0.5496  decode.d8.loss_mask: 2.2443  decode.d8.loss_dice: 1.9544  mix_decode.loss_cls: 0.3328  mix_decode.loss_mask: 0.6654  mix_decode.loss_dice: 0.9806  mix_decode.d0.loss_cls: 0.3351  mix_decode.d0.loss_mask: 0.6390  mix_decode.d0.loss_dice: 1.0147  mix_decode.d1.loss_cls: 0.3201  mix_decode.d1.loss_mask: 0.6506  mix_decode.d1.loss_dice: 0.9552  mix_decode.d2.loss_cls: 0.3508  mix_decode.d2.loss_mask: 0.6620  mix_decode.d2.loss_dice: 0.9316  mix_decode.d3.loss_cls: 0.3251  mix_decode.d3.loss_mask: 0.6604  mix_decode.d3.loss_dice: 0.9692  mix_decode.d4.loss_cls: 0.3611  mix_decode.d4.loss_mask: 0.6819  mix_decode.d4.loss_dice: 0.9764  mix_decode.d5.loss_cls: 0.3619  mix_decode.d5.loss_mask: 0.6828  mix_decode.d5.loss_dice: 0.9805  mix_decode.d6.loss_cls: 0.3750  mix_decode.d6.loss_mask: 0.6482  mix_decode.d6.loss_dice: 0.9737  mix_decode.d7.loss_cls: 0.4219  mix_decode.d7.loss_mask: 0.6641  mix_decode.d7.loss_dice: 0.9473  mix_decode.d8.loss_cls: 0.3937  mix_decode.d8.loss_mask: 0.6873  mix_decode.d8.loss_dice: 0.9741
2025/03/29 13:04:48 - mmengine - INFO - Iter(train) [ 5500/20000]  base_lr: 7.4873e-05 lr: 7.4873e-05  eta: 2:52:23  time: 1.1427  data_time: 0.0224  memory: 11209  loss: 67.9626  decode.loss_cls: 0.6334  decode.loss_mask: 2.3124  decode.loss_dice: 1.9535  decode.d0.loss_cls: 0.6763  decode.d0.loss_mask: 2.2331  decode.d0.loss_dice: 1.8757  decode.d1.loss_cls: 0.5059  decode.d1.loss_mask: 2.2289  decode.d1.loss_dice: 1.8805  decode.d2.loss_cls: 0.5882  decode.d2.loss_mask: 2.2276  decode.d2.loss_dice: 1.8503  decode.d3.loss_cls: 0.4946  decode.d3.loss_mask: 2.3083  decode.d3.loss_dice: 1.8356  decode.d4.loss_cls: 0.4990  decode.d4.loss_mask: 2.2976  decode.d4.loss_dice: 1.8809  decode.d5.loss_cls: 0.4453  decode.d5.loss_mask: 2.4072  decode.d5.loss_dice: 1.8838  decode.d6.loss_cls: 0.5884  decode.d6.loss_mask: 2.2565  decode.d6.loss_dice: 1.9753  decode.d7.loss_cls: 0.6461  decode.d7.loss_mask: 2.2851  decode.d7.loss_dice: 1.8979  decode.d8.loss_cls: 0.6439  decode.d8.loss_mask: 2.2754  decode.d8.loss_dice: 1.9120  mix_decode.loss_cls: 0.3099  mix_decode.loss_mask: 0.7834  mix_decode.loss_dice: 0.9161  mix_decode.d0.loss_cls: 0.3027  mix_decode.d0.loss_mask: 0.7793  mix_decode.d0.loss_dice: 0.9494  mix_decode.d1.loss_cls: 0.2625  mix_decode.d1.loss_mask: 0.7908  mix_decode.d1.loss_dice: 0.9423  mix_decode.d2.loss_cls: 0.2753  mix_decode.d2.loss_mask: 0.7871  mix_decode.d2.loss_dice: 0.9252  mix_decode.d3.loss_cls: 0.3067  mix_decode.d3.loss_mask: 0.7899  mix_decode.d3.loss_dice: 0.9070  mix_decode.d4.loss_cls: 0.2874  mix_decode.d4.loss_mask: 0.8072  mix_decode.d4.loss_dice: 0.9555  mix_decode.d5.loss_cls: 0.2775  mix_decode.d5.loss_mask: 0.8483  mix_decode.d5.loss_dice: 0.9668  mix_decode.d6.loss_cls: 0.3881  mix_decode.d6.loss_mask: 0.7844  mix_decode.d6.loss_dice: 1.0029  mix_decode.d7.loss_cls: 0.3533  mix_decode.d7.loss_mask: 0.8074  mix_decode.d7.loss_dice: 0.9207  mix_decode.d8.loss_cls: 0.3198  mix_decode.d8.loss_mask: 0.7720  mix_decode.d8.loss_dice: 0.9446
2025/03/29 13:05:45 - mmengine - INFO - Iter(train) [ 5550/20000]  base_lr: 7.4640e-05 lr: 7.4640e-05  eta: 2:52:43  time: 1.1456  data_time: 0.0220  memory: 11210  loss: 61.0683  decode.loss_cls: 0.4231  decode.loss_mask: 1.8730  decode.loss_dice: 1.7320  decode.d0.loss_cls: 0.4806  decode.d0.loss_mask: 1.9333  decode.d0.loss_dice: 1.7589  decode.d1.loss_cls: 0.4106  decode.d1.loss_mask: 1.8858  decode.d1.loss_dice: 1.7065  decode.d2.loss_cls: 0.3506  decode.d2.loss_mask: 1.8815  decode.d2.loss_dice: 1.7530  decode.d3.loss_cls: 0.3742  decode.d3.loss_mask: 1.8720  decode.d3.loss_dice: 1.7512  decode.d4.loss_cls: 0.3832  decode.d4.loss_mask: 1.8425  decode.d4.loss_dice: 1.7234  decode.d5.loss_cls: 0.3237  decode.d5.loss_mask: 1.9087  decode.d5.loss_dice: 1.7449  decode.d6.loss_cls: 0.4116  decode.d6.loss_mask: 1.8996  decode.d6.loss_dice: 1.7404  decode.d7.loss_cls: 0.3793  decode.d7.loss_mask: 1.8925  decode.d7.loss_dice: 1.7758  decode.d8.loss_cls: 0.4206  decode.d8.loss_mask: 1.8715  decode.d8.loss_dice: 1.7497  mix_decode.loss_cls: 0.3178  mix_decode.loss_mask: 0.7287  mix_decode.loss_dice: 1.0531  mix_decode.d0.loss_cls: 0.3065  mix_decode.d0.loss_mask: 0.7040  mix_decode.d0.loss_dice: 1.0883  mix_decode.d1.loss_cls: 0.2877  mix_decode.d1.loss_mask: 0.7307  mix_decode.d1.loss_dice: 1.0378  mix_decode.d2.loss_cls: 0.3088  mix_decode.d2.loss_mask: 0.7196  mix_decode.d2.loss_dice: 1.0110  mix_decode.d3.loss_cls: 0.3159  mix_decode.d3.loss_mask: 0.7336  mix_decode.d3.loss_dice: 1.0599  mix_decode.d4.loss_cls: 0.2994  mix_decode.d4.loss_mask: 0.7115  mix_decode.d4.loss_dice: 1.0344  mix_decode.d5.loss_cls: 0.3262  mix_decode.d5.loss_mask: 0.7332  mix_decode.d5.loss_dice: 1.0451  mix_decode.d6.loss_cls: 0.3284  mix_decode.d6.loss_mask: 0.7151  mix_decode.d6.loss_dice: 1.0255  mix_decode.d7.loss_cls: 0.3358  mix_decode.d7.loss_mask: 0.7178  mix_decode.d7.loss_dice: 1.0352  mix_decode.d8.loss_cls: 0.3424  mix_decode.d8.loss_mask: 0.7251  mix_decode.d8.loss_dice: 1.0359
2025/03/29 13:06:42 - mmengine - INFO - Iter(train) [ 5600/20000]  base_lr: 7.4408e-05 lr: 7.4408e-05  eta: 2:53:02  time: 1.1467  data_time: 0.0224  memory: 11221  loss: 65.3499  decode.loss_cls: 0.2987  decode.loss_mask: 2.1937  decode.loss_dice: 1.8006  decode.d0.loss_cls: 0.5764  decode.d0.loss_mask: 2.2057  decode.d0.loss_dice: 1.7573  decode.d1.loss_cls: 0.4511  decode.d1.loss_mask: 2.1482  decode.d1.loss_dice: 1.7278  decode.d2.loss_cls: 0.4552  decode.d2.loss_mask: 2.1389  decode.d2.loss_dice: 1.7440  decode.d3.loss_cls: 0.4143  decode.d3.loss_mask: 2.1369  decode.d3.loss_dice: 1.7469  decode.d4.loss_cls: 0.4547  decode.d4.loss_mask: 2.1618  decode.d4.loss_dice: 1.7459  decode.d5.loss_cls: 0.3569  decode.d5.loss_mask: 2.2000  decode.d5.loss_dice: 1.8288  decode.d6.loss_cls: 0.5051  decode.d6.loss_mask: 2.1198  decode.d6.loss_dice: 1.7868  decode.d7.loss_cls: 0.3382  decode.d7.loss_mask: 2.1102  decode.d7.loss_dice: 1.8005  decode.d8.loss_cls: 0.3669  decode.d8.loss_mask: 2.1431  decode.d8.loss_dice: 1.8101  mix_decode.loss_cls: 0.3150  mix_decode.loss_mask: 0.8665  mix_decode.loss_dice: 1.0023  mix_decode.d0.loss_cls: 0.3071  mix_decode.d0.loss_mask: 0.8615  mix_decode.d0.loss_dice: 0.9960  mix_decode.d1.loss_cls: 0.2272  mix_decode.d1.loss_mask: 0.8523  mix_decode.d1.loss_dice: 1.0293  mix_decode.d2.loss_cls: 0.2404  mix_decode.d2.loss_mask: 0.8600  mix_decode.d2.loss_dice: 1.0277  mix_decode.d3.loss_cls: 0.3036  mix_decode.d3.loss_mask: 0.8332  mix_decode.d3.loss_dice: 1.0206  mix_decode.d4.loss_cls: 0.3223  mix_decode.d4.loss_mask: 0.8762  mix_decode.d4.loss_dice: 0.9962  mix_decode.d5.loss_cls: 0.2851  mix_decode.d5.loss_mask: 0.8970  mix_decode.d5.loss_dice: 1.0146  mix_decode.d6.loss_cls: 0.3978  mix_decode.d6.loss_mask: 0.8407  mix_decode.d6.loss_dice: 0.9913  mix_decode.d7.loss_cls: 0.3307  mix_decode.d7.loss_mask: 0.8605  mix_decode.d7.loss_dice: 1.0048  mix_decode.d8.loss_cls: 0.3538  mix_decode.d8.loss_mask: 0.8909  mix_decode.d8.loss_dice: 1.0205
2025/03/29 13:07:40 - mmengine - INFO - Iter(train) [ 5650/20000]  base_lr: 7.4175e-05 lr: 7.4175e-05  eta: 2:53:20  time: 1.1419  data_time: 0.0225  memory: 11211  loss: 65.9393  decode.loss_cls: 0.6089  decode.loss_mask: 1.8247  decode.loss_dice: 1.8865  decode.d0.loss_cls: 0.5461  decode.d0.loss_mask: 1.8781  decode.d0.loss_dice: 2.0346  decode.d1.loss_cls: 0.5417  decode.d1.loss_mask: 1.9099  decode.d1.loss_dice: 1.9466  decode.d2.loss_cls: 0.6559  decode.d2.loss_mask: 1.8192  decode.d2.loss_dice: 1.9157  decode.d3.loss_cls: 0.6294  decode.d3.loss_mask: 1.8743  decode.d3.loss_dice: 1.8910  decode.d4.loss_cls: 0.5327  decode.d4.loss_mask: 1.8721  decode.d4.loss_dice: 1.9074  decode.d5.loss_cls: 0.5239  decode.d5.loss_mask: 1.8829  decode.d5.loss_dice: 1.9021  decode.d6.loss_cls: 0.6516  decode.d6.loss_mask: 1.7832  decode.d6.loss_dice: 1.9008  decode.d7.loss_cls: 0.6303  decode.d7.loss_mask: 1.9142  decode.d7.loss_dice: 1.9338  decode.d8.loss_cls: 0.5464  decode.d8.loss_mask: 1.9248  decode.d8.loss_dice: 1.9438  mix_decode.loss_cls: 0.4015  mix_decode.loss_mask: 0.7415  mix_decode.loss_dice: 1.0413  mix_decode.d0.loss_cls: 0.3378  mix_decode.d0.loss_mask: 0.7642  mix_decode.d0.loss_dice: 1.1006  mix_decode.d1.loss_cls: 0.3665  mix_decode.d1.loss_mask: 0.7580  mix_decode.d1.loss_dice: 1.0630  mix_decode.d2.loss_cls: 0.4563  mix_decode.d2.loss_mask: 0.7084  mix_decode.d2.loss_dice: 1.0447  mix_decode.d3.loss_cls: 0.4516  mix_decode.d3.loss_mask: 0.7314  mix_decode.d3.loss_dice: 1.0269  mix_decode.d4.loss_cls: 0.3595  mix_decode.d4.loss_mask: 0.7638  mix_decode.d4.loss_dice: 1.0620  mix_decode.d5.loss_cls: 0.4376  mix_decode.d5.loss_mask: 0.7516  mix_decode.d5.loss_dice: 1.0292  mix_decode.d6.loss_cls: 0.4989  mix_decode.d6.loss_mask: 0.7672  mix_decode.d6.loss_dice: 1.0198  mix_decode.d7.loss_cls: 0.4752  mix_decode.d7.loss_mask: 0.7529  mix_decode.d7.loss_dice: 1.0377  mix_decode.d8.loss_cls: 0.4803  mix_decode.d8.loss_mask: 0.6991  mix_decode.d8.loss_dice: 0.9982
2025/03/29 13:08:37 - mmengine - INFO - Iter(train) [ 5700/20000]  base_lr: 7.3943e-05 lr: 7.3943e-05  eta: 2:53:37  time: 1.1469  data_time: 0.0224  memory: 11209  loss: 65.2770  decode.loss_cls: 0.6077  decode.loss_mask: 1.9222  decode.loss_dice: 1.9408  decode.d0.loss_cls: 0.6475  decode.d0.loss_mask: 1.9534  decode.d0.loss_dice: 1.9851  decode.d1.loss_cls: 0.4705  decode.d1.loss_mask: 1.9024  decode.d1.loss_dice: 1.8463  decode.d2.loss_cls: 0.5242  decode.d2.loss_mask: 1.9463  decode.d2.loss_dice: 1.8892  decode.d3.loss_cls: 0.5946  decode.d3.loss_mask: 1.9359  decode.d3.loss_dice: 1.8645  decode.d4.loss_cls: 0.5890  decode.d4.loss_mask: 1.9767  decode.d4.loss_dice: 1.9538  decode.d5.loss_cls: 0.5851  decode.d5.loss_mask: 2.0090  decode.d5.loss_dice: 2.0151  decode.d6.loss_cls: 0.6564  decode.d6.loss_mask: 1.9435  decode.d6.loss_dice: 1.9308  decode.d7.loss_cls: 0.5747  decode.d7.loss_mask: 2.0043  decode.d7.loss_dice: 1.9561  decode.d8.loss_cls: 0.5485  decode.d8.loss_mask: 1.9443  decode.d8.loss_dice: 1.8481  mix_decode.loss_cls: 0.3799  mix_decode.loss_mask: 0.7301  mix_decode.loss_dice: 0.9715  mix_decode.d0.loss_cls: 0.3814  mix_decode.d0.loss_mask: 0.7109  mix_decode.d0.loss_dice: 0.9895  mix_decode.d1.loss_cls: 0.3083  mix_decode.d1.loss_mask: 0.6994  mix_decode.d1.loss_dice: 0.9698  mix_decode.d2.loss_cls: 0.3129  mix_decode.d2.loss_mask: 0.7502  mix_decode.d2.loss_dice: 0.9883  mix_decode.d3.loss_cls: 0.3436  mix_decode.d3.loss_mask: 0.7232  mix_decode.d3.loss_dice: 0.9723  mix_decode.d4.loss_cls: 0.3376  mix_decode.d4.loss_mask: 0.7476  mix_decode.d4.loss_dice: 1.0097  mix_decode.d5.loss_cls: 0.3461  mix_decode.d5.loss_mask: 0.7754  mix_decode.d5.loss_dice: 0.9954  mix_decode.d6.loss_cls: 0.3609  mix_decode.d6.loss_mask: 0.7560  mix_decode.d6.loss_dice: 0.9757  mix_decode.d7.loss_cls: 0.3635  mix_decode.d7.loss_mask: 0.7513  mix_decode.d7.loss_dice: 0.9837  mix_decode.d8.loss_cls: 0.3541  mix_decode.d8.loss_mask: 0.7252  mix_decode.d8.loss_dice: 0.9978
2025/03/29 13:09:34 - mmengine - INFO - Iter(train) [ 5750/20000]  base_lr: 7.3710e-05 lr: 7.3710e-05  eta: 2:53:52  time: 1.1469  data_time: 0.0224  memory: 11213  loss: 64.9708  decode.loss_cls: 0.5816  decode.loss_mask: 2.0369  decode.loss_dice: 1.8711  decode.d0.loss_cls: 0.6000  decode.d0.loss_mask: 1.9665  decode.d0.loss_dice: 1.9053  decode.d1.loss_cls: 0.5507  decode.d1.loss_mask: 1.9467  decode.d1.loss_dice: 1.8274  decode.d2.loss_cls: 0.5210  decode.d2.loss_mask: 1.9861  decode.d2.loss_dice: 1.8443  decode.d3.loss_cls: 0.5527  decode.d3.loss_mask: 1.9546  decode.d3.loss_dice: 1.8515  decode.d4.loss_cls: 0.5698  decode.d4.loss_mask: 1.9855  decode.d4.loss_dice: 1.8567  decode.d5.loss_cls: 0.5970  decode.d5.loss_mask: 2.0092  decode.d5.loss_dice: 1.8583  decode.d6.loss_cls: 0.6436  decode.d6.loss_mask: 1.9793  decode.d6.loss_dice: 1.8700  decode.d7.loss_cls: 0.5522  decode.d7.loss_mask: 1.9398  decode.d7.loss_dice: 1.8745  decode.d8.loss_cls: 0.5561  decode.d8.loss_mask: 2.0080  decode.d8.loss_dice: 1.9476  mix_decode.loss_cls: 0.4190  mix_decode.loss_mask: 0.7058  mix_decode.loss_dice: 0.9435  mix_decode.d0.loss_cls: 0.3888  mix_decode.d0.loss_mask: 0.7184  mix_decode.d0.loss_dice: 0.9829  mix_decode.d1.loss_cls: 0.3300  mix_decode.d1.loss_mask: 0.7314  mix_decode.d1.loss_dice: 0.9835  mix_decode.d2.loss_cls: 0.3773  mix_decode.d2.loss_mask: 0.7281  mix_decode.d2.loss_dice: 0.9761  mix_decode.d3.loss_cls: 0.4042  mix_decode.d3.loss_mask: 0.7034  mix_decode.d3.loss_dice: 0.9494  mix_decode.d4.loss_cls: 0.4166  mix_decode.d4.loss_mask: 0.7055  mix_decode.d4.loss_dice: 0.9718  mix_decode.d5.loss_cls: 0.3857  mix_decode.d5.loss_mask: 0.7168  mix_decode.d5.loss_dice: 0.9874  mix_decode.d6.loss_cls: 0.4689  mix_decode.d6.loss_mask: 0.6899  mix_decode.d6.loss_dice: 0.9578  mix_decode.d7.loss_cls: 0.3808  mix_decode.d7.loss_mask: 0.7223  mix_decode.d7.loss_dice: 0.9666  mix_decode.d8.loss_cls: 0.3520  mix_decode.d8.loss_mask: 0.7115  mix_decode.d8.loss_dice: 0.9516
2025/03/29 13:10:32 - mmengine - INFO - Iter(train) [ 5800/20000]  base_lr: 7.3477e-05 lr: 7.3477e-05  eta: 2:54:06  time: 1.1453  data_time: 0.0223  memory: 11209  loss: 67.2366  decode.loss_cls: 0.3910  decode.loss_mask: 2.3354  decode.loss_dice: 1.9715  decode.d0.loss_cls: 0.5948  decode.d0.loss_mask: 2.2056  decode.d0.loss_dice: 1.9380  decode.d1.loss_cls: 0.4569  decode.d1.loss_mask: 2.2890  decode.d1.loss_dice: 1.8942  decode.d2.loss_cls: 0.3911  decode.d2.loss_mask: 2.2880  decode.d2.loss_dice: 1.8801  decode.d3.loss_cls: 0.4178  decode.d3.loss_mask: 2.3154  decode.d3.loss_dice: 1.8461  decode.d4.loss_cls: 0.4371  decode.d4.loss_mask: 2.2997  decode.d4.loss_dice: 1.8578  decode.d5.loss_cls: 0.4222  decode.d5.loss_mask: 2.3261  decode.d5.loss_dice: 1.9321  decode.d6.loss_cls: 0.5165  decode.d6.loss_mask: 2.2019  decode.d6.loss_dice: 1.8628  decode.d7.loss_cls: 0.4479  decode.d7.loss_mask: 2.2768  decode.d7.loss_dice: 1.9010  decode.d8.loss_cls: 0.3463  decode.d8.loss_mask: 2.3345  decode.d8.loss_dice: 1.9322  mix_decode.loss_cls: 0.4337  mix_decode.loss_mask: 0.7297  mix_decode.loss_dice: 1.0125  mix_decode.d0.loss_cls: 0.3365  mix_decode.d0.loss_mask: 0.7030  mix_decode.d0.loss_dice: 1.0121  mix_decode.d1.loss_cls: 0.3518  mix_decode.d1.loss_mask: 0.7055  mix_decode.d1.loss_dice: 0.9582  mix_decode.d2.loss_cls: 0.4203  mix_decode.d2.loss_mask: 0.7038  mix_decode.d2.loss_dice: 0.9703  mix_decode.d3.loss_cls: 0.3911  mix_decode.d3.loss_mask: 0.6983  mix_decode.d3.loss_dice: 0.9772  mix_decode.d4.loss_cls: 0.4105  mix_decode.d4.loss_mask: 0.6862  mix_decode.d4.loss_dice: 0.9761  mix_decode.d5.loss_cls: 0.4378  mix_decode.d5.loss_mask: 0.7089  mix_decode.d5.loss_dice: 0.9729  mix_decode.d6.loss_cls: 0.4735  mix_decode.d6.loss_mask: 0.6779  mix_decode.d6.loss_dice: 0.9760  mix_decode.d7.loss_cls: 0.4459  mix_decode.d7.loss_mask: 0.6960  mix_decode.d7.loss_dice: 0.9831  mix_decode.d8.loss_cls: 0.4292  mix_decode.d8.loss_mask: 0.6793  mix_decode.d8.loss_dice: 0.9696
2025/03/29 13:11:29 - mmengine - INFO - Iter(train) [ 5850/20000]  base_lr: 7.3244e-05 lr: 7.3244e-05  eta: 2:54:18  time: 1.1446  data_time: 0.0224  memory: 11214  loss: 61.8956  decode.loss_cls: 0.2943  decode.loss_mask: 2.0529  decode.loss_dice: 1.8884  decode.d0.loss_cls: 0.3730  decode.d0.loss_mask: 2.0410  decode.d0.loss_dice: 1.9193  decode.d1.loss_cls: 0.3016  decode.d1.loss_mask: 2.0086  decode.d1.loss_dice: 1.8593  decode.d2.loss_cls: 0.3724  decode.d2.loss_mask: 2.0373  decode.d2.loss_dice: 1.8608  decode.d3.loss_cls: 0.3574  decode.d3.loss_mask: 1.9923  decode.d3.loss_dice: 1.8660  decode.d4.loss_cls: 0.3243  decode.d4.loss_mask: 2.0857  decode.d4.loss_dice: 1.9170  decode.d5.loss_cls: 0.3155  decode.d5.loss_mask: 2.0502  decode.d5.loss_dice: 1.9220  decode.d6.loss_cls: 0.3703  decode.d6.loss_mask: 2.0406  decode.d6.loss_dice: 1.9122  decode.d7.loss_cls: 0.3841  decode.d7.loss_mask: 2.0356  decode.d7.loss_dice: 1.8626  decode.d8.loss_cls: 0.3212  decode.d8.loss_mask: 2.0521  decode.d8.loss_dice: 1.9033  mix_decode.loss_cls: 0.3184  mix_decode.loss_mask: 0.7822  mix_decode.loss_dice: 0.8543  mix_decode.d0.loss_cls: 0.3218  mix_decode.d0.loss_mask: 0.7508  mix_decode.d0.loss_dice: 0.8960  mix_decode.d1.loss_cls: 0.2543  mix_decode.d1.loss_mask: 0.7621  mix_decode.d1.loss_dice: 0.8217  mix_decode.d2.loss_cls: 0.2904  mix_decode.d2.loss_mask: 0.7919  mix_decode.d2.loss_dice: 0.8119  mix_decode.d3.loss_cls: 0.2814  mix_decode.d3.loss_mask: 0.7546  mix_decode.d3.loss_dice: 0.8101  mix_decode.d4.loss_cls: 0.3219  mix_decode.d4.loss_mask: 0.7692  mix_decode.d4.loss_dice: 0.8431  mix_decode.d5.loss_cls: 0.3338  mix_decode.d5.loss_mask: 0.7629  mix_decode.d5.loss_dice: 0.8162  mix_decode.d6.loss_cls: 0.3501  mix_decode.d6.loss_mask: 0.7460  mix_decode.d6.loss_dice: 0.8220  mix_decode.d7.loss_cls: 0.3574  mix_decode.d7.loss_mask: 0.7510  mix_decode.d7.loss_dice: 0.8307  mix_decode.d8.loss_cls: 0.3255  mix_decode.d8.loss_mask: 0.7842  mix_decode.d8.loss_dice: 0.8586
2025/03/29 13:12:26 - mmengine - INFO - Iter(train) [ 5900/20000]  base_lr: 7.3011e-05 lr: 7.3011e-05  eta: 2:54:30  time: 1.1536  data_time: 0.0235  memory: 11211  loss: 65.7917  decode.loss_cls: 0.8039  decode.loss_mask: 1.7446  decode.loss_dice: 1.8431  decode.d0.loss_cls: 0.7027  decode.d0.loss_mask: 1.8821  decode.d0.loss_dice: 2.0135  decode.d1.loss_cls: 0.7172  decode.d1.loss_mask: 1.8923  decode.d1.loss_dice: 1.9422  decode.d2.loss_cls: 0.6506  decode.d2.loss_mask: 1.8660  decode.d2.loss_dice: 1.9199  decode.d3.loss_cls: 0.7250  decode.d3.loss_mask: 1.8260  decode.d3.loss_dice: 1.9061  decode.d4.loss_cls: 0.6497  decode.d4.loss_mask: 1.8529  decode.d4.loss_dice: 1.8670  decode.d5.loss_cls: 0.6753  decode.d5.loss_mask: 1.8972  decode.d5.loss_dice: 2.0204  decode.d6.loss_cls: 0.8186  decode.d6.loss_mask: 1.8376  decode.d6.loss_dice: 1.9322  decode.d7.loss_cls: 0.7359  decode.d7.loss_mask: 1.9223  decode.d7.loss_dice: 1.9869  decode.d8.loss_cls: 0.7580  decode.d8.loss_mask: 1.8798  decode.d8.loss_dice: 1.9186  mix_decode.loss_cls: 0.4161  mix_decode.loss_mask: 0.7331  mix_decode.loss_dice: 0.8985  mix_decode.d0.loss_cls: 0.3883  mix_decode.d0.loss_mask: 0.7415  mix_decode.d0.loss_dice: 0.9410  mix_decode.d1.loss_cls: 0.3754  mix_decode.d1.loss_mask: 0.7423  mix_decode.d1.loss_dice: 0.9033  mix_decode.d2.loss_cls: 0.3457  mix_decode.d2.loss_mask: 0.7799  mix_decode.d2.loss_dice: 0.9107  mix_decode.d3.loss_cls: 0.3606  mix_decode.d3.loss_mask: 0.7871  mix_decode.d3.loss_dice: 0.8889  mix_decode.d4.loss_cls: 0.3983  mix_decode.d4.loss_mask: 0.7594  mix_decode.d4.loss_dice: 0.8979  mix_decode.d5.loss_cls: 0.4408  mix_decode.d5.loss_mask: 0.7171  mix_decode.d5.loss_dice: 0.9021  mix_decode.d6.loss_cls: 0.4458  mix_decode.d6.loss_mask: 0.7517  mix_decode.d6.loss_dice: 0.9044  mix_decode.d7.loss_cls: 0.4202  mix_decode.d7.loss_mask: 0.7602  mix_decode.d7.loss_dice: 0.9085  mix_decode.d8.loss_cls: 0.4033  mix_decode.d8.loss_mask: 0.7642  mix_decode.d8.loss_dice: 0.9179
2025/03/29 13:13:23 - mmengine - INFO - Iter(train) [ 5950/20000]  base_lr: 7.2778e-05 lr: 7.2778e-05  eta: 2:54:41  time: 1.1433  data_time: 0.0219  memory: 11223  loss: 64.5154  decode.loss_cls: 0.4805  decode.loss_mask: 2.0965  decode.loss_dice: 1.8183  decode.d0.loss_cls: 0.5524  decode.d0.loss_mask: 2.0912  decode.d0.loss_dice: 1.8603  decode.d1.loss_cls: 0.4290  decode.d1.loss_mask: 2.0872  decode.d1.loss_dice: 1.8352  decode.d2.loss_cls: 0.4252  decode.d2.loss_mask: 2.0655  decode.d2.loss_dice: 1.8459  decode.d3.loss_cls: 0.4506  decode.d3.loss_mask: 2.0600  decode.d3.loss_dice: 1.8170  decode.d4.loss_cls: 0.4101  decode.d4.loss_mask: 2.0897  decode.d4.loss_dice: 1.8461  decode.d5.loss_cls: 0.4237  decode.d5.loss_mask: 2.0963  decode.d5.loss_dice: 1.8376  decode.d6.loss_cls: 0.4430  decode.d6.loss_mask: 2.0856  decode.d6.loss_dice: 1.8266  decode.d7.loss_cls: 0.4254  decode.d7.loss_mask: 2.1068  decode.d7.loss_dice: 1.8542  decode.d8.loss_cls: 0.4894  decode.d8.loss_mask: 2.0582  decode.d8.loss_dice: 1.7934  mix_decode.loss_cls: 0.3821  mix_decode.loss_mask: 0.7210  mix_decode.loss_dice: 0.9198  mix_decode.d0.loss_cls: 0.4186  mix_decode.d0.loss_mask: 0.7187  mix_decode.d0.loss_dice: 1.0221  mix_decode.d1.loss_cls: 0.3970  mix_decode.d1.loss_mask: 0.7033  mix_decode.d1.loss_dice: 0.9413  mix_decode.d2.loss_cls: 0.4298  mix_decode.d2.loss_mask: 0.7139  mix_decode.d2.loss_dice: 0.9196  mix_decode.d3.loss_cls: 0.4120  mix_decode.d3.loss_mask: 0.7101  mix_decode.d3.loss_dice: 0.9253  mix_decode.d4.loss_cls: 0.4139  mix_decode.d4.loss_mask: 0.7200  mix_decode.d4.loss_dice: 0.9690  mix_decode.d5.loss_cls: 0.4405  mix_decode.d5.loss_mask: 0.7115  mix_decode.d5.loss_dice: 0.9486  mix_decode.d6.loss_cls: 0.4817  mix_decode.d6.loss_mask: 0.7190  mix_decode.d6.loss_dice: 0.9441  mix_decode.d7.loss_cls: 0.4329  mix_decode.d7.loss_mask: 0.7236  mix_decode.d7.loss_dice: 0.9186  mix_decode.d8.loss_cls: 0.3940  mix_decode.d8.loss_mask: 0.7138  mix_decode.d8.loss_dice: 0.9487
2025/03/29 13:14:21 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 13:14:21 - mmengine - INFO - Iter(train) [ 6000/20000]  base_lr: 7.2545e-05 lr: 7.2545e-05  eta: 2:54:50  time: 1.1453  data_time: 0.0228  memory: 11213  loss: 61.6935  decode.loss_cls: 0.3902  decode.loss_mask: 2.1066  decode.loss_dice: 1.7680  decode.d0.loss_cls: 0.5017  decode.d0.loss_mask: 2.1720  decode.d0.loss_dice: 1.8824  decode.d1.loss_cls: 0.3613  decode.d1.loss_mask: 2.0956  decode.d1.loss_dice: 1.8081  decode.d2.loss_cls: 0.3306  decode.d2.loss_mask: 2.1910  decode.d2.loss_dice: 1.7780  decode.d3.loss_cls: 0.3675  decode.d3.loss_mask: 2.0973  decode.d3.loss_dice: 1.7686  decode.d4.loss_cls: 0.3716  decode.d4.loss_mask: 2.1162  decode.d4.loss_dice: 1.7665  decode.d5.loss_cls: 0.3863  decode.d5.loss_mask: 2.1638  decode.d5.loss_dice: 1.8102  decode.d6.loss_cls: 0.4039  decode.d6.loss_mask: 2.1171  decode.d6.loss_dice: 1.7756  decode.d7.loss_cls: 0.3445  decode.d7.loss_mask: 2.1466  decode.d7.loss_dice: 1.7941  decode.d8.loss_cls: 0.4275  decode.d8.loss_mask: 2.1967  decode.d8.loss_dice: 1.7991  mix_decode.loss_cls: 0.3048  mix_decode.loss_mask: 0.6708  mix_decode.loss_dice: 0.8742  mix_decode.d0.loss_cls: 0.3229  mix_decode.d0.loss_mask: 0.6875  mix_decode.d0.loss_dice: 0.9520  mix_decode.d1.loss_cls: 0.2449  mix_decode.d1.loss_mask: 0.6692  mix_decode.d1.loss_dice: 0.8707  mix_decode.d2.loss_cls: 0.3101  mix_decode.d2.loss_mask: 0.6567  mix_decode.d2.loss_dice: 0.8827  mix_decode.d3.loss_cls: 0.2651  mix_decode.d3.loss_mask: 0.6753  mix_decode.d3.loss_dice: 0.8781  mix_decode.d4.loss_cls: 0.2441  mix_decode.d4.loss_mask: 0.6704  mix_decode.d4.loss_dice: 0.8860  mix_decode.d5.loss_cls: 0.2940  mix_decode.d5.loss_mask: 0.6561  mix_decode.d5.loss_dice: 0.8579  mix_decode.d6.loss_cls: 0.3172  mix_decode.d6.loss_mask: 0.6558  mix_decode.d6.loss_dice: 0.8609  mix_decode.d7.loss_cls: 0.3334  mix_decode.d7.loss_mask: 0.6666  mix_decode.d7.loss_dice: 0.8573  mix_decode.d8.loss_cls: 0.3360  mix_decode.d8.loss_mask: 0.6745  mix_decode.d8.loss_dice: 0.8798
2025/03/29 13:14:21 - mmengine - INFO - Saving checkpoint at 6000 iterations
2025/03/29 13:14:26 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:05:58  time: 0.0908  data_time: 0.0018  memory: 3081  
2025/03/29 13:14:31 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:52  time: 0.0910  data_time: 0.0017  memory: 3081  
2025/03/29 13:14:35 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:47  time: 0.0908  data_time: 0.0017  memory: 3081  
2025/03/29 13:14:40 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:42  time: 0.0908  data_time: 0.0017  memory: 3081  
2025/03/29 13:14:44 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:38  time: 0.0909  data_time: 0.0018  memory: 3081  
2025/03/29 13:14:49 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:33  time: 0.0909  data_time: 0.0018  memory: 3081  
2025/03/29 13:14:54 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:29  time: 0.0909  data_time: 0.0017  memory: 3081  
2025/03/29 13:14:58 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:24  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:03 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:20  time: 0.0909  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:07 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:15  time: 0.0909  data_time: 0.0017  memory: 3081  
2025/03/29 13:15:12 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:10  time: 0.0910  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:16 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:06  time: 0.0910  data_time: 0.0017  memory: 3081  
2025/03/29 13:15:21 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:01  time: 0.0910  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:25 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:57  time: 0.0910  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:30 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:52  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:35 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:48  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:15:39 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:43  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:44 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:39  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:48 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:34  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 13:15:53 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:30  time: 0.0911  data_time: 0.0017  memory: 3081  
2025/03/29 13:15:57 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:25  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:02 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:21  time: 0.0934  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:07 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:16  time: 0.0910  data_time: 0.0017  memory: 3081  
2025/03/29 13:16:11 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:12  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:16 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:07  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:20 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:02  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:25 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:58  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:29 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:53  time: 0.0910  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:34 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:49  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:38 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:44  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:43 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:40  time: 0.0913  data_time: 0.0017  memory: 3081  
2025/03/29 13:16:48 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:35  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 13:16:52 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:31  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:16:57 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:26  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:01 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:22  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:06 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:17  time: 0.0930  data_time: 0.0020  memory: 3081  
2025/03/29 13:17:11 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:13  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:15 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:08  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:17:20 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:03  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:24 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:02:59  time: 0.0910  data_time: 0.0017  memory: 3081  
2025/03/29 13:17:29 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:54  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:17:33 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:50  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:38 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:45  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:17:42 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:41  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:47 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:36  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:17:52 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 13:17:56 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:27  time: 0.0913  data_time: 0.0017  memory: 3081  
2025/03/29 13:18:01 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:22  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:05 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:18  time: 0.0914  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:10 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:13  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:14 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 13:18:19 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:04  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 13:18:24 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0917  data_time: 0.0017  memory: 3081  
2025/03/29 13:18:28 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:55  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:33 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:37 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:46  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:42 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:41  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:47 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:51 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:32  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 13:18:56 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0916  data_time: 0.0017  memory: 3081  
2025/03/29 13:19:00 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:23  time: 0.0927  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:05 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0919  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:10 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:14  time: 0.0917  data_time: 0.0017  memory: 3081  
2025/03/29 13:19:14 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:19 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0918  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:23 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:00  time: 0.0943  data_time: 0.0021  memory: 3081  
2025/03/29 13:19:28 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0918  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:33 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:51  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:37 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:42 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:46 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 13:19:51 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:19:56 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:00 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0918  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:05 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:19  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:09 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0918  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:14 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:19 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:23 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0918  data_time: 0.0018  memory: 3081  
2025/03/29 13:20:25 - mmengine - INFO - per class results:
2025/03/29 13:20:25 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 50.33 | 71.56 |
|   building   | 42.25 | 44.39 |
|     road     | 38.97 | 40.64 |
|    water     | 64.93 | 77.83 |
|    barren    |  9.01 | 37.11 |
|    forest    | 27.96 | 31.09 |
| agricultural | 58.38 | 63.49 |
+--------------+-------+-------+
2025/03/29 13:20:25 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 64.7300  mIoU: 41.6900  mAcc: 52.3000  data_time: 0.0018  time: 0.0914
2025/03/29 13:20:25 - mmengine - INFO - The previous best checkpoint /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0/best_mIoU_iter_4000.pth is removed
2025/03/29 13:20:26 - mmengine - INFO - The best checkpoint with 41.6900 mIoU at 6000 iter is saved to best_mIoU_iter_6000.pth.
2025/03/29 13:21:24 - mmengine - INFO - Iter(train) [ 6050/20000]  base_lr: 7.2312e-05 lr: 7.2312e-05  eta: 2:55:03  time: 1.1487  data_time: 0.0226  memory: 11217  loss: 66.2776  decode.loss_cls: 0.3524  decode.loss_mask: 2.2063  decode.loss_dice: 2.0595  decode.d0.loss_cls: 0.5704  decode.d0.loss_mask: 2.1024  decode.d0.loss_dice: 2.0205  decode.d1.loss_cls: 0.3652  decode.d1.loss_mask: 2.1250  decode.d1.loss_dice: 2.0405  decode.d2.loss_cls: 0.3791  decode.d2.loss_mask: 2.1526  decode.d2.loss_dice: 2.0375  decode.d3.loss_cls: 0.3559  decode.d3.loss_mask: 2.1994  decode.d3.loss_dice: 2.0904  decode.d4.loss_cls: 0.3662  decode.d4.loss_mask: 2.1958  decode.d4.loss_dice: 2.0284  decode.d5.loss_cls: 0.3422  decode.d5.loss_mask: 2.1852  decode.d5.loss_dice: 2.0527  decode.d6.loss_cls: 0.4391  decode.d6.loss_mask: 2.1618  decode.d6.loss_dice: 2.0258  decode.d7.loss_cls: 0.3380  decode.d7.loss_mask: 2.1562  decode.d7.loss_dice: 2.0944  decode.d8.loss_cls: 0.3358  decode.d8.loss_mask: 2.2545  decode.d8.loss_dice: 2.0910  mix_decode.loss_cls: 0.4210  mix_decode.loss_mask: 0.7323  mix_decode.loss_dice: 0.8903  mix_decode.d0.loss_cls: 0.3670  mix_decode.d0.loss_mask: 0.7102  mix_decode.d0.loss_dice: 0.9488  mix_decode.d1.loss_cls: 0.3340  mix_decode.d1.loss_mask: 0.6893  mix_decode.d1.loss_dice: 0.9225  mix_decode.d2.loss_cls: 0.3912  mix_decode.d2.loss_mask: 0.6880  mix_decode.d2.loss_dice: 0.9083  mix_decode.d3.loss_cls: 0.4004  mix_decode.d3.loss_mask: 0.6859  mix_decode.d3.loss_dice: 0.9015  mix_decode.d4.loss_cls: 0.4378  mix_decode.d4.loss_mask: 0.6719  mix_decode.d4.loss_dice: 0.9061  mix_decode.d5.loss_cls: 0.4915  mix_decode.d5.loss_mask: 0.6847  mix_decode.d5.loss_dice: 0.9219  mix_decode.d6.loss_cls: 0.4082  mix_decode.d6.loss_mask: 0.6677  mix_decode.d6.loss_dice: 0.9051  mix_decode.d7.loss_cls: 0.4324  mix_decode.d7.loss_mask: 0.6643  mix_decode.d7.loss_dice: 0.8988  mix_decode.d8.loss_cls: 0.4904  mix_decode.d8.loss_mask: 0.6802  mix_decode.d8.loss_dice: 0.9015
2025/03/29 13:22:22 - mmengine - INFO - Iter(train) [ 6100/20000]  base_lr: 7.2079e-05 lr: 7.2079e-05  eta: 2:55:10  time: 1.1508  data_time: 0.0229  memory: 11223  loss: 60.3948  decode.loss_cls: 0.4642  decode.loss_mask: 1.8545  decode.loss_dice: 1.8488  decode.d0.loss_cls: 0.4785  decode.d0.loss_mask: 1.8374  decode.d0.loss_dice: 1.9420  decode.d1.loss_cls: 0.3312  decode.d1.loss_mask: 1.8509  decode.d1.loss_dice: 1.8961  decode.d2.loss_cls: 0.4974  decode.d2.loss_mask: 1.8402  decode.d2.loss_dice: 1.8262  decode.d3.loss_cls: 0.5276  decode.d3.loss_mask: 1.7916  decode.d3.loss_dice: 1.8208  decode.d4.loss_cls: 0.5071  decode.d4.loss_mask: 1.8658  decode.d4.loss_dice: 1.8440  decode.d5.loss_cls: 0.3840  decode.d5.loss_mask: 1.8466  decode.d5.loss_dice: 1.8584  decode.d6.loss_cls: 0.5096  decode.d6.loss_mask: 1.8144  decode.d6.loss_dice: 1.8257  decode.d7.loss_cls: 0.4432  decode.d7.loss_mask: 1.8462  decode.d7.loss_dice: 1.8421  decode.d8.loss_cls: 0.5374  decode.d8.loss_mask: 1.8687  decode.d8.loss_dice: 1.8276  mix_decode.loss_cls: 0.2934  mix_decode.loss_mask: 0.7007  mix_decode.loss_dice: 0.8748  mix_decode.d0.loss_cls: 0.2780  mix_decode.d0.loss_mask: 0.7050  mix_decode.d0.loss_dice: 0.9170  mix_decode.d1.loss_cls: 0.2379  mix_decode.d1.loss_mask: 0.6895  mix_decode.d1.loss_dice: 0.8877  mix_decode.d2.loss_cls: 0.2425  mix_decode.d2.loss_mask: 0.7079  mix_decode.d2.loss_dice: 0.8962  mix_decode.d3.loss_cls: 0.2818  mix_decode.d3.loss_mask: 0.6866  mix_decode.d3.loss_dice: 0.8768  mix_decode.d4.loss_cls: 0.3241  mix_decode.d4.loss_mask: 0.6896  mix_decode.d4.loss_dice: 0.9096  mix_decode.d5.loss_cls: 0.2946  mix_decode.d5.loss_mask: 0.6833  mix_decode.d5.loss_dice: 0.8846  mix_decode.d6.loss_cls: 0.3138  mix_decode.d6.loss_mask: 0.6902  mix_decode.d6.loss_dice: 0.8922  mix_decode.d7.loss_cls: 0.3124  mix_decode.d7.loss_mask: 0.7027  mix_decode.d7.loss_dice: 0.8825  mix_decode.d8.loss_cls: 0.3073  mix_decode.d8.loss_mask: 0.7058  mix_decode.d8.loss_dice: 0.8981
2025/03/29 13:23:19 - mmengine - INFO - Iter(train) [ 6150/20000]  base_lr: 7.1845e-05 lr: 7.1845e-05  eta: 2:55:17  time: 1.1497  data_time: 0.0224  memory: 11219  loss: 65.0230  decode.loss_cls: 0.3887  decode.loss_mask: 2.2150  decode.loss_dice: 1.8228  decode.d0.loss_cls: 0.4831  decode.d0.loss_mask: 2.1979  decode.d0.loss_dice: 1.9314  decode.d1.loss_cls: 0.3727  decode.d1.loss_mask: 2.1995  decode.d1.loss_dice: 1.8900  decode.d2.loss_cls: 0.4249  decode.d2.loss_mask: 2.2058  decode.d2.loss_dice: 1.8895  decode.d3.loss_cls: 0.4259  decode.d3.loss_mask: 2.1400  decode.d3.loss_dice: 1.8348  decode.d4.loss_cls: 0.4699  decode.d4.loss_mask: 2.1994  decode.d4.loss_dice: 1.8649  decode.d5.loss_cls: 0.5314  decode.d5.loss_mask: 2.1320  decode.d5.loss_dice: 1.8514  decode.d6.loss_cls: 0.4875  decode.d6.loss_mask: 2.1377  decode.d6.loss_dice: 1.8389  decode.d7.loss_cls: 0.4474  decode.d7.loss_mask: 2.1638  decode.d7.loss_dice: 1.8454  decode.d8.loss_cls: 0.4512  decode.d8.loss_mask: 2.1574  decode.d8.loss_dice: 1.8310  mix_decode.loss_cls: 0.3569  mix_decode.loss_mask: 0.7784  mix_decode.loss_dice: 0.8976  mix_decode.d0.loss_cls: 0.3033  mix_decode.d0.loss_mask: 0.7947  mix_decode.d0.loss_dice: 0.9484  mix_decode.d1.loss_cls: 0.2581  mix_decode.d1.loss_mask: 0.7932  mix_decode.d1.loss_dice: 0.9052  mix_decode.d2.loss_cls: 0.2909  mix_decode.d2.loss_mask: 0.7834  mix_decode.d2.loss_dice: 0.9222  mix_decode.d3.loss_cls: 0.2969  mix_decode.d3.loss_mask: 0.8331  mix_decode.d3.loss_dice: 0.9327  mix_decode.d4.loss_cls: 0.2957  mix_decode.d4.loss_mask: 0.8009  mix_decode.d4.loss_dice: 0.9108  mix_decode.d5.loss_cls: 0.3041  mix_decode.d5.loss_mask: 0.8081  mix_decode.d5.loss_dice: 0.9228  mix_decode.d6.loss_cls: 0.3143  mix_decode.d6.loss_mask: 0.7862  mix_decode.d6.loss_dice: 0.8872  mix_decode.d7.loss_cls: 0.3215  mix_decode.d7.loss_mask: 0.7853  mix_decode.d7.loss_dice: 0.9087  mix_decode.d8.loss_cls: 0.3242  mix_decode.d8.loss_mask: 0.7887  mix_decode.d8.loss_dice: 0.9382
2025/03/29 13:24:17 - mmengine - INFO - Iter(train) [ 6200/20000]  base_lr: 7.1612e-05 lr: 7.1612e-05  eta: 2:55:22  time: 1.1499  data_time: 0.0226  memory: 11205  loss: 63.2481  decode.loss_cls: 0.3916  decode.loss_mask: 1.9771  decode.loss_dice: 1.7927  decode.d0.loss_cls: 0.5138  decode.d0.loss_mask: 2.0504  decode.d0.loss_dice: 1.8466  decode.d1.loss_cls: 0.3868  decode.d1.loss_mask: 1.9967  decode.d1.loss_dice: 1.8610  decode.d2.loss_cls: 0.4818  decode.d2.loss_mask: 2.0167  decode.d2.loss_dice: 1.8514  decode.d3.loss_cls: 0.4885  decode.d3.loss_mask: 1.9644  decode.d3.loss_dice: 1.7985  decode.d4.loss_cls: 0.3808  decode.d4.loss_mask: 2.0174  decode.d4.loss_dice: 1.8626  decode.d5.loss_cls: 0.3988  decode.d5.loss_mask: 1.9822  decode.d5.loss_dice: 1.8405  decode.d6.loss_cls: 0.4259  decode.d6.loss_mask: 2.0504  decode.d6.loss_dice: 1.8254  decode.d7.loss_cls: 0.4417  decode.d7.loss_mask: 1.9964  decode.d7.loss_dice: 1.8319  decode.d8.loss_cls: 0.3734  decode.d8.loss_mask: 2.0046  decode.d8.loss_dice: 1.8368  mix_decode.loss_cls: 0.3484  mix_decode.loss_mask: 0.7667  mix_decode.loss_dice: 0.9428  mix_decode.d0.loss_cls: 0.2834  mix_decode.d0.loss_mask: 0.7478  mix_decode.d0.loss_dice: 1.0454  mix_decode.d1.loss_cls: 0.3722  mix_decode.d1.loss_mask: 0.7267  mix_decode.d1.loss_dice: 0.9756  mix_decode.d2.loss_cls: 0.3387  mix_decode.d2.loss_mask: 0.7491  mix_decode.d2.loss_dice: 0.9735  mix_decode.d3.loss_cls: 0.3486  mix_decode.d3.loss_mask: 0.7294  mix_decode.d3.loss_dice: 0.9433  mix_decode.d4.loss_cls: 0.3687  mix_decode.d4.loss_mask: 0.7444  mix_decode.d4.loss_dice: 0.9583  mix_decode.d5.loss_cls: 0.3645  mix_decode.d5.loss_mask: 0.7257  mix_decode.d5.loss_dice: 0.9432  mix_decode.d6.loss_cls: 0.3852  mix_decode.d6.loss_mask: 0.7470  mix_decode.d6.loss_dice: 0.9245  mix_decode.d7.loss_cls: 0.3343  mix_decode.d7.loss_mask: 0.7602  mix_decode.d7.loss_dice: 0.9874  mix_decode.d8.loss_cls: 0.3275  mix_decode.d8.loss_mask: 0.7649  mix_decode.d8.loss_dice: 0.9339
2025/03/29 13:25:14 - mmengine - INFO - Iter(train) [ 6250/20000]  base_lr: 7.1378e-05 lr: 7.1378e-05  eta: 2:55:27  time: 1.1485  data_time: 0.0228  memory: 11208  loss: 62.5116  decode.loss_cls: 0.6243  decode.loss_mask: 1.9417  decode.loss_dice: 1.6367  decode.d0.loss_cls: 0.8007  decode.d0.loss_mask: 1.8815  decode.d0.loss_dice: 1.6691  decode.d1.loss_cls: 0.7405  decode.d1.loss_mask: 1.8622  decode.d1.loss_dice: 1.6809  decode.d2.loss_cls: 0.6231  decode.d2.loss_mask: 2.0123  decode.d2.loss_dice: 1.6743  decode.d3.loss_cls: 0.7134  decode.d3.loss_mask: 1.9236  decode.d3.loss_dice: 1.6718  decode.d4.loss_cls: 0.7007  decode.d4.loss_mask: 1.9536  decode.d4.loss_dice: 1.6961  decode.d5.loss_cls: 0.6962  decode.d5.loss_mask: 1.8995  decode.d5.loss_dice: 1.7204  decode.d6.loss_cls: 0.6047  decode.d6.loss_mask: 2.0154  decode.d6.loss_dice: 1.7237  decode.d7.loss_cls: 0.5987  decode.d7.loss_mask: 2.1502  decode.d7.loss_dice: 1.7642  decode.d8.loss_cls: 0.6658  decode.d8.loss_mask: 1.9768  decode.d8.loss_dice: 1.6736  mix_decode.loss_cls: 0.2570  mix_decode.loss_mask: 0.7603  mix_decode.loss_dice: 0.9022  mix_decode.d0.loss_cls: 0.3194  mix_decode.d0.loss_mask: 0.7553  mix_decode.d0.loss_dice: 0.9071  mix_decode.d1.loss_cls: 0.2909  mix_decode.d1.loss_mask: 0.7108  mix_decode.d1.loss_dice: 0.8773  mix_decode.d2.loss_cls: 0.2593  mix_decode.d2.loss_mask: 0.7611  mix_decode.d2.loss_dice: 0.8939  mix_decode.d3.loss_cls: 0.2345  mix_decode.d3.loss_mask: 0.7777  mix_decode.d3.loss_dice: 0.8949  mix_decode.d4.loss_cls: 0.2715  mix_decode.d4.loss_mask: 0.7601  mix_decode.d4.loss_dice: 0.8956  mix_decode.d5.loss_cls: 0.3290  mix_decode.d5.loss_mask: 0.7288  mix_decode.d5.loss_dice: 0.8685  mix_decode.d6.loss_cls: 0.2981  mix_decode.d6.loss_mask: 0.7337  mix_decode.d6.loss_dice: 0.8858  mix_decode.d7.loss_cls: 0.2810  mix_decode.d7.loss_mask: 0.7569  mix_decode.d7.loss_dice: 0.8862  mix_decode.d8.loss_cls: 0.2880  mix_decode.d8.loss_mask: 0.7490  mix_decode.d8.loss_dice: 0.8821
2025/03/29 13:26:12 - mmengine - INFO - Iter(train) [ 6300/20000]  base_lr: 7.1144e-05 lr: 7.1144e-05  eta: 2:55:30  time: 1.1445  data_time: 0.0226  memory: 11206  loss: 65.5419  decode.loss_cls: 0.4265  decode.loss_mask: 2.3300  decode.loss_dice: 1.8995  decode.d0.loss_cls: 0.5099  decode.d0.loss_mask: 2.2157  decode.d0.loss_dice: 1.9703  decode.d1.loss_cls: 0.4719  decode.d1.loss_mask: 2.2289  decode.d1.loss_dice: 1.8847  decode.d2.loss_cls: 0.4899  decode.d2.loss_mask: 2.2473  decode.d2.loss_dice: 1.8872  decode.d3.loss_cls: 0.4135  decode.d3.loss_mask: 2.2178  decode.d3.loss_dice: 1.8669  decode.d4.loss_cls: 0.3924  decode.d4.loss_mask: 2.2672  decode.d4.loss_dice: 1.9655  decode.d5.loss_cls: 0.3176  decode.d5.loss_mask: 2.2857  decode.d5.loss_dice: 1.9618  decode.d6.loss_cls: 0.3485  decode.d6.loss_mask: 2.2601  decode.d6.loss_dice: 1.9389  decode.d7.loss_cls: 0.3462  decode.d7.loss_mask: 2.2336  decode.d7.loss_dice: 1.9069  decode.d8.loss_cls: 0.3775  decode.d8.loss_mask: 2.2438  decode.d8.loss_dice: 1.8546  mix_decode.loss_cls: 0.3478  mix_decode.loss_mask: 0.7306  mix_decode.loss_dice: 0.9082  mix_decode.d0.loss_cls: 0.2930  mix_decode.d0.loss_mask: 0.7074  mix_decode.d0.loss_dice: 0.9858  mix_decode.d1.loss_cls: 0.2670  mix_decode.d1.loss_mask: 0.7265  mix_decode.d1.loss_dice: 0.9372  mix_decode.d2.loss_cls: 0.2951  mix_decode.d2.loss_mask: 0.7280  mix_decode.d2.loss_dice: 0.9236  mix_decode.d3.loss_cls: 0.3174  mix_decode.d3.loss_mask: 0.7380  mix_decode.d3.loss_dice: 0.9447  mix_decode.d4.loss_cls: 0.3295  mix_decode.d4.loss_mask: 0.7112  mix_decode.d4.loss_dice: 0.9345  mix_decode.d5.loss_cls: 0.2993  mix_decode.d5.loss_mask: 0.6925  mix_decode.d5.loss_dice: 0.9082  mix_decode.d6.loss_cls: 0.3512  mix_decode.d6.loss_mask: 0.7112  mix_decode.d6.loss_dice: 0.9224  mix_decode.d7.loss_cls: 0.3784  mix_decode.d7.loss_mask: 0.7240  mix_decode.d7.loss_dice: 0.9279  mix_decode.d8.loss_cls: 0.4100  mix_decode.d8.loss_mask: 0.7143  mix_decode.d8.loss_dice: 0.9168
2025/03/29 13:27:09 - mmengine - INFO - Iter(train) [ 6350/20000]  base_lr: 7.0911e-05 lr: 7.0911e-05  eta: 2:55:32  time: 1.1452  data_time: 0.0224  memory: 11212  loss: 65.0111  decode.loss_cls: 0.3615  decode.loss_mask: 2.1696  decode.loss_dice: 1.8481  decode.d0.loss_cls: 0.4582  decode.d0.loss_mask: 2.2426  decode.d0.loss_dice: 1.8734  decode.d1.loss_cls: 0.3150  decode.d1.loss_mask: 2.3408  decode.d1.loss_dice: 1.8843  decode.d2.loss_cls: 0.4443  decode.d2.loss_mask: 2.2341  decode.d2.loss_dice: 1.8716  decode.d3.loss_cls: 0.3619  decode.d3.loss_mask: 2.3200  decode.d3.loss_dice: 1.8653  decode.d4.loss_cls: 0.3747  decode.d4.loss_mask: 2.3229  decode.d4.loss_dice: 1.8960  decode.d5.loss_cls: 0.4104  decode.d5.loss_mask: 2.2530  decode.d5.loss_dice: 1.9292  decode.d6.loss_cls: 0.4133  decode.d6.loss_mask: 2.3036  decode.d6.loss_dice: 1.8431  decode.d7.loss_cls: 0.4302  decode.d7.loss_mask: 2.2410  decode.d7.loss_dice: 1.8609  decode.d8.loss_cls: 0.3959  decode.d8.loss_mask: 2.2917  decode.d8.loss_dice: 1.9229  mix_decode.loss_cls: 0.2635  mix_decode.loss_mask: 0.8375  mix_decode.loss_dice: 0.8445  mix_decode.d0.loss_cls: 0.2747  mix_decode.d0.loss_mask: 0.7975  mix_decode.d0.loss_dice: 0.8915  mix_decode.d1.loss_cls: 0.2358  mix_decode.d1.loss_mask: 0.8485  mix_decode.d1.loss_dice: 0.8155  mix_decode.d2.loss_cls: 0.2645  mix_decode.d2.loss_mask: 0.8092  mix_decode.d2.loss_dice: 0.8216  mix_decode.d3.loss_cls: 0.3065  mix_decode.d3.loss_mask: 0.8148  mix_decode.d3.loss_dice: 0.8201  mix_decode.d4.loss_cls: 0.2961  mix_decode.d4.loss_mask: 0.8073  mix_decode.d4.loss_dice: 0.8435  mix_decode.d5.loss_cls: 0.3275  mix_decode.d5.loss_mask: 0.8252  mix_decode.d5.loss_dice: 0.8178  mix_decode.d6.loss_cls: 0.3197  mix_decode.d6.loss_mask: 0.8362  mix_decode.d6.loss_dice: 0.8418  mix_decode.d7.loss_cls: 0.3305  mix_decode.d7.loss_mask: 0.8105  mix_decode.d7.loss_dice: 0.8360  mix_decode.d8.loss_cls: 0.3091  mix_decode.d8.loss_mask: 0.8253  mix_decode.d8.loss_dice: 0.8595
2025/03/29 13:28:06 - mmengine - INFO - Iter(train) [ 6400/20000]  base_lr: 7.0677e-05 lr: 7.0677e-05  eta: 2:55:34  time: 1.1463  data_time: 0.0226  memory: 11209  loss: 57.8708  decode.loss_cls: 0.3169  decode.loss_mask: 1.7884  decode.loss_dice: 1.7303  decode.d0.loss_cls: 0.3868  decode.d0.loss_mask: 1.8399  decode.d0.loss_dice: 1.7457  decode.d1.loss_cls: 0.4351  decode.d1.loss_mask: 1.7428  decode.d1.loss_dice: 1.6751  decode.d2.loss_cls: 0.3921  decode.d2.loss_mask: 1.7473  decode.d2.loss_dice: 1.6998  decode.d3.loss_cls: 0.4076  decode.d3.loss_mask: 1.7932  decode.d3.loss_dice: 1.7245  decode.d4.loss_cls: 0.4090  decode.d4.loss_mask: 1.7706  decode.d4.loss_dice: 1.7225  decode.d5.loss_cls: 0.3704  decode.d5.loss_mask: 1.7581  decode.d5.loss_dice: 1.7009  decode.d6.loss_cls: 0.4105  decode.d6.loss_mask: 1.7663  decode.d6.loss_dice: 1.6932  decode.d7.loss_cls: 0.4197  decode.d7.loss_mask: 1.7730  decode.d7.loss_dice: 1.7011  decode.d8.loss_cls: 0.3953  decode.d8.loss_mask: 1.7636  decode.d8.loss_dice: 1.7052  mix_decode.loss_cls: 0.2917  mix_decode.loss_mask: 0.6655  mix_decode.loss_dice: 0.9075  mix_decode.d0.loss_cls: 0.3294  mix_decode.d0.loss_mask: 0.6863  mix_decode.d0.loss_dice: 0.9506  mix_decode.d1.loss_cls: 0.3450  mix_decode.d1.loss_mask: 0.6700  mix_decode.d1.loss_dice: 0.8836  mix_decode.d2.loss_cls: 0.3037  mix_decode.d2.loss_mask: 0.6612  mix_decode.d2.loss_dice: 0.9243  mix_decode.d3.loss_cls: 0.3057  mix_decode.d3.loss_mask: 0.7036  mix_decode.d3.loss_dice: 0.9141  mix_decode.d4.loss_cls: 0.3578  mix_decode.d4.loss_mask: 0.6581  mix_decode.d4.loss_dice: 0.8899  mix_decode.d5.loss_cls: 0.3752  mix_decode.d5.loss_mask: 0.6880  mix_decode.d5.loss_dice: 0.8923  mix_decode.d6.loss_cls: 0.3975  mix_decode.d6.loss_mask: 0.6571  mix_decode.d6.loss_dice: 0.8838  mix_decode.d7.loss_cls: 0.3558  mix_decode.d7.loss_mask: 0.6704  mix_decode.d7.loss_dice: 0.8743  mix_decode.d8.loss_cls: 0.2828  mix_decode.d8.loss_mask: 0.6810  mix_decode.d8.loss_dice: 0.8794
2025/03/29 13:29:04 - mmengine - INFO - Iter(train) [ 6450/20000]  base_lr: 7.0443e-05 lr: 7.0443e-05  eta: 2:55:34  time: 1.1486  data_time: 0.0231  memory: 11222  loss: 58.8432  decode.loss_cls: 0.3559  decode.loss_mask: 1.8790  decode.loss_dice: 1.9119  decode.d0.loss_cls: 0.4524  decode.d0.loss_mask: 1.8164  decode.d0.loss_dice: 1.8442  decode.d1.loss_cls: 0.3606  decode.d1.loss_mask: 1.7798  decode.d1.loss_dice: 1.8341  decode.d2.loss_cls: 0.3627  decode.d2.loss_mask: 1.8015  decode.d2.loss_dice: 1.8587  decode.d3.loss_cls: 0.4259  decode.d3.loss_mask: 1.8502  decode.d3.loss_dice: 1.8095  decode.d4.loss_cls: 0.3611  decode.d4.loss_mask: 1.8184  decode.d4.loss_dice: 1.8491  decode.d5.loss_cls: 0.3437  decode.d5.loss_mask: 1.7869  decode.d5.loss_dice: 1.8655  decode.d6.loss_cls: 0.3440  decode.d6.loss_mask: 1.7631  decode.d6.loss_dice: 1.9049  decode.d7.loss_cls: 0.4306  decode.d7.loss_mask: 1.8160  decode.d7.loss_dice: 1.8678  decode.d8.loss_cls: 0.3652  decode.d8.loss_mask: 1.8810  decode.d8.loss_dice: 1.9053  mix_decode.loss_cls: 0.2879  mix_decode.loss_mask: 0.7010  mix_decode.loss_dice: 0.8447  mix_decode.d0.loss_cls: 0.2796  mix_decode.d0.loss_mask: 0.6870  mix_decode.d0.loss_dice: 0.8827  mix_decode.d1.loss_cls: 0.2908  mix_decode.d1.loss_mask: 0.6657  mix_decode.d1.loss_dice: 0.7945  mix_decode.d2.loss_cls: 0.2680  mix_decode.d2.loss_mask: 0.6981  mix_decode.d2.loss_dice: 0.8422  mix_decode.d3.loss_cls: 0.2751  mix_decode.d3.loss_mask: 0.6915  mix_decode.d3.loss_dice: 0.8220  mix_decode.d4.loss_cls: 0.2917  mix_decode.d4.loss_mask: 0.6847  mix_decode.d4.loss_dice: 0.8392  mix_decode.d5.loss_cls: 0.2778  mix_decode.d5.loss_mask: 0.6977  mix_decode.d5.loss_dice: 0.8591  mix_decode.d6.loss_cls: 0.2763  mix_decode.d6.loss_mask: 0.6843  mix_decode.d6.loss_dice: 0.8666  mix_decode.d7.loss_cls: 0.3320  mix_decode.d7.loss_mask: 0.7036  mix_decode.d7.loss_dice: 0.8321  mix_decode.d8.loss_cls: 0.2822  mix_decode.d8.loss_mask: 0.6873  mix_decode.d8.loss_dice: 0.8527
2025/03/29 13:30:01 - mmengine - INFO - Iter(train) [ 6500/20000]  base_lr: 7.0209e-05 lr: 7.0209e-05  eta: 2:55:33  time: 1.1446  data_time: 0.0220  memory: 11218  loss: 66.1398  decode.loss_cls: 0.3679  decode.loss_mask: 2.2250  decode.loss_dice: 1.9064  decode.d0.loss_cls: 0.4542  decode.d0.loss_mask: 2.1569  decode.d0.loss_dice: 1.9475  decode.d1.loss_cls: 0.3692  decode.d1.loss_mask: 2.2075  decode.d1.loss_dice: 1.8951  decode.d2.loss_cls: 0.3568  decode.d2.loss_mask: 2.1689  decode.d2.loss_dice: 1.9065  decode.d3.loss_cls: 0.4131  decode.d3.loss_mask: 2.1987  decode.d3.loss_dice: 1.8514  decode.d4.loss_cls: 0.4531  decode.d4.loss_mask: 2.1485  decode.d4.loss_dice: 1.8570  decode.d5.loss_cls: 0.4248  decode.d5.loss_mask: 2.0971  decode.d5.loss_dice: 1.8503  decode.d6.loss_cls: 0.3874  decode.d6.loss_mask: 2.1547  decode.d6.loss_dice: 1.8682  decode.d7.loss_cls: 0.3957  decode.d7.loss_mask: 2.2143  decode.d7.loss_dice: 1.8944  decode.d8.loss_cls: 0.3570  decode.d8.loss_mask: 2.2178  decode.d8.loss_dice: 1.8927  mix_decode.loss_cls: 0.3856  mix_decode.loss_mask: 0.8150  mix_decode.loss_dice: 1.0106  mix_decode.d0.loss_cls: 0.3439  mix_decode.d0.loss_mask: 0.7810  mix_decode.d0.loss_dice: 1.0761  mix_decode.d1.loss_cls: 0.3501  mix_decode.d1.loss_mask: 0.7634  mix_decode.d1.loss_dice: 0.9765  mix_decode.d2.loss_cls: 0.3314  mix_decode.d2.loss_mask: 0.7833  mix_decode.d2.loss_dice: 0.9595  mix_decode.d3.loss_cls: 0.4445  mix_decode.d3.loss_mask: 0.7517  mix_decode.d3.loss_dice: 0.9247  mix_decode.d4.loss_cls: 0.4040  mix_decode.d4.loss_mask: 0.7447  mix_decode.d4.loss_dice: 0.9467  mix_decode.d5.loss_cls: 0.4481  mix_decode.d5.loss_mask: 0.7589  mix_decode.d5.loss_dice: 0.9758  mix_decode.d6.loss_cls: 0.4145  mix_decode.d6.loss_mask: 0.7939  mix_decode.d6.loss_dice: 0.9803  mix_decode.d7.loss_cls: 0.3956  mix_decode.d7.loss_mask: 0.7871  mix_decode.d7.loss_dice: 0.9804  mix_decode.d8.loss_cls: 0.3722  mix_decode.d8.loss_mask: 0.8013  mix_decode.d8.loss_dice: 1.0011
2025/03/29 13:30:58 - mmengine - INFO - Iter(train) [ 6550/20000]  base_lr: 6.9975e-05 lr: 6.9975e-05  eta: 2:55:32  time: 1.1468  data_time: 0.0224  memory: 11234  loss: 58.5581  decode.loss_cls: 0.6160  decode.loss_mask: 1.8067  decode.loss_dice: 1.7455  decode.d0.loss_cls: 0.5610  decode.d0.loss_mask: 1.7980  decode.d0.loss_dice: 1.7928  decode.d1.loss_cls: 0.5455  decode.d1.loss_mask: 1.7642  decode.d1.loss_dice: 1.7267  decode.d2.loss_cls: 0.6038  decode.d2.loss_mask: 1.7833  decode.d2.loss_dice: 1.7387  decode.d3.loss_cls: 0.5835  decode.d3.loss_mask: 1.7958  decode.d3.loss_dice: 1.7237  decode.d4.loss_cls: 0.6309  decode.d4.loss_mask: 1.7541  decode.d4.loss_dice: 1.7169  decode.d5.loss_cls: 0.7219  decode.d5.loss_mask: 1.7730  decode.d5.loss_dice: 1.7567  decode.d6.loss_cls: 0.6776  decode.d6.loss_mask: 1.7779  decode.d6.loss_dice: 1.7061  decode.d7.loss_cls: 0.5829  decode.d7.loss_mask: 1.8587  decode.d7.loss_dice: 1.7936  decode.d8.loss_cls: 0.5252  decode.d8.loss_mask: 1.8243  decode.d8.loss_dice: 1.7793  mix_decode.loss_cls: 0.3110  mix_decode.loss_mask: 0.6214  mix_decode.loss_dice: 0.7876  mix_decode.d0.loss_cls: 0.3395  mix_decode.d0.loss_mask: 0.6335  mix_decode.d0.loss_dice: 0.8383  mix_decode.d1.loss_cls: 0.3112  mix_decode.d1.loss_mask: 0.6008  mix_decode.d1.loss_dice: 0.7584  mix_decode.d2.loss_cls: 0.3531  mix_decode.d2.loss_mask: 0.6130  mix_decode.d2.loss_dice: 0.7463  mix_decode.d3.loss_cls: 0.3416  mix_decode.d3.loss_mask: 0.6153  mix_decode.d3.loss_dice: 0.7472  mix_decode.d4.loss_cls: 0.3297  mix_decode.d4.loss_mask: 0.6088  mix_decode.d4.loss_dice: 0.7543  mix_decode.d5.loss_cls: 0.3425  mix_decode.d5.loss_mask: 0.5999  mix_decode.d5.loss_dice: 0.7758  mix_decode.d6.loss_cls: 0.3403  mix_decode.d6.loss_mask: 0.5967  mix_decode.d6.loss_dice: 0.7703  mix_decode.d7.loss_cls: 0.2991  mix_decode.d7.loss_mask: 0.6070  mix_decode.d7.loss_dice: 0.7656  mix_decode.d8.loss_cls: 0.2804  mix_decode.d8.loss_mask: 0.6297  mix_decode.d8.loss_dice: 0.7755
2025/03/29 13:31:56 - mmengine - INFO - Iter(train) [ 6600/20000]  base_lr: 6.9741e-05 lr: 6.9741e-05  eta: 2:55:30  time: 1.1582  data_time: 0.0222  memory: 11217  loss: 66.9812  decode.loss_cls: 0.5833  decode.loss_mask: 2.2076  decode.loss_dice: 1.9958  decode.d0.loss_cls: 0.5490  decode.d0.loss_mask: 2.2233  decode.d0.loss_dice: 2.0255  decode.d1.loss_cls: 0.5713  decode.d1.loss_mask: 2.1630  decode.d1.loss_dice: 1.9754  decode.d2.loss_cls: 0.5743  decode.d2.loss_mask: 2.1344  decode.d2.loss_dice: 1.9306  decode.d3.loss_cls: 0.5951  decode.d3.loss_mask: 2.1423  decode.d3.loss_dice: 1.9620  decode.d4.loss_cls: 0.5745  decode.d4.loss_mask: 2.1490  decode.d4.loss_dice: 1.9353  decode.d5.loss_cls: 0.6232  decode.d5.loss_mask: 2.1379  decode.d5.loss_dice: 1.9169  decode.d6.loss_cls: 0.6548  decode.d6.loss_mask: 2.1325  decode.d6.loss_dice: 1.9911  decode.d7.loss_cls: 0.5410  decode.d7.loss_mask: 2.1829  decode.d7.loss_dice: 1.9529  decode.d8.loss_cls: 0.5827  decode.d8.loss_mask: 2.2117  decode.d8.loss_dice: 1.9950  mix_decode.loss_cls: 0.3736  mix_decode.loss_mask: 0.6796  mix_decode.loss_dice: 0.8990  mix_decode.d0.loss_cls: 0.2516  mix_decode.d0.loss_mask: 0.7506  mix_decode.d0.loss_dice: 0.9921  mix_decode.d1.loss_cls: 0.3389  mix_decode.d1.loss_mask: 0.6964  mix_decode.d1.loss_dice: 0.9438  mix_decode.d2.loss_cls: 0.3813  mix_decode.d2.loss_mask: 0.6752  mix_decode.d2.loss_dice: 0.9107  mix_decode.d3.loss_cls: 0.3236  mix_decode.d3.loss_mask: 0.6980  mix_decode.d3.loss_dice: 0.9231  mix_decode.d4.loss_cls: 0.3625  mix_decode.d4.loss_mask: 0.6951  mix_decode.d4.loss_dice: 0.9144  mix_decode.d5.loss_cls: 0.3821  mix_decode.d5.loss_mask: 0.7015  mix_decode.d5.loss_dice: 0.9080  mix_decode.d6.loss_cls: 0.3941  mix_decode.d6.loss_mask: 0.6923  mix_decode.d6.loss_dice: 0.9170  mix_decode.d7.loss_cls: 0.3977  mix_decode.d7.loss_mask: 0.6834  mix_decode.d7.loss_dice: 0.8803  mix_decode.d8.loss_cls: 0.3950  mix_decode.d8.loss_mask: 0.6862  mix_decode.d8.loss_dice: 0.9196
2025/03/29 13:32:53 - mmengine - INFO - Iter(train) [ 6650/20000]  base_lr: 6.9507e-05 lr: 6.9507e-05  eta: 2:55:27  time: 1.1465  data_time: 0.0223  memory: 11213  loss: 61.5459  decode.loss_cls: 0.4761  decode.loss_mask: 1.9465  decode.loss_dice: 1.8284  decode.d0.loss_cls: 0.5233  decode.d0.loss_mask: 1.9056  decode.d0.loss_dice: 1.8808  decode.d1.loss_cls: 0.4250  decode.d1.loss_mask: 1.9652  decode.d1.loss_dice: 1.9245  decode.d2.loss_cls: 0.4546  decode.d2.loss_mask: 1.9309  decode.d2.loss_dice: 1.8920  decode.d3.loss_cls: 0.4286  decode.d3.loss_mask: 1.9741  decode.d3.loss_dice: 1.9050  decode.d4.loss_cls: 0.4343  decode.d4.loss_mask: 1.9365  decode.d4.loss_dice: 1.9172  decode.d5.loss_cls: 0.4744  decode.d5.loss_mask: 1.9251  decode.d5.loss_dice: 1.9462  decode.d6.loss_cls: 0.5400  decode.d6.loss_mask: 1.9263  decode.d6.loss_dice: 1.9472  decode.d7.loss_cls: 0.5300  decode.d7.loss_mask: 1.9003  decode.d7.loss_dice: 1.8708  decode.d8.loss_cls: 0.4191  decode.d8.loss_mask: 1.9498  decode.d8.loss_dice: 1.8691  mix_decode.loss_cls: 0.3913  mix_decode.loss_mask: 0.7095  mix_decode.loss_dice: 0.7928  mix_decode.d0.loss_cls: 0.3232  mix_decode.d0.loss_mask: 0.6549  mix_decode.d0.loss_dice: 0.8935  mix_decode.d1.loss_cls: 0.2631  mix_decode.d1.loss_mask: 0.6942  mix_decode.d1.loss_dice: 0.8453  mix_decode.d2.loss_cls: 0.3107  mix_decode.d2.loss_mask: 0.6658  mix_decode.d2.loss_dice: 0.8260  mix_decode.d3.loss_cls: 0.2834  mix_decode.d3.loss_mask: 0.7126  mix_decode.d3.loss_dice: 0.8305  mix_decode.d4.loss_cls: 0.3051  mix_decode.d4.loss_mask: 0.6969  mix_decode.d4.loss_dice: 0.8410  mix_decode.d5.loss_cls: 0.3171  mix_decode.d5.loss_mask: 0.6997  mix_decode.d5.loss_dice: 0.8627  mix_decode.d6.loss_cls: 0.3249  mix_decode.d6.loss_mask: 0.7082  mix_decode.d6.loss_dice: 0.8134  mix_decode.d7.loss_cls: 0.3649  mix_decode.d7.loss_mask: 0.7002  mix_decode.d7.loss_dice: 0.8125  mix_decode.d8.loss_cls: 0.3403  mix_decode.d8.loss_mask: 0.7133  mix_decode.d8.loss_dice: 0.8018
2025/03/29 13:33:51 - mmengine - INFO - Iter(train) [ 6700/20000]  base_lr: 6.9272e-05 lr: 6.9272e-05  eta: 2:55:24  time: 1.1549  data_time: 0.0228  memory: 11217  loss: 62.6960  decode.loss_cls: 0.3504  decode.loss_mask: 2.0419  decode.loss_dice: 1.7729  decode.d0.loss_cls: 0.4844  decode.d0.loss_mask: 2.0817  decode.d0.loss_dice: 1.8491  decode.d1.loss_cls: 0.3517  decode.d1.loss_mask: 2.0704  decode.d1.loss_dice: 1.8146  decode.d2.loss_cls: 0.4121  decode.d2.loss_mask: 1.9883  decode.d2.loss_dice: 1.7741  decode.d3.loss_cls: 0.3395  decode.d3.loss_mask: 2.0513  decode.d3.loss_dice: 1.7908  decode.d4.loss_cls: 0.3126  decode.d4.loss_mask: 2.0448  decode.d4.loss_dice: 1.7989  decode.d5.loss_cls: 0.3840  decode.d5.loss_mask: 2.0257  decode.d5.loss_dice: 1.7922  decode.d6.loss_cls: 0.3657  decode.d6.loss_mask: 2.0732  decode.d6.loss_dice: 1.8379  decode.d7.loss_cls: 0.3534  decode.d7.loss_mask: 2.0247  decode.d7.loss_dice: 1.8343  decode.d8.loss_cls: 0.3083  decode.d8.loss_mask: 2.0552  decode.d8.loss_dice: 1.8429  mix_decode.loss_cls: 0.3897  mix_decode.loss_mask: 0.6867  mix_decode.loss_dice: 0.9872  mix_decode.d0.loss_cls: 0.2898  mix_decode.d0.loss_mask: 0.7163  mix_decode.d0.loss_dice: 1.0497  mix_decode.d1.loss_cls: 0.3062  mix_decode.d1.loss_mask: 0.6936  mix_decode.d1.loss_dice: 1.0063  mix_decode.d2.loss_cls: 0.3418  mix_decode.d2.loss_mask: 0.7224  mix_decode.d2.loss_dice: 0.9969  mix_decode.d3.loss_cls: 0.2959  mix_decode.d3.loss_mask: 0.7324  mix_decode.d3.loss_dice: 1.0143  mix_decode.d4.loss_cls: 0.2992  mix_decode.d4.loss_mask: 0.7238  mix_decode.d4.loss_dice: 1.0093  mix_decode.d5.loss_cls: 0.2998  mix_decode.d5.loss_mask: 0.6983  mix_decode.d5.loss_dice: 1.0022  mix_decode.d6.loss_cls: 0.3412  mix_decode.d6.loss_mask: 0.6805  mix_decode.d6.loss_dice: 1.0226  mix_decode.d7.loss_cls: 0.3622  mix_decode.d7.loss_mask: 0.7090  mix_decode.d7.loss_dice: 1.0060  mix_decode.d8.loss_cls: 0.3748  mix_decode.d8.loss_mask: 0.7084  mix_decode.d8.loss_dice: 1.0029
2025/03/29 13:34:48 - mmengine - INFO - Iter(train) [ 6750/20000]  base_lr: 6.9038e-05 lr: 6.9038e-05  eta: 2:55:19  time: 1.1499  data_time: 0.0222  memory: 11217  loss: 54.3142  decode.loss_cls: 0.4838  decode.loss_mask: 1.5444  decode.loss_dice: 1.5606  decode.d0.loss_cls: 0.5710  decode.d0.loss_mask: 1.5747  decode.d0.loss_dice: 1.5820  decode.d1.loss_cls: 0.4684  decode.d1.loss_mask: 1.5878  decode.d1.loss_dice: 1.5489  decode.d2.loss_cls: 0.4681  decode.d2.loss_mask: 1.5874  decode.d2.loss_dice: 1.5969  decode.d3.loss_cls: 0.5094  decode.d3.loss_mask: 1.5455  decode.d3.loss_dice: 1.5756  decode.d4.loss_cls: 0.4516  decode.d4.loss_mask: 1.6142  decode.d4.loss_dice: 1.6009  decode.d5.loss_cls: 0.4503  decode.d5.loss_mask: 1.6033  decode.d5.loss_dice: 1.6668  decode.d6.loss_cls: 0.5400  decode.d6.loss_mask: 1.5572  decode.d6.loss_dice: 1.6141  decode.d7.loss_cls: 0.4213  decode.d7.loss_mask: 1.5824  decode.d7.loss_dice: 1.5852  decode.d8.loss_cls: 0.3688  decode.d8.loss_mask: 1.5950  decode.d8.loss_dice: 1.5949  mix_decode.loss_cls: 0.2744  mix_decode.loss_mask: 0.6753  mix_decode.loss_dice: 0.8285  mix_decode.d0.loss_cls: 0.2440  mix_decode.d0.loss_mask: 0.6722  mix_decode.d0.loss_dice: 0.9200  mix_decode.d1.loss_cls: 0.2261  mix_decode.d1.loss_mask: 0.6931  mix_decode.d1.loss_dice: 0.8581  mix_decode.d2.loss_cls: 0.2189  mix_decode.d2.loss_mask: 0.6958  mix_decode.d2.loss_dice: 0.8566  mix_decode.d3.loss_cls: 0.2890  mix_decode.d3.loss_mask: 0.6512  mix_decode.d3.loss_dice: 0.8209  mix_decode.d4.loss_cls: 0.2516  mix_decode.d4.loss_mask: 0.6788  mix_decode.d4.loss_dice: 0.8536  mix_decode.d5.loss_cls: 0.2487  mix_decode.d5.loss_mask: 0.6793  mix_decode.d5.loss_dice: 0.8607  mix_decode.d6.loss_cls: 0.2500  mix_decode.d6.loss_mask: 0.6447  mix_decode.d6.loss_dice: 0.8564  mix_decode.d7.loss_cls: 0.2447  mix_decode.d7.loss_mask: 0.7015  mix_decode.d7.loss_dice: 0.8707  mix_decode.d8.loss_cls: 0.2460  mix_decode.d8.loss_mask: 0.6865  mix_decode.d8.loss_dice: 0.8665
2025/03/29 13:35:46 - mmengine - INFO - Iter(train) [ 6800/20000]  base_lr: 6.8803e-05 lr: 6.8803e-05  eta: 2:55:14  time: 1.1437  data_time: 0.0223  memory: 11209  loss: 61.4930  decode.loss_cls: 0.3970  decode.loss_mask: 1.7916  decode.loss_dice: 1.6762  decode.d0.loss_cls: 0.6069  decode.d0.loss_mask: 1.8365  decode.d0.loss_dice: 1.7025  decode.d1.loss_cls: 0.4450  decode.d1.loss_mask: 1.8446  decode.d1.loss_dice: 1.6805  decode.d2.loss_cls: 0.5120  decode.d2.loss_mask: 1.7633  decode.d2.loss_dice: 1.6627  decode.d3.loss_cls: 0.4291  decode.d3.loss_mask: 1.8694  decode.d3.loss_dice: 1.7018  decode.d4.loss_cls: 0.4912  decode.d4.loss_mask: 1.7801  decode.d4.loss_dice: 1.6688  decode.d5.loss_cls: 0.4834  decode.d5.loss_mask: 1.7985  decode.d5.loss_dice: 1.6763  decode.d6.loss_cls: 0.4919  decode.d6.loss_mask: 1.8623  decode.d6.loss_dice: 1.6612  decode.d7.loss_cls: 0.4465  decode.d7.loss_mask: 1.8505  decode.d7.loss_dice: 1.7261  decode.d8.loss_cls: 0.3388  decode.d8.loss_mask: 1.8818  decode.d8.loss_dice: 1.7464  mix_decode.loss_cls: 0.4761  mix_decode.loss_mask: 0.7833  mix_decode.loss_dice: 0.9408  mix_decode.d0.loss_cls: 0.4084  mix_decode.d0.loss_mask: 0.7873  mix_decode.d0.loss_dice: 1.0007  mix_decode.d1.loss_cls: 0.4072  mix_decode.d1.loss_mask: 0.7842  mix_decode.d1.loss_dice: 0.9287  mix_decode.d2.loss_cls: 0.4976  mix_decode.d2.loss_mask: 0.7281  mix_decode.d2.loss_dice: 0.9053  mix_decode.d3.loss_cls: 0.4076  mix_decode.d3.loss_mask: 0.7595  mix_decode.d3.loss_dice: 0.9310  mix_decode.d4.loss_cls: 0.4401  mix_decode.d4.loss_mask: 0.7811  mix_decode.d4.loss_dice: 0.9480  mix_decode.d5.loss_cls: 0.4478  mix_decode.d5.loss_mask: 0.7611  mix_decode.d5.loss_dice: 0.9534  mix_decode.d6.loss_cls: 0.4477  mix_decode.d6.loss_mask: 0.7851  mix_decode.d6.loss_dice: 0.9527  mix_decode.d7.loss_cls: 0.4864  mix_decode.d7.loss_mask: 0.7503  mix_decode.d7.loss_dice: 0.9235  mix_decode.d8.loss_cls: 0.4977  mix_decode.d8.loss_mask: 0.7837  mix_decode.d8.loss_dice: 0.9657
2025/03/29 13:36:43 - mmengine - INFO - Iter(train) [ 6850/20000]  base_lr: 6.8569e-05 lr: 6.8569e-05  eta: 2:55:08  time: 1.1437  data_time: 0.0218  memory: 11217  loss: 68.4052  decode.loss_cls: 0.2319  decode.loss_mask: 2.4488  decode.loss_dice: 2.0533  decode.d0.loss_cls: 0.3620  decode.d0.loss_mask: 2.5297  decode.d0.loss_dice: 2.0519  decode.d1.loss_cls: 0.2875  decode.d1.loss_mask: 2.4243  decode.d1.loss_dice: 2.0095  decode.d2.loss_cls: 0.3202  decode.d2.loss_mask: 2.3824  decode.d2.loss_dice: 2.0183  decode.d3.loss_cls: 0.2871  decode.d3.loss_mask: 2.3793  decode.d3.loss_dice: 2.0094  decode.d4.loss_cls: 0.3424  decode.d4.loss_mask: 2.4352  decode.d4.loss_dice: 2.0515  decode.d5.loss_cls: 0.2925  decode.d5.loss_mask: 2.4446  decode.d5.loss_dice: 2.0306  decode.d6.loss_cls: 0.3416  decode.d6.loss_mask: 2.4065  decode.d6.loss_dice: 1.9940  decode.d7.loss_cls: 0.3279  decode.d7.loss_mask: 2.3864  decode.d7.loss_dice: 2.0037  decode.d8.loss_cls: 0.2837  decode.d8.loss_mask: 2.4361  decode.d8.loss_dice: 2.0302  mix_decode.loss_cls: 0.3701  mix_decode.loss_mask: 0.7896  mix_decode.loss_dice: 0.9109  mix_decode.d0.loss_cls: 0.2658  mix_decode.d0.loss_mask: 0.8084  mix_decode.d0.loss_dice: 0.9925  mix_decode.d1.loss_cls: 0.3034  mix_decode.d1.loss_mask: 0.7924  mix_decode.d1.loss_dice: 0.9505  mix_decode.d2.loss_cls: 0.3160  mix_decode.d2.loss_mask: 0.8000  mix_decode.d2.loss_dice: 0.9341  mix_decode.d3.loss_cls: 0.3926  mix_decode.d3.loss_mask: 0.7831  mix_decode.d3.loss_dice: 0.9144  mix_decode.d4.loss_cls: 0.4087  mix_decode.d4.loss_mask: 0.7725  mix_decode.d4.loss_dice: 0.9315  mix_decode.d5.loss_cls: 0.3817  mix_decode.d5.loss_mask: 0.7582  mix_decode.d5.loss_dice: 0.9099  mix_decode.d6.loss_cls: 0.3730  mix_decode.d6.loss_mask: 0.7740  mix_decode.d6.loss_dice: 0.9131  mix_decode.d7.loss_cls: 0.4235  mix_decode.d7.loss_mask: 0.7693  mix_decode.d7.loss_dice: 0.9061  mix_decode.d8.loss_cls: 0.4290  mix_decode.d8.loss_mask: 0.7959  mix_decode.d8.loss_dice: 0.9324
2025/03/29 13:37:40 - mmengine - INFO - Iter(train) [ 6900/20000]  base_lr: 6.8334e-05 lr: 6.8334e-05  eta: 2:55:01  time: 1.1461  data_time: 0.0220  memory: 11212  loss: 60.5506  decode.loss_cls: 0.3066  decode.loss_mask: 1.8251  decode.loss_dice: 2.0224  decode.d0.loss_cls: 0.3774  decode.d0.loss_mask: 1.8172  decode.d0.loss_dice: 2.0046  decode.d1.loss_cls: 0.3278  decode.d1.loss_mask: 1.7921  decode.d1.loss_dice: 1.9976  decode.d2.loss_cls: 0.3797  decode.d2.loss_mask: 1.8128  decode.d2.loss_dice: 1.9623  decode.d3.loss_cls: 0.3516  decode.d3.loss_mask: 1.8000  decode.d3.loss_dice: 1.9770  decode.d4.loss_cls: 0.3962  decode.d4.loss_mask: 1.8011  decode.d4.loss_dice: 1.9985  decode.d5.loss_cls: 0.3287  decode.d5.loss_mask: 1.8054  decode.d5.loss_dice: 2.0127  decode.d6.loss_cls: 0.2834  decode.d6.loss_mask: 1.7998  decode.d6.loss_dice: 2.0045  decode.d7.loss_cls: 0.3167  decode.d7.loss_mask: 1.7891  decode.d7.loss_dice: 2.0368  decode.d8.loss_cls: 0.3138  decode.d8.loss_mask: 1.8353  decode.d8.loss_dice: 2.0566  mix_decode.loss_cls: 0.3153  mix_decode.loss_mask: 0.7373  mix_decode.loss_dice: 0.8194  mix_decode.d0.loss_cls: 0.2843  mix_decode.d0.loss_mask: 0.7681  mix_decode.d0.loss_dice: 0.9283  mix_decode.d1.loss_cls: 0.2718  mix_decode.d1.loss_mask: 0.7468  mix_decode.d1.loss_dice: 0.8447  mix_decode.d2.loss_cls: 0.2914  mix_decode.d2.loss_mask: 0.7547  mix_decode.d2.loss_dice: 0.8427  mix_decode.d3.loss_cls: 0.2798  mix_decode.d3.loss_mask: 0.7312  mix_decode.d3.loss_dice: 0.8286  mix_decode.d4.loss_cls: 0.3223  mix_decode.d4.loss_mask: 0.7775  mix_decode.d4.loss_dice: 0.8408  mix_decode.d5.loss_cls: 0.3051  mix_decode.d5.loss_mask: 0.7509  mix_decode.d5.loss_dice: 0.8275  mix_decode.d6.loss_cls: 0.3105  mix_decode.d6.loss_mask: 0.7479  mix_decode.d6.loss_dice: 0.8549  mix_decode.d7.loss_cls: 0.3311  mix_decode.d7.loss_mask: 0.7641  mix_decode.d7.loss_dice: 0.8214  mix_decode.d8.loss_cls: 0.3111  mix_decode.d8.loss_mask: 0.7602  mix_decode.d8.loss_dice: 0.8482
2025/03/29 13:38:38 - mmengine - INFO - Iter(train) [ 6950/20000]  base_lr: 6.8099e-05 lr: 6.8099e-05  eta: 2:54:53  time: 1.1409  data_time: 0.0220  memory: 11213  loss: 58.5841  decode.loss_cls: 0.3236  decode.loss_mask: 1.9309  decode.loss_dice: 1.7071  decode.d0.loss_cls: 0.4641  decode.d0.loss_mask: 1.8649  decode.d0.loss_dice: 1.7175  decode.d1.loss_cls: 0.3999  decode.d1.loss_mask: 1.8822  decode.d1.loss_dice: 1.6856  decode.d2.loss_cls: 0.3616  decode.d2.loss_mask: 1.8983  decode.d2.loss_dice: 1.6995  decode.d3.loss_cls: 0.3383  decode.d3.loss_mask: 1.9035  decode.d3.loss_dice: 1.6991  decode.d4.loss_cls: 0.4148  decode.d4.loss_mask: 1.8879  decode.d4.loss_dice: 1.6357  decode.d5.loss_cls: 0.3658  decode.d5.loss_mask: 1.8995  decode.d5.loss_dice: 1.6468  decode.d6.loss_cls: 0.3615  decode.d6.loss_mask: 1.9211  decode.d6.loss_dice: 1.6557  decode.d7.loss_cls: 0.3427  decode.d7.loss_mask: 1.9278  decode.d7.loss_dice: 1.6588  decode.d8.loss_cls: 0.3234  decode.d8.loss_mask: 1.9261  decode.d8.loss_dice: 1.6978  mix_decode.loss_cls: 0.2779  mix_decode.loss_mask: 0.7763  mix_decode.loss_dice: 0.8679  mix_decode.d0.loss_cls: 0.2346  mix_decode.d0.loss_mask: 0.7939  mix_decode.d0.loss_dice: 0.9090  mix_decode.d1.loss_cls: 0.2340  mix_decode.d1.loss_mask: 0.7524  mix_decode.d1.loss_dice: 0.8731  mix_decode.d2.loss_cls: 0.2262  mix_decode.d2.loss_mask: 0.7809  mix_decode.d2.loss_dice: 0.8629  mix_decode.d3.loss_cls: 0.2364  mix_decode.d3.loss_mask: 0.7733  mix_decode.d3.loss_dice: 0.8807  mix_decode.d4.loss_cls: 0.3119  mix_decode.d4.loss_mask: 0.7549  mix_decode.d4.loss_dice: 0.8632  mix_decode.d5.loss_cls: 0.2922  mix_decode.d5.loss_mask: 0.7601  mix_decode.d5.loss_dice: 0.8718  mix_decode.d6.loss_cls: 0.2405  mix_decode.d6.loss_mask: 0.7495  mix_decode.d6.loss_dice: 0.8524  mix_decode.d7.loss_cls: 0.2550  mix_decode.d7.loss_mask: 0.7656  mix_decode.d7.loss_dice: 0.9041  mix_decode.d8.loss_cls: 0.3220  mix_decode.d8.loss_mask: 0.7545  mix_decode.d8.loss_dice: 0.8654
2025/03/29 13:39:35 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 13:39:35 - mmengine - INFO - Iter(train) [ 7000/20000]  base_lr: 6.7864e-05 lr: 6.7864e-05  eta: 2:54:45  time: 1.1441  data_time: 0.0223  memory: 11223  loss: 58.9881  decode.loss_cls: 0.3903  decode.loss_mask: 1.8370  decode.loss_dice: 1.8735  decode.d0.loss_cls: 0.4899  decode.d0.loss_mask: 1.8404  decode.d0.loss_dice: 1.8780  decode.d1.loss_cls: 0.3870  decode.d1.loss_mask: 1.8400  decode.d1.loss_dice: 1.8369  decode.d2.loss_cls: 0.4754  decode.d2.loss_mask: 1.8013  decode.d2.loss_dice: 1.7809  decode.d3.loss_cls: 0.3599  decode.d3.loss_mask: 1.8522  decode.d3.loss_dice: 1.8424  decode.d4.loss_cls: 0.3696  decode.d4.loss_mask: 1.8326  decode.d4.loss_dice: 1.7937  decode.d5.loss_cls: 0.3863  decode.d5.loss_mask: 1.8122  decode.d5.loss_dice: 1.8072  decode.d6.loss_cls: 0.4127  decode.d6.loss_mask: 1.8619  decode.d6.loss_dice: 1.8351  decode.d7.loss_cls: 0.4882  decode.d7.loss_mask: 1.8472  decode.d7.loss_dice: 1.7994  decode.d8.loss_cls: 0.4108  decode.d8.loss_mask: 1.8558  decode.d8.loss_dice: 1.8453  mix_decode.loss_cls: 0.2310  mix_decode.loss_mask: 0.6780  mix_decode.loss_dice: 0.8938  mix_decode.d0.loss_cls: 0.2056  mix_decode.d0.loss_mask: 0.7225  mix_decode.d0.loss_dice: 0.9714  mix_decode.d1.loss_cls: 0.1973  mix_decode.d1.loss_mask: 0.6757  mix_decode.d1.loss_dice: 0.9121  mix_decode.d2.loss_cls: 0.1766  mix_decode.d2.loss_mask: 0.6803  mix_decode.d2.loss_dice: 0.9145  mix_decode.d3.loss_cls: 0.1763  mix_decode.d3.loss_mask: 0.6835  mix_decode.d3.loss_dice: 0.9189  mix_decode.d4.loss_cls: 0.2448  mix_decode.d4.loss_mask: 0.6771  mix_decode.d4.loss_dice: 0.9117  mix_decode.d5.loss_cls: 0.2081  mix_decode.d5.loss_mask: 0.6983  mix_decode.d5.loss_dice: 0.9141  mix_decode.d6.loss_cls: 0.2528  mix_decode.d6.loss_mask: 0.6713  mix_decode.d6.loss_dice: 0.9013  mix_decode.d7.loss_cls: 0.2697  mix_decode.d7.loss_mask: 0.6604  mix_decode.d7.loss_dice: 0.9082  mix_decode.d8.loss_cls: 0.2317  mix_decode.d8.loss_mask: 0.6558  mix_decode.d8.loss_dice: 0.9024
2025/03/29 13:40:32 - mmengine - INFO - Iter(train) [ 7050/20000]  base_lr: 6.7629e-05 lr: 6.7629e-05  eta: 2:54:36  time: 1.1476  data_time: 0.0223  memory: 11225  loss: 63.0063  decode.loss_cls: 0.3554  decode.loss_mask: 2.1811  decode.loss_dice: 1.7615  decode.d0.loss_cls: 0.5609  decode.d0.loss_mask: 2.1381  decode.d0.loss_dice: 1.8130  decode.d1.loss_cls: 0.4030  decode.d1.loss_mask: 2.1254  decode.d1.loss_dice: 1.7610  decode.d2.loss_cls: 0.2767  decode.d2.loss_mask: 2.2091  decode.d2.loss_dice: 1.7675  decode.d3.loss_cls: 0.3410  decode.d3.loss_mask: 2.1571  decode.d3.loss_dice: 1.7707  decode.d4.loss_cls: 0.4309  decode.d4.loss_mask: 2.1997  decode.d4.loss_dice: 1.8127  decode.d5.loss_cls: 0.3578  decode.d5.loss_mask: 2.2042  decode.d5.loss_dice: 1.7756  decode.d6.loss_cls: 0.3798  decode.d6.loss_mask: 2.1833  decode.d6.loss_dice: 1.8385  decode.d7.loss_cls: 0.3915  decode.d7.loss_mask: 2.1496  decode.d7.loss_dice: 1.7821  decode.d8.loss_cls: 0.3729  decode.d8.loss_mask: 2.1695  decode.d8.loss_dice: 1.7894  mix_decode.loss_cls: 0.2293  mix_decode.loss_mask: 0.8613  mix_decode.loss_dice: 0.8960  mix_decode.d0.loss_cls: 0.2264  mix_decode.d0.loss_mask: 0.8175  mix_decode.d0.loss_dice: 0.8937  mix_decode.d1.loss_cls: 0.2651  mix_decode.d1.loss_mask: 0.8037  mix_decode.d1.loss_dice: 0.8881  mix_decode.d2.loss_cls: 0.2328  mix_decode.d2.loss_mask: 0.8061  mix_decode.d2.loss_dice: 0.8607  mix_decode.d3.loss_cls: 0.2353  mix_decode.d3.loss_mask: 0.7872  mix_decode.d3.loss_dice: 0.8573  mix_decode.d4.loss_cls: 0.3027  mix_decode.d4.loss_mask: 0.8318  mix_decode.d4.loss_dice: 0.8675  mix_decode.d5.loss_cls: 0.3078  mix_decode.d5.loss_mask: 0.8035  mix_decode.d5.loss_dice: 0.8440  mix_decode.d6.loss_cls: 0.3393  mix_decode.d6.loss_mask: 0.7874  mix_decode.d6.loss_dice: 0.8667  mix_decode.d7.loss_cls: 0.3313  mix_decode.d7.loss_mask: 0.7876  mix_decode.d7.loss_dice: 0.8762  mix_decode.d8.loss_cls: 0.2909  mix_decode.d8.loss_mask: 0.7763  mix_decode.d8.loss_dice: 0.8744
2025/03/29 13:41:30 - mmengine - INFO - Iter(train) [ 7100/20000]  base_lr: 6.7394e-05 lr: 6.7394e-05  eta: 2:54:26  time: 1.1496  data_time: 0.0218  memory: 11211  loss: 63.5791  decode.loss_cls: 0.4909  decode.loss_mask: 2.0646  decode.loss_dice: 1.9577  decode.d0.loss_cls: 0.3332  decode.d0.loss_mask: 2.0958  decode.d0.loss_dice: 2.0635  decode.d1.loss_cls: 0.4433  decode.d1.loss_mask: 2.0972  decode.d1.loss_dice: 1.9187  decode.d2.loss_cls: 0.3964  decode.d2.loss_mask: 2.0439  decode.d2.loss_dice: 1.9534  decode.d3.loss_cls: 0.4353  decode.d3.loss_mask: 2.0834  decode.d3.loss_dice: 1.9368  decode.d4.loss_cls: 0.4043  decode.d4.loss_mask: 2.0908  decode.d4.loss_dice: 1.9327  decode.d5.loss_cls: 0.4182  decode.d5.loss_mask: 2.0746  decode.d5.loss_dice: 1.9493  decode.d6.loss_cls: 0.3498  decode.d6.loss_mask: 2.0837  decode.d6.loss_dice: 2.0575  decode.d7.loss_cls: 0.3849  decode.d7.loss_mask: 2.0882  decode.d7.loss_dice: 1.9334  decode.d8.loss_cls: 0.3724  decode.d8.loss_mask: 2.0780  decode.d8.loss_dice: 2.0035  mix_decode.loss_cls: 0.2776  mix_decode.loss_mask: 0.6818  mix_decode.loss_dice: 0.9611  mix_decode.d0.loss_cls: 0.2269  mix_decode.d0.loss_mask: 0.6843  mix_decode.d0.loss_dice: 1.0149  mix_decode.d1.loss_cls: 0.2511  mix_decode.d1.loss_mask: 0.6882  mix_decode.d1.loss_dice: 0.9652  mix_decode.d2.loss_cls: 0.2156  mix_decode.d2.loss_mask: 0.6806  mix_decode.d2.loss_dice: 0.9493  mix_decode.d3.loss_cls: 0.2586  mix_decode.d3.loss_mask: 0.6796  mix_decode.d3.loss_dice: 0.9716  mix_decode.d4.loss_cls: 0.2681  mix_decode.d4.loss_mask: 0.6863  mix_decode.d4.loss_dice: 0.9511  mix_decode.d5.loss_cls: 0.2528  mix_decode.d5.loss_mask: 0.7000  mix_decode.d5.loss_dice: 0.9621  mix_decode.d6.loss_cls: 0.2527  mix_decode.d6.loss_mask: 0.7006  mix_decode.d6.loss_dice: 0.9456  mix_decode.d7.loss_cls: 0.2574  mix_decode.d7.loss_mask: 0.6967  mix_decode.d7.loss_dice: 0.9461  mix_decode.d8.loss_cls: 0.2562  mix_decode.d8.loss_mask: 0.6975  mix_decode.d8.loss_dice: 0.9641
2025/03/29 13:42:27 - mmengine - INFO - Iter(train) [ 7150/20000]  base_lr: 6.7159e-05 lr: 6.7159e-05  eta: 2:54:16  time: 1.1413  data_time: 0.0227  memory: 11224  loss: 52.1739  decode.loss_cls: 0.2040  decode.loss_mask: 1.9108  decode.loss_dice: 1.4749  decode.d0.loss_cls: 0.3410  decode.d0.loss_mask: 1.9300  decode.d0.loss_dice: 1.4612  decode.d1.loss_cls: 0.1938  decode.d1.loss_mask: 1.8909  decode.d1.loss_dice: 1.4447  decode.d2.loss_cls: 0.1770  decode.d2.loss_mask: 1.9487  decode.d2.loss_dice: 1.4874  decode.d3.loss_cls: 0.1956  decode.d3.loss_mask: 1.9091  decode.d3.loss_dice: 1.4710  decode.d4.loss_cls: 0.2067  decode.d4.loss_mask: 1.8897  decode.d4.loss_dice: 1.4676  decode.d5.loss_cls: 0.2278  decode.d5.loss_mask: 1.8857  decode.d5.loss_dice: 1.4524  decode.d6.loss_cls: 0.2272  decode.d6.loss_mask: 1.9103  decode.d6.loss_dice: 1.4557  decode.d7.loss_cls: 0.1918  decode.d7.loss_mask: 1.9411  decode.d7.loss_dice: 1.4715  decode.d8.loss_cls: 0.1511  decode.d8.loss_mask: 1.9191  decode.d8.loss_dice: 1.4955  mix_decode.loss_cls: 0.1579  mix_decode.loss_mask: 0.7049  mix_decode.loss_dice: 0.7661  mix_decode.d0.loss_cls: 0.2215  mix_decode.d0.loss_mask: 0.6920  mix_decode.d0.loss_dice: 0.8055  mix_decode.d1.loss_cls: 0.1698  mix_decode.d1.loss_mask: 0.6762  mix_decode.d1.loss_dice: 0.7397  mix_decode.d2.loss_cls: 0.2039  mix_decode.d2.loss_mask: 0.6511  mix_decode.d2.loss_dice: 0.7284  mix_decode.d3.loss_cls: 0.2161  mix_decode.d3.loss_mask: 0.6856  mix_decode.d3.loss_dice: 0.7530  mix_decode.d4.loss_cls: 0.2206  mix_decode.d4.loss_mask: 0.6559  mix_decode.d4.loss_dice: 0.7479  mix_decode.d5.loss_cls: 0.2002  mix_decode.d5.loss_mask: 0.6572  mix_decode.d5.loss_dice: 0.7291  mix_decode.d6.loss_cls: 0.2042  mix_decode.d6.loss_mask: 0.6682  mix_decode.d6.loss_dice: 0.7540  mix_decode.d7.loss_cls: 0.1906  mix_decode.d7.loss_mask: 0.6890  mix_decode.d7.loss_dice: 0.7416  mix_decode.d8.loss_cls: 0.1522  mix_decode.d8.loss_mask: 0.6765  mix_decode.d8.loss_dice: 0.7816
2025/03/29 13:43:24 - mmengine - INFO - Iter(train) [ 7200/20000]  base_lr: 6.6924e-05 lr: 6.6924e-05  eta: 2:54:05  time: 1.1468  data_time: 0.0222  memory: 11217  loss: 56.0866  decode.loss_cls: 0.2860  decode.loss_mask: 1.7354  decode.loss_dice: 1.7876  decode.d0.loss_cls: 0.4128  decode.d0.loss_mask: 1.7040  decode.d0.loss_dice: 1.8403  decode.d1.loss_cls: 0.2732  decode.d1.loss_mask: 1.6630  decode.d1.loss_dice: 1.7837  decode.d2.loss_cls: 0.2876  decode.d2.loss_mask: 1.6621  decode.d2.loss_dice: 1.7891  decode.d3.loss_cls: 0.3111  decode.d3.loss_mask: 1.6526  decode.d3.loss_dice: 1.8429  decode.d4.loss_cls: 0.2967  decode.d4.loss_mask: 1.6706  decode.d4.loss_dice: 1.8244  decode.d5.loss_cls: 0.3046  decode.d5.loss_mask: 1.7127  decode.d5.loss_dice: 1.8289  decode.d6.loss_cls: 0.2468  decode.d6.loss_mask: 1.7264  decode.d6.loss_dice: 1.8053  decode.d7.loss_cls: 0.2197  decode.d7.loss_mask: 1.7437  decode.d7.loss_dice: 1.8176  decode.d8.loss_cls: 0.2724  decode.d8.loss_mask: 1.7238  decode.d8.loss_dice: 1.7877  mix_decode.loss_cls: 0.2079  mix_decode.loss_mask: 0.7235  mix_decode.loss_dice: 0.8933  mix_decode.d0.loss_cls: 0.2234  mix_decode.d0.loss_mask: 0.7137  mix_decode.d0.loss_dice: 0.9212  mix_decode.d1.loss_cls: 0.1791  mix_decode.d1.loss_mask: 0.7129  mix_decode.d1.loss_dice: 0.8785  mix_decode.d2.loss_cls: 0.1831  mix_decode.d2.loss_mask: 0.7100  mix_decode.d2.loss_dice: 0.8698  mix_decode.d3.loss_cls: 0.1718  mix_decode.d3.loss_mask: 0.7164  mix_decode.d3.loss_dice: 0.8906  mix_decode.d4.loss_cls: 0.2381  mix_decode.d4.loss_mask: 0.6960  mix_decode.d4.loss_dice: 0.8629  mix_decode.d5.loss_cls: 0.2015  mix_decode.d5.loss_mask: 0.7126  mix_decode.d5.loss_dice: 0.8745  mix_decode.d6.loss_cls: 0.2089  mix_decode.d6.loss_mask: 0.7052  mix_decode.d6.loss_dice: 0.8904  mix_decode.d7.loss_cls: 0.1820  mix_decode.d7.loss_mask: 0.7259  mix_decode.d7.loss_dice: 0.9093  mix_decode.d8.loss_cls: 0.2227  mix_decode.d8.loss_mask: 0.7259  mix_decode.d8.loss_dice: 0.9228
2025/03/29 13:44:22 - mmengine - INFO - Iter(train) [ 7250/20000]  base_lr: 6.6689e-05 lr: 6.6689e-05  eta: 2:53:53  time: 1.1648  data_time: 0.0241  memory: 11207  loss: 58.4996  decode.loss_cls: 0.5601  decode.loss_mask: 1.8481  decode.loss_dice: 1.5902  decode.d0.loss_cls: 0.6161  decode.d0.loss_mask: 1.8555  decode.d0.loss_dice: 1.6555  decode.d1.loss_cls: 0.5636  decode.d1.loss_mask: 1.8551  decode.d1.loss_dice: 1.5939  decode.d2.loss_cls: 0.5246  decode.d2.loss_mask: 1.8555  decode.d2.loss_dice: 1.6063  decode.d3.loss_cls: 0.5878  decode.d3.loss_mask: 1.8298  decode.d3.loss_dice: 1.5939  decode.d4.loss_cls: 0.5075  decode.d4.loss_mask: 1.8441  decode.d4.loss_dice: 1.6051  decode.d5.loss_cls: 0.5738  decode.d5.loss_mask: 1.8697  decode.d5.loss_dice: 1.5805  decode.d6.loss_cls: 0.5727  decode.d6.loss_mask: 1.8416  decode.d6.loss_dice: 1.5971  decode.d7.loss_cls: 0.5744  decode.d7.loss_mask: 1.8575  decode.d7.loss_dice: 1.6140  decode.d8.loss_cls: 0.5678  decode.d8.loss_mask: 1.8542  decode.d8.loss_dice: 1.5588  mix_decode.loss_cls: 0.2710  mix_decode.loss_mask: 0.6605  mix_decode.loss_dice: 0.8438  mix_decode.d0.loss_cls: 0.2767  mix_decode.d0.loss_mask: 0.6611  mix_decode.d0.loss_dice: 0.8917  mix_decode.d1.loss_cls: 0.3006  mix_decode.d1.loss_mask: 0.6805  mix_decode.d1.loss_dice: 0.8400  mix_decode.d2.loss_cls: 0.3319  mix_decode.d2.loss_mask: 0.6930  mix_decode.d2.loss_dice: 0.8536  mix_decode.d3.loss_cls: 0.3768  mix_decode.d3.loss_mask: 0.6910  mix_decode.d3.loss_dice: 0.8525  mix_decode.d4.loss_cls: 0.3389  mix_decode.d4.loss_mask: 0.6544  mix_decode.d4.loss_dice: 0.8395  mix_decode.d5.loss_cls: 0.3284  mix_decode.d5.loss_mask: 0.6600  mix_decode.d5.loss_dice: 0.8401  mix_decode.d6.loss_cls: 0.3074  mix_decode.d6.loss_mask: 0.6641  mix_decode.d6.loss_dice: 0.8569  mix_decode.d7.loss_cls: 0.3091  mix_decode.d7.loss_mask: 0.6552  mix_decode.d7.loss_dice: 0.8444  mix_decode.d8.loss_cls: 0.2900  mix_decode.d8.loss_mask: 0.6737  mix_decode.d8.loss_dice: 0.8580
2025/03/29 13:45:20 - mmengine - INFO - Iter(train) [ 7300/20000]  base_lr: 6.6453e-05 lr: 6.6453e-05  eta: 2:53:41  time: 1.1535  data_time: 0.0224  memory: 11219  loss: 58.6177  decode.loss_cls: 0.3467  decode.loss_mask: 1.8766  decode.loss_dice: 1.8650  decode.d0.loss_cls: 0.3124  decode.d0.loss_mask: 1.9270  decode.d0.loss_dice: 1.9389  decode.d1.loss_cls: 0.3181  decode.d1.loss_mask: 1.8989  decode.d1.loss_dice: 1.8720  decode.d2.loss_cls: 0.3658  decode.d2.loss_mask: 1.8808  decode.d2.loss_dice: 1.8280  decode.d3.loss_cls: 0.3125  decode.d3.loss_mask: 1.8670  decode.d3.loss_dice: 1.8369  decode.d4.loss_cls: 0.4140  decode.d4.loss_mask: 1.8359  decode.d4.loss_dice: 1.8016  decode.d5.loss_cls: 0.3502  decode.d5.loss_mask: 1.9007  decode.d5.loss_dice: 1.8401  decode.d6.loss_cls: 0.4134  decode.d6.loss_mask: 1.8540  decode.d6.loss_dice: 1.8788  decode.d7.loss_cls: 0.4555  decode.d7.loss_mask: 1.8363  decode.d7.loss_dice: 1.8106  decode.d8.loss_cls: 0.3905  decode.d8.loss_mask: 1.8696  decode.d8.loss_dice: 1.8614  mix_decode.loss_cls: 0.3620  mix_decode.loss_mask: 0.6187  mix_decode.loss_dice: 0.7681  mix_decode.d0.loss_cls: 0.2535  mix_decode.d0.loss_mask: 0.6530  mix_decode.d0.loss_dice: 0.8534  mix_decode.d1.loss_cls: 0.2959  mix_decode.d1.loss_mask: 0.6173  mix_decode.d1.loss_dice: 0.7780  mix_decode.d2.loss_cls: 0.3046  mix_decode.d2.loss_mask: 0.6319  mix_decode.d2.loss_dice: 0.7926  mix_decode.d3.loss_cls: 0.4001  mix_decode.d3.loss_mask: 0.6331  mix_decode.d3.loss_dice: 0.7743  mix_decode.d4.loss_cls: 0.3890  mix_decode.d4.loss_mask: 0.6193  mix_decode.d4.loss_dice: 0.7564  mix_decode.d5.loss_cls: 0.3858  mix_decode.d5.loss_mask: 0.6128  mix_decode.d5.loss_dice: 0.7726  mix_decode.d6.loss_cls: 0.3931  mix_decode.d6.loss_mask: 0.6214  mix_decode.d6.loss_dice: 0.7896  mix_decode.d7.loss_cls: 0.3846  mix_decode.d7.loss_mask: 0.6534  mix_decode.d7.loss_dice: 0.7718  mix_decode.d8.loss_cls: 0.3961  mix_decode.d8.loss_mask: 0.6123  mix_decode.d8.loss_dice: 0.7637
2025/03/29 13:46:18 - mmengine - INFO - Iter(train) [ 7350/20000]  base_lr: 6.6218e-05 lr: 6.6218e-05  eta: 2:53:29  time: 1.1519  data_time: 0.0223  memory: 11212  loss: 59.1227  decode.loss_cls: 0.3413  decode.loss_mask: 2.0573  decode.loss_dice: 1.8902  decode.d0.loss_cls: 0.4868  decode.d0.loss_mask: 1.8970  decode.d0.loss_dice: 1.9012  decode.d1.loss_cls: 0.4064  decode.d1.loss_mask: 1.9145  decode.d1.loss_dice: 1.8483  decode.d2.loss_cls: 0.4197  decode.d2.loss_mask: 1.8673  decode.d2.loss_dice: 1.7524  decode.d3.loss_cls: 0.4701  decode.d3.loss_mask: 1.8627  decode.d3.loss_dice: 1.7570  decode.d4.loss_cls: 0.4545  decode.d4.loss_mask: 1.9136  decode.d4.loss_dice: 1.8029  decode.d5.loss_cls: 0.4960  decode.d5.loss_mask: 1.8665  decode.d5.loss_dice: 1.7691  decode.d6.loss_cls: 0.5413  decode.d6.loss_mask: 1.8656  decode.d6.loss_dice: 1.7624  decode.d7.loss_cls: 0.5687  decode.d7.loss_mask: 1.8817  decode.d7.loss_dice: 1.8495  decode.d8.loss_cls: 0.4326  decode.d8.loss_mask: 1.9105  decode.d8.loss_dice: 1.8652  mix_decode.loss_cls: 0.2587  mix_decode.loss_mask: 0.6627  mix_decode.loss_dice: 0.8068  mix_decode.d0.loss_cls: 0.3067  mix_decode.d0.loss_mask: 0.6359  mix_decode.d0.loss_dice: 0.8160  mix_decode.d1.loss_cls: 0.3105  mix_decode.d1.loss_mask: 0.6274  mix_decode.d1.loss_dice: 0.7508  mix_decode.d2.loss_cls: 0.3196  mix_decode.d2.loss_mask: 0.6342  mix_decode.d2.loss_dice: 0.7606  mix_decode.d3.loss_cls: 0.3056  mix_decode.d3.loss_mask: 0.6387  mix_decode.d3.loss_dice: 0.7567  mix_decode.d4.loss_cls: 0.3146  mix_decode.d4.loss_mask: 0.6891  mix_decode.d4.loss_dice: 0.7519  mix_decode.d5.loss_cls: 0.3183  mix_decode.d5.loss_mask: 0.6321  mix_decode.d5.loss_dice: 0.7759  mix_decode.d6.loss_cls: 0.3535  mix_decode.d6.loss_mask: 0.6211  mix_decode.d6.loss_dice: 0.7616  mix_decode.d7.loss_cls: 0.3629  mix_decode.d7.loss_mask: 0.6276  mix_decode.d7.loss_dice: 0.7579  mix_decode.d8.loss_cls: 0.2960  mix_decode.d8.loss_mask: 0.6605  mix_decode.d8.loss_dice: 0.7566
2025/03/29 13:47:15 - mmengine - INFO - Iter(train) [ 7400/20000]  base_lr: 6.5982e-05 lr: 6.5982e-05  eta: 2:53:16  time: 1.1507  data_time: 0.0218  memory: 11208  loss: 60.0053  decode.loss_cls: 0.4944  decode.loss_mask: 1.8051  decode.loss_dice: 1.8922  decode.d0.loss_cls: 0.5082  decode.d0.loss_mask: 1.8367  decode.d0.loss_dice: 1.9293  decode.d1.loss_cls: 0.6384  decode.d1.loss_mask: 1.7704  decode.d1.loss_dice: 1.8101  decode.d2.loss_cls: 0.5437  decode.d2.loss_mask: 1.7794  decode.d2.loss_dice: 1.8258  decode.d3.loss_cls: 0.5540  decode.d3.loss_mask: 1.7858  decode.d3.loss_dice: 1.8464  decode.d4.loss_cls: 0.4993  decode.d4.loss_mask: 1.8368  decode.d4.loss_dice: 1.8425  decode.d5.loss_cls: 0.5414  decode.d5.loss_mask: 1.8034  decode.d5.loss_dice: 1.8924  decode.d6.loss_cls: 0.5318  decode.d6.loss_mask: 1.8519  decode.d6.loss_dice: 1.9087  decode.d7.loss_cls: 0.5981  decode.d7.loss_mask: 1.7897  decode.d7.loss_dice: 1.8495  decode.d8.loss_cls: 0.4452  decode.d8.loss_mask: 1.8326  decode.d8.loss_dice: 1.8688  mix_decode.loss_cls: 0.3403  mix_decode.loss_mask: 0.6478  mix_decode.loss_dice: 0.7804  mix_decode.d0.loss_cls: 0.3286  mix_decode.d0.loss_mask: 0.7051  mix_decode.d0.loss_dice: 0.8489  mix_decode.d1.loss_cls: 0.3210  mix_decode.d1.loss_mask: 0.6714  mix_decode.d1.loss_dice: 0.7680  mix_decode.d2.loss_cls: 0.3161  mix_decode.d2.loss_mask: 0.7004  mix_decode.d2.loss_dice: 0.7618  mix_decode.d3.loss_cls: 0.3522  mix_decode.d3.loss_mask: 0.6463  mix_decode.d3.loss_dice: 0.7825  mix_decode.d4.loss_cls: 0.3484  mix_decode.d4.loss_mask: 0.6526  mix_decode.d4.loss_dice: 0.7568  mix_decode.d5.loss_cls: 0.3447  mix_decode.d5.loss_mask: 0.6837  mix_decode.d5.loss_dice: 0.7834  mix_decode.d6.loss_cls: 0.3569  mix_decode.d6.loss_mask: 0.6927  mix_decode.d6.loss_dice: 0.7871  mix_decode.d7.loss_cls: 0.3344  mix_decode.d7.loss_mask: 0.6766  mix_decode.d7.loss_dice: 0.7611  mix_decode.d8.loss_cls: 0.2761  mix_decode.d8.loss_mask: 0.6931  mix_decode.d8.loss_dice: 0.7748
2025/03/29 13:48:13 - mmengine - INFO - Iter(train) [ 7450/20000]  base_lr: 6.5746e-05 lr: 6.5746e-05  eta: 2:53:03  time: 1.1578  data_time: 0.0237  memory: 11219  loss: 61.0456  decode.loss_cls: 0.3733  decode.loss_mask: 1.9983  decode.loss_dice: 1.8792  decode.d0.loss_cls: 0.5446  decode.d0.loss_mask: 1.9437  decode.d0.loss_dice: 1.8756  decode.d1.loss_cls: 0.3803  decode.d1.loss_mask: 1.9179  decode.d1.loss_dice: 1.8387  decode.d2.loss_cls: 0.3489  decode.d2.loss_mask: 1.9031  decode.d2.loss_dice: 1.8115  decode.d3.loss_cls: 0.3782  decode.d3.loss_mask: 1.9186  decode.d3.loss_dice: 1.8290  decode.d4.loss_cls: 0.3048  decode.d4.loss_mask: 1.9793  decode.d4.loss_dice: 1.8682  decode.d5.loss_cls: 0.4335  decode.d5.loss_mask: 1.9062  decode.d5.loss_dice: 1.8213  decode.d6.loss_cls: 0.4128  decode.d6.loss_mask: 1.9514  decode.d6.loss_dice: 1.8257  decode.d7.loss_cls: 0.4024  decode.d7.loss_mask: 1.9584  decode.d7.loss_dice: 1.8330  decode.d8.loss_cls: 0.3445  decode.d8.loss_mask: 1.9654  decode.d8.loss_dice: 1.8687  mix_decode.loss_cls: 0.3471  mix_decode.loss_mask: 0.6947  mix_decode.loss_dice: 0.9467  mix_decode.d0.loss_cls: 0.2959  mix_decode.d0.loss_mask: 0.6853  mix_decode.d0.loss_dice: 0.9593  mix_decode.d1.loss_cls: 0.2717  mix_decode.d1.loss_mask: 0.6601  mix_decode.d1.loss_dice: 0.9420  mix_decode.d2.loss_cls: 0.2783  mix_decode.d2.loss_mask: 0.6673  mix_decode.d2.loss_dice: 0.9234  mix_decode.d3.loss_cls: 0.3045  mix_decode.d3.loss_mask: 0.6634  mix_decode.d3.loss_dice: 0.9030  mix_decode.d4.loss_cls: 0.3421  mix_decode.d4.loss_mask: 0.6555  mix_decode.d4.loss_dice: 0.8973  mix_decode.d5.loss_cls: 0.3206  mix_decode.d5.loss_mask: 0.6565  mix_decode.d5.loss_dice: 0.9255  mix_decode.d6.loss_cls: 0.3552  mix_decode.d6.loss_mask: 0.6664  mix_decode.d6.loss_dice: 0.9410  mix_decode.d7.loss_cls: 0.3595  mix_decode.d7.loss_mask: 0.6473  mix_decode.d7.loss_dice: 0.9383  mix_decode.d8.loss_cls: 0.3154  mix_decode.d8.loss_mask: 0.7143  mix_decode.d8.loss_dice: 0.9516
2025/03/29 13:49:10 - mmengine - INFO - Iter(train) [ 7500/20000]  base_lr: 6.5511e-05 lr: 6.5511e-05  eta: 2:52:48  time: 1.1474  data_time: 0.0226  memory: 11214  loss: 52.4186  decode.loss_cls: 0.3452  decode.loss_mask: 1.6684  decode.loss_dice: 1.6092  decode.d0.loss_cls: 0.4071  decode.d0.loss_mask: 1.7211  decode.d0.loss_dice: 1.6030  decode.d1.loss_cls: 0.2695  decode.d1.loss_mask: 1.6884  decode.d1.loss_dice: 1.6397  decode.d2.loss_cls: 0.3069  decode.d2.loss_mask: 1.7071  decode.d2.loss_dice: 1.6352  decode.d3.loss_cls: 0.3471  decode.d3.loss_mask: 1.6939  decode.d3.loss_dice: 1.6425  decode.d4.loss_cls: 0.2741  decode.d4.loss_mask: 1.6904  decode.d4.loss_dice: 1.6314  decode.d5.loss_cls: 0.3060  decode.d5.loss_mask: 1.7034  decode.d5.loss_dice: 1.6056  decode.d6.loss_cls: 0.3620  decode.d6.loss_mask: 1.7501  decode.d6.loss_dice: 1.6242  decode.d7.loss_cls: 0.3672  decode.d7.loss_mask: 1.6786  decode.d7.loss_dice: 1.5821  decode.d8.loss_cls: 0.3011  decode.d8.loss_mask: 1.6468  decode.d8.loss_dice: 1.6031  mix_decode.loss_cls: 0.3154  mix_decode.loss_mask: 0.5440  mix_decode.loss_dice: 0.7485  mix_decode.d0.loss_cls: 0.2765  mix_decode.d0.loss_mask: 0.5524  mix_decode.d0.loss_dice: 0.8031  mix_decode.d1.loss_cls: 0.2500  mix_decode.d1.loss_mask: 0.5439  mix_decode.d1.loss_dice: 0.7632  mix_decode.d2.loss_cls: 0.3453  mix_decode.d2.loss_mask: 0.5307  mix_decode.d2.loss_dice: 0.7306  mix_decode.d3.loss_cls: 0.3163  mix_decode.d3.loss_mask: 0.5459  mix_decode.d3.loss_dice: 0.7536  mix_decode.d4.loss_cls: 0.3175  mix_decode.d4.loss_mask: 0.5287  mix_decode.d4.loss_dice: 0.7422  mix_decode.d5.loss_cls: 0.3571  mix_decode.d5.loss_mask: 0.5249  mix_decode.d5.loss_dice: 0.7123  mix_decode.d6.loss_cls: 0.3562  mix_decode.d6.loss_mask: 0.5229  mix_decode.d6.loss_dice: 0.7222  mix_decode.d7.loss_cls: 0.3243  mix_decode.d7.loss_mask: 0.5324  mix_decode.d7.loss_dice: 0.7422  mix_decode.d8.loss_cls: 0.3212  mix_decode.d8.loss_mask: 0.5422  mix_decode.d8.loss_dice: 0.7426
2025/03/29 13:50:08 - mmengine - INFO - Iter(train) [ 7550/20000]  base_lr: 6.5275e-05 lr: 6.5275e-05  eta: 2:52:33  time: 1.1507  data_time: 0.0222  memory: 11205  loss: 61.6961  decode.loss_cls: 0.5201  decode.loss_mask: 2.0135  decode.loss_dice: 1.7131  decode.d0.loss_cls: 0.5071  decode.d0.loss_mask: 2.0739  decode.d0.loss_dice: 1.7715  decode.d1.loss_cls: 0.5151  decode.d1.loss_mask: 1.9385  decode.d1.loss_dice: 1.6318  decode.d2.loss_cls: 0.5858  decode.d2.loss_mask: 1.9729  decode.d2.loss_dice: 1.6905  decode.d3.loss_cls: 0.5141  decode.d3.loss_mask: 2.0395  decode.d3.loss_dice: 1.7079  decode.d4.loss_cls: 0.5289  decode.d4.loss_mask: 2.0047  decode.d4.loss_dice: 1.6922  decode.d5.loss_cls: 0.5670  decode.d5.loss_mask: 2.0155  decode.d5.loss_dice: 1.7052  decode.d6.loss_cls: 0.5723  decode.d6.loss_mask: 2.0132  decode.d6.loss_dice: 1.7076  decode.d7.loss_cls: 0.5530  decode.d7.loss_mask: 1.9956  decode.d7.loss_dice: 1.7081  decode.d8.loss_cls: 0.4749  decode.d8.loss_mask: 2.0534  decode.d8.loss_dice: 1.6863  mix_decode.loss_cls: 0.2469  mix_decode.loss_mask: 0.7858  mix_decode.loss_dice: 0.8622  mix_decode.d0.loss_cls: 0.3232  mix_decode.d0.loss_mask: 0.8054  mix_decode.d0.loss_dice: 0.8891  mix_decode.d1.loss_cls: 0.2811  mix_decode.d1.loss_mask: 0.7616  mix_decode.d1.loss_dice: 0.8439  mix_decode.d2.loss_cls: 0.2762  mix_decode.d2.loss_mask: 0.7793  mix_decode.d2.loss_dice: 0.8481  mix_decode.d3.loss_cls: 0.2928  mix_decode.d3.loss_mask: 0.7721  mix_decode.d3.loss_dice: 0.8520  mix_decode.d4.loss_cls: 0.2891  mix_decode.d4.loss_mask: 0.7590  mix_decode.d4.loss_dice: 0.8615  mix_decode.d5.loss_cls: 0.3192  mix_decode.d5.loss_mask: 0.7799  mix_decode.d5.loss_dice: 0.8494  mix_decode.d6.loss_cls: 0.2812  mix_decode.d6.loss_mask: 0.7878  mix_decode.d6.loss_dice: 0.8366  mix_decode.d7.loss_cls: 0.2981  mix_decode.d7.loss_mask: 0.7813  mix_decode.d7.loss_dice: 0.8441  mix_decode.d8.loss_cls: 0.2562  mix_decode.d8.loss_mask: 0.8036  mix_decode.d8.loss_dice: 0.8560
2025/03/29 13:51:05 - mmengine - INFO - Iter(train) [ 7600/20000]  base_lr: 6.5039e-05 lr: 6.5039e-05  eta: 2:52:17  time: 1.1529  data_time: 0.0230  memory: 11211  loss: 60.9154  decode.loss_cls: 0.3565  decode.loss_mask: 1.8895  decode.loss_dice: 1.8511  decode.d0.loss_cls: 0.5296  decode.d0.loss_mask: 1.9074  decode.d0.loss_dice: 1.8094  decode.d1.loss_cls: 0.4237  decode.d1.loss_mask: 1.8971  decode.d1.loss_dice: 1.8300  decode.d2.loss_cls: 0.3889  decode.d2.loss_mask: 1.9068  decode.d2.loss_dice: 1.8083  decode.d3.loss_cls: 0.3688  decode.d3.loss_mask: 1.9679  decode.d3.loss_dice: 1.8027  decode.d4.loss_cls: 0.4467  decode.d4.loss_mask: 1.8378  decode.d4.loss_dice: 1.8154  decode.d5.loss_cls: 0.4040  decode.d5.loss_mask: 1.8488  decode.d5.loss_dice: 1.8475  decode.d6.loss_cls: 0.3950  decode.d6.loss_mask: 1.8878  decode.d6.loss_dice: 1.8785  decode.d7.loss_cls: 0.4459  decode.d7.loss_mask: 1.8101  decode.d7.loss_dice: 1.7839  decode.d8.loss_cls: 0.3832  decode.d8.loss_mask: 1.8730  decode.d8.loss_dice: 1.8522  mix_decode.loss_cls: 0.2774  mix_decode.loss_mask: 0.8195  mix_decode.loss_dice: 0.9005  mix_decode.d0.loss_cls: 0.3217  mix_decode.d0.loss_mask: 0.7972  mix_decode.d0.loss_dice: 0.8393  mix_decode.d1.loss_cls: 0.2618  mix_decode.d1.loss_mask: 0.8052  mix_decode.d1.loss_dice: 0.8650  mix_decode.d2.loss_cls: 0.3015  mix_decode.d2.loss_mask: 0.7968  mix_decode.d2.loss_dice: 0.8507  mix_decode.d3.loss_cls: 0.2716  mix_decode.d3.loss_mask: 0.8373  mix_decode.d3.loss_dice: 0.8814  mix_decode.d4.loss_cls: 0.3048  mix_decode.d4.loss_mask: 0.8266  mix_decode.d4.loss_dice: 0.8678  mix_decode.d5.loss_cls: 0.2787  mix_decode.d5.loss_mask: 0.8230  mix_decode.d5.loss_dice: 0.8944  mix_decode.d6.loss_cls: 0.3272  mix_decode.d6.loss_mask: 0.7716  mix_decode.d6.loss_dice: 0.8599  mix_decode.d7.loss_cls: 0.2774  mix_decode.d7.loss_mask: 0.8040  mix_decode.d7.loss_dice: 0.8703  mix_decode.d8.loss_cls: 0.2643  mix_decode.d8.loss_mask: 0.7959  mix_decode.d8.loss_dice: 0.8752
2025/03/29 13:52:03 - mmengine - INFO - Iter(train) [ 7650/20000]  base_lr: 6.4803e-05 lr: 6.4803e-05  eta: 2:52:02  time: 1.1547  data_time: 0.0222  memory: 11213  loss: 54.6352  decode.loss_cls: 0.3549  decode.loss_mask: 1.8485  decode.loss_dice: 1.6032  decode.d0.loss_cls: 0.4857  decode.d0.loss_mask: 1.8413  decode.d0.loss_dice: 1.5778  decode.d1.loss_cls: 0.3326  decode.d1.loss_mask: 1.8515  decode.d1.loss_dice: 1.5852  decode.d2.loss_cls: 0.4267  decode.d2.loss_mask: 1.8015  decode.d2.loss_dice: 1.5404  decode.d3.loss_cls: 0.3964  decode.d3.loss_mask: 1.8110  decode.d3.loss_dice: 1.5433  decode.d4.loss_cls: 0.4037  decode.d4.loss_mask: 1.8137  decode.d4.loss_dice: 1.5415  decode.d5.loss_cls: 0.4488  decode.d5.loss_mask: 1.7810  decode.d5.loss_dice: 1.5370  decode.d6.loss_cls: 0.3701  decode.d6.loss_mask: 1.8274  decode.d6.loss_dice: 1.6369  decode.d7.loss_cls: 0.3945  decode.d7.loss_mask: 1.8460  decode.d7.loss_dice: 1.5738  decode.d8.loss_cls: 0.2961  decode.d8.loss_mask: 1.8440  decode.d8.loss_dice: 1.5781  mix_decode.loss_cls: 0.2107  mix_decode.loss_mask: 0.6157  mix_decode.loss_dice: 0.8338  mix_decode.d0.loss_cls: 0.2258  mix_decode.d0.loss_mask: 0.6336  mix_decode.d0.loss_dice: 0.8552  mix_decode.d1.loss_cls: 0.2152  mix_decode.d1.loss_mask: 0.6186  mix_decode.d1.loss_dice: 0.8215  mix_decode.d2.loss_cls: 0.2108  mix_decode.d2.loss_mask: 0.6279  mix_decode.d2.loss_dice: 0.8239  mix_decode.d3.loss_cls: 0.1932  mix_decode.d3.loss_mask: 0.6324  mix_decode.d3.loss_dice: 0.8444  mix_decode.d4.loss_cls: 0.2223  mix_decode.d4.loss_mask: 0.6111  mix_decode.d4.loss_dice: 0.8308  mix_decode.d5.loss_cls: 0.2273  mix_decode.d5.loss_mask: 0.6121  mix_decode.d5.loss_dice: 0.8385  mix_decode.d6.loss_cls: 0.2544  mix_decode.d6.loss_mask: 0.6057  mix_decode.d6.loss_dice: 0.8171  mix_decode.d7.loss_cls: 0.2131  mix_decode.d7.loss_mask: 0.6503  mix_decode.d7.loss_dice: 0.8444  mix_decode.d8.loss_cls: 0.1930  mix_decode.d8.loss_mask: 0.6277  mix_decode.d8.loss_dice: 0.8325
2025/03/29 13:53:01 - mmengine - INFO - Iter(train) [ 7700/20000]  base_lr: 6.4566e-05 lr: 6.4566e-05  eta: 2:51:45  time: 1.1585  data_time: 0.0239  memory: 11225  loss: 66.1084  decode.loss_cls: 0.5152  decode.loss_mask: 1.9974  decode.loss_dice: 2.0522  decode.d0.loss_cls: 0.5848  decode.d0.loss_mask: 2.0326  decode.d0.loss_dice: 2.0866  decode.d1.loss_cls: 0.5902  decode.d1.loss_mask: 1.9384  decode.d1.loss_dice: 2.0315  decode.d2.loss_cls: 0.5636  decode.d2.loss_mask: 1.9337  decode.d2.loss_dice: 2.0128  decode.d3.loss_cls: 0.6301  decode.d3.loss_mask: 1.9720  decode.d3.loss_dice: 1.9534  decode.d4.loss_cls: 0.4565  decode.d4.loss_mask: 1.9813  decode.d4.loss_dice: 2.0689  decode.d5.loss_cls: 0.6853  decode.d5.loss_mask: 1.9572  decode.d5.loss_dice: 2.0377  decode.d6.loss_cls: 0.5591  decode.d6.loss_mask: 2.0422  decode.d6.loss_dice: 2.0130  decode.d7.loss_cls: 0.6035  decode.d7.loss_mask: 2.0390  decode.d7.loss_dice: 2.0175  decode.d8.loss_cls: 0.5193  decode.d8.loss_mask: 2.0024  decode.d8.loss_dice: 2.0421  mix_decode.loss_cls: 0.3691  mix_decode.loss_mask: 0.8302  mix_decode.loss_dice: 0.8328  mix_decode.d0.loss_cls: 0.3853  mix_decode.d0.loss_mask: 0.7794  mix_decode.d0.loss_dice: 0.9107  mix_decode.d1.loss_cls: 0.3969  mix_decode.d1.loss_mask: 0.7800  mix_decode.d1.loss_dice: 0.8131  mix_decode.d2.loss_cls: 0.4008  mix_decode.d2.loss_mask: 0.7844  mix_decode.d2.loss_dice: 0.8276  mix_decode.d3.loss_cls: 0.4158  mix_decode.d3.loss_mask: 0.7819  mix_decode.d3.loss_dice: 0.7854  mix_decode.d4.loss_cls: 0.4376  mix_decode.d4.loss_mask: 0.7862  mix_decode.d4.loss_dice: 0.7919  mix_decode.d5.loss_cls: 0.4390  mix_decode.d5.loss_mask: 0.7537  mix_decode.d5.loss_dice: 0.7837  mix_decode.d6.loss_cls: 0.4203  mix_decode.d6.loss_mask: 0.8078  mix_decode.d6.loss_dice: 0.8087  mix_decode.d7.loss_cls: 0.4341  mix_decode.d7.loss_mask: 0.8124  mix_decode.d7.loss_dice: 0.7910  mix_decode.d8.loss_cls: 0.4061  mix_decode.d8.loss_mask: 0.8269  mix_decode.d8.loss_dice: 0.7961
2025/03/29 13:53:58 - mmengine - INFO - Iter(train) [ 7750/20000]  base_lr: 6.4330e-05 lr: 6.4330e-05  eta: 2:51:28  time: 1.1438  data_time: 0.0228  memory: 11207  loss: 63.4629  decode.loss_cls: 0.4335  decode.loss_mask: 2.2987  decode.loss_dice: 1.7525  decode.d0.loss_cls: 0.6113  decode.d0.loss_mask: 2.1660  decode.d0.loss_dice: 1.7431  decode.d1.loss_cls: 0.4456  decode.d1.loss_mask: 2.2577  decode.d1.loss_dice: 1.7321  decode.d2.loss_cls: 0.4677  decode.d2.loss_mask: 2.2606  decode.d2.loss_dice: 1.7435  decode.d3.loss_cls: 0.3779  decode.d3.loss_mask: 2.3129  decode.d3.loss_dice: 1.7141  decode.d4.loss_cls: 0.3666  decode.d4.loss_mask: 2.3603  decode.d4.loss_dice: 1.7001  decode.d5.loss_cls: 0.4169  decode.d5.loss_mask: 2.2453  decode.d5.loss_dice: 1.7680  decode.d6.loss_cls: 0.5385  decode.d6.loss_mask: 2.2282  decode.d6.loss_dice: 1.7241  decode.d7.loss_cls: 0.3592  decode.d7.loss_mask: 2.3039  decode.d7.loss_dice: 1.7146  decode.d8.loss_cls: 0.3646  decode.d8.loss_mask: 2.3507  decode.d8.loss_dice: 1.7680  mix_decode.loss_cls: 0.3539  mix_decode.loss_mask: 0.7046  mix_decode.loss_dice: 0.8766  mix_decode.d0.loss_cls: 0.2335  mix_decode.d0.loss_mask: 0.7085  mix_decode.d0.loss_dice: 0.9542  mix_decode.d1.loss_cls: 0.2689  mix_decode.d1.loss_mask: 0.7200  mix_decode.d1.loss_dice: 0.9038  mix_decode.d2.loss_cls: 0.2483  mix_decode.d2.loss_mask: 0.7306  mix_decode.d2.loss_dice: 0.9378  mix_decode.d3.loss_cls: 0.2558  mix_decode.d3.loss_mask: 0.7468  mix_decode.d3.loss_dice: 0.8848  mix_decode.d4.loss_cls: 0.2754  mix_decode.d4.loss_mask: 0.7286  mix_decode.d4.loss_dice: 0.8778  mix_decode.d5.loss_cls: 0.3149  mix_decode.d5.loss_mask: 0.7117  mix_decode.d5.loss_dice: 0.9001  mix_decode.d6.loss_cls: 0.3313  mix_decode.d6.loss_mask: 0.7030  mix_decode.d6.loss_dice: 0.8744  mix_decode.d7.loss_cls: 0.2743  mix_decode.d7.loss_mask: 0.6822  mix_decode.d7.loss_dice: 0.8617  mix_decode.d8.loss_cls: 0.2944  mix_decode.d8.loss_mask: 0.7201  mix_decode.d8.loss_dice: 0.8588
2025/03/29 13:54:55 - mmengine - INFO - Iter(train) [ 7800/20000]  base_lr: 6.4094e-05 lr: 6.4094e-05  eta: 2:51:09  time: 1.1451  data_time: 0.0228  memory: 11225  loss: 59.3980  decode.loss_cls: 0.2870  decode.loss_mask: 1.9795  decode.loss_dice: 1.8210  decode.d0.loss_cls: 0.4079  decode.d0.loss_mask: 2.0356  decode.d0.loss_dice: 1.8917  decode.d1.loss_cls: 0.3369  decode.d1.loss_mask: 1.9551  decode.d1.loss_dice: 1.8389  decode.d2.loss_cls: 0.3534  decode.d2.loss_mask: 1.9453  decode.d2.loss_dice: 1.8621  decode.d3.loss_cls: 0.3469  decode.d3.loss_mask: 1.9459  decode.d3.loss_dice: 1.8558  decode.d4.loss_cls: 0.3366  decode.d4.loss_mask: 1.9326  decode.d4.loss_dice: 1.8641  decode.d5.loss_cls: 0.3395  decode.d5.loss_mask: 1.9544  decode.d5.loss_dice: 1.8646  decode.d6.loss_cls: 0.3610  decode.d6.loss_mask: 1.9548  decode.d6.loss_dice: 1.8281  decode.d7.loss_cls: 0.3557  decode.d7.loss_mask: 1.9566  decode.d7.loss_dice: 1.8214  decode.d8.loss_cls: 0.3188  decode.d8.loss_mask: 1.9852  decode.d8.loss_dice: 1.8824  mix_decode.loss_cls: 0.2494  mix_decode.loss_mask: 0.6619  mix_decode.loss_dice: 0.8431  mix_decode.d0.loss_cls: 0.2188  mix_decode.d0.loss_mask: 0.6780  mix_decode.d0.loss_dice: 0.9102  mix_decode.d1.loss_cls: 0.2083  mix_decode.d1.loss_mask: 0.6638  mix_decode.d1.loss_dice: 0.8731  mix_decode.d2.loss_cls: 0.2119  mix_decode.d2.loss_mask: 0.6665  mix_decode.d2.loss_dice: 0.9013  mix_decode.d3.loss_cls: 0.2492  mix_decode.d3.loss_mask: 0.6692  mix_decode.d3.loss_dice: 0.8626  mix_decode.d4.loss_cls: 0.2307  mix_decode.d4.loss_mask: 0.6422  mix_decode.d4.loss_dice: 0.8653  mix_decode.d5.loss_cls: 0.2687  mix_decode.d5.loss_mask: 0.6487  mix_decode.d5.loss_dice: 0.8778  mix_decode.d6.loss_cls: 0.3192  mix_decode.d6.loss_mask: 0.6439  mix_decode.d6.loss_dice: 0.8572  mix_decode.d7.loss_cls: 0.2744  mix_decode.d7.loss_mask: 0.6536  mix_decode.d7.loss_dice: 0.8600  mix_decode.d8.loss_cls: 0.2612  mix_decode.d8.loss_mask: 0.6532  mix_decode.d8.loss_dice: 0.8556
2025/03/29 13:55:53 - mmengine - INFO - Iter(train) [ 7850/20000]  base_lr: 6.3857e-05 lr: 6.3857e-05  eta: 2:50:51  time: 1.1460  data_time: 0.0221  memory: 11222  loss: 64.9692  decode.loss_cls: 0.5009  decode.loss_mask: 1.9739  decode.loss_dice: 1.9977  decode.d0.loss_cls: 0.6622  decode.d0.loss_mask: 1.9504  decode.d0.loss_dice: 2.0815  decode.d1.loss_cls: 0.5901  decode.d1.loss_mask: 1.9377  decode.d1.loss_dice: 1.9532  decode.d2.loss_cls: 0.5820  decode.d2.loss_mask: 1.9314  decode.d2.loss_dice: 1.9675  decode.d3.loss_cls: 0.6751  decode.d3.loss_mask: 1.9188  decode.d3.loss_dice: 1.9509  decode.d4.loss_cls: 0.5763  decode.d4.loss_mask: 1.9807  decode.d4.loss_dice: 1.9825  decode.d5.loss_cls: 0.6308  decode.d5.loss_mask: 1.9426  decode.d5.loss_dice: 1.9559  decode.d6.loss_cls: 0.6370  decode.d6.loss_mask: 1.9297  decode.d6.loss_dice: 1.9545  decode.d7.loss_cls: 0.6439  decode.d7.loss_mask: 1.9524  decode.d7.loss_dice: 1.9408  decode.d8.loss_cls: 0.5680  decode.d8.loss_mask: 1.9819  decode.d8.loss_dice: 2.0004  mix_decode.loss_cls: 0.3870  mix_decode.loss_mask: 0.6372  mix_decode.loss_dice: 0.8859  mix_decode.d0.loss_cls: 0.4313  mix_decode.d0.loss_mask: 0.6528  mix_decode.d0.loss_dice: 0.9704  mix_decode.d1.loss_cls: 0.3852  mix_decode.d1.loss_mask: 0.6440  mix_decode.d1.loss_dice: 0.8931  mix_decode.d2.loss_cls: 0.4145  mix_decode.d2.loss_mask: 0.6682  mix_decode.d2.loss_dice: 0.8944  mix_decode.d3.loss_cls: 0.3923  mix_decode.d3.loss_mask: 0.6683  mix_decode.d3.loss_dice: 0.8851  mix_decode.d4.loss_cls: 0.4210  mix_decode.d4.loss_mask: 0.6304  mix_decode.d4.loss_dice: 0.8717  mix_decode.d5.loss_cls: 0.4153  mix_decode.d5.loss_mask: 0.6569  mix_decode.d5.loss_dice: 0.9212  mix_decode.d6.loss_cls: 0.4316  mix_decode.d6.loss_mask: 0.6181  mix_decode.d6.loss_dice: 0.8883  mix_decode.d7.loss_cls: 0.3982  mix_decode.d7.loss_mask: 0.6623  mix_decode.d7.loss_dice: 0.9070  mix_decode.d8.loss_cls: 0.4147  mix_decode.d8.loss_mask: 0.6850  mix_decode.d8.loss_dice: 0.8872
2025/03/29 13:56:50 - mmengine - INFO - Iter(train) [ 7900/20000]  base_lr: 6.3621e-05 lr: 6.3621e-05  eta: 2:50:32  time: 1.1485  data_time: 0.0224  memory: 11205  loss: 62.0314  decode.loss_cls: 0.3984  decode.loss_mask: 1.8561  decode.loss_dice: 1.9414  decode.d0.loss_cls: 0.4272  decode.d0.loss_mask: 1.8856  decode.d0.loss_dice: 2.0426  decode.d1.loss_cls: 0.4725  decode.d1.loss_mask: 1.8562  decode.d1.loss_dice: 1.9299  decode.d2.loss_cls: 0.4455  decode.d2.loss_mask: 1.8611  decode.d2.loss_dice: 1.9935  decode.d3.loss_cls: 0.4543  decode.d3.loss_mask: 1.8550  decode.d3.loss_dice: 1.9863  decode.d4.loss_cls: 0.4213  decode.d4.loss_mask: 1.8268  decode.d4.loss_dice: 1.9796  decode.d5.loss_cls: 0.4368  decode.d5.loss_mask: 1.8356  decode.d5.loss_dice: 1.9937  decode.d6.loss_cls: 0.3985  decode.d6.loss_mask: 1.7714  decode.d6.loss_dice: 1.9578  decode.d7.loss_cls: 0.4074  decode.d7.loss_mask: 1.8257  decode.d7.loss_dice: 1.9772  decode.d8.loss_cls: 0.3871  decode.d8.loss_mask: 1.8559  decode.d8.loss_dice: 1.9918  mix_decode.loss_cls: 0.3362  mix_decode.loss_mask: 0.6802  mix_decode.loss_dice: 0.8396  mix_decode.d0.loss_cls: 0.2967  mix_decode.d0.loss_mask: 0.7331  mix_decode.d0.loss_dice: 0.9745  mix_decode.d1.loss_cls: 0.2466  mix_decode.d1.loss_mask: 0.7294  mix_decode.d1.loss_dice: 0.9126  mix_decode.d2.loss_cls: 0.3464  mix_decode.d2.loss_mask: 0.7262  mix_decode.d2.loss_dice: 0.8986  mix_decode.d3.loss_cls: 0.3612  mix_decode.d3.loss_mask: 0.7488  mix_decode.d3.loss_dice: 0.8650  mix_decode.d4.loss_cls: 0.3801  mix_decode.d4.loss_mask: 0.7088  mix_decode.d4.loss_dice: 0.8901  mix_decode.d5.loss_cls: 0.3743  mix_decode.d5.loss_mask: 0.7210  mix_decode.d5.loss_dice: 0.9485  mix_decode.d6.loss_cls: 0.3321  mix_decode.d6.loss_mask: 0.7357  mix_decode.d6.loss_dice: 0.8993  mix_decode.d7.loss_cls: 0.3359  mix_decode.d7.loss_mask: 0.6998  mix_decode.d7.loss_dice: 0.8926  mix_decode.d8.loss_cls: 0.3523  mix_decode.d8.loss_mask: 0.7053  mix_decode.d8.loss_dice: 0.8882
2025/03/29 13:57:48 - mmengine - INFO - Iter(train) [ 7950/20000]  base_lr: 6.3384e-05 lr: 6.3384e-05  eta: 2:50:13  time: 1.1502  data_time: 0.0220  memory: 11212  loss: 56.4878  decode.loss_cls: 0.3542  decode.loss_mask: 1.8511  decode.loss_dice: 1.7980  decode.d0.loss_cls: 0.4809  decode.d0.loss_mask: 1.7964  decode.d0.loss_dice: 1.7471  decode.d1.loss_cls: 0.3684  decode.d1.loss_mask: 1.8182  decode.d1.loss_dice: 1.7729  decode.d2.loss_cls: 0.3680  decode.d2.loss_mask: 1.8617  decode.d2.loss_dice: 1.7788  decode.d3.loss_cls: 0.3463  decode.d3.loss_mask: 1.8268  decode.d3.loss_dice: 1.7928  decode.d4.loss_cls: 0.3628  decode.d4.loss_mask: 1.8245  decode.d4.loss_dice: 1.7514  decode.d5.loss_cls: 0.4117  decode.d5.loss_mask: 1.8209  decode.d5.loss_dice: 1.7278  decode.d6.loss_cls: 0.3909  decode.d6.loss_mask: 1.8630  decode.d6.loss_dice: 1.7509  decode.d7.loss_cls: 0.3429  decode.d7.loss_mask: 1.8664  decode.d7.loss_dice: 1.7674  decode.d8.loss_cls: 0.3404  decode.d8.loss_mask: 1.8119  decode.d8.loss_dice: 1.7586  mix_decode.loss_cls: 0.2710  mix_decode.loss_mask: 0.5998  mix_decode.loss_dice: 0.7667  mix_decode.d0.loss_cls: 0.3006  mix_decode.d0.loss_mask: 0.6042  mix_decode.d0.loss_dice: 0.8141  mix_decode.d1.loss_cls: 0.2579  mix_decode.d1.loss_mask: 0.6232  mix_decode.d1.loss_dice: 0.7492  mix_decode.d2.loss_cls: 0.2883  mix_decode.d2.loss_mask: 0.6147  mix_decode.d2.loss_dice: 0.7544  mix_decode.d3.loss_cls: 0.2912  mix_decode.d3.loss_mask: 0.6176  mix_decode.d3.loss_dice: 0.7397  mix_decode.d4.loss_cls: 0.2888  mix_decode.d4.loss_mask: 0.6669  mix_decode.d4.loss_dice: 0.7854  mix_decode.d5.loss_cls: 0.3393  mix_decode.d5.loss_mask: 0.6447  mix_decode.d5.loss_dice: 0.7279  mix_decode.d6.loss_cls: 0.2715  mix_decode.d6.loss_mask: 0.6345  mix_decode.d6.loss_dice: 0.7742  mix_decode.d7.loss_cls: 0.2951  mix_decode.d7.loss_mask: 0.5926  mix_decode.d7.loss_dice: 0.7714  mix_decode.d8.loss_cls: 0.3026  mix_decode.d8.loss_mask: 0.5791  mix_decode.d8.loss_dice: 0.7679
2025/03/29 13:58:45 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 13:58:45 - mmengine - INFO - Iter(train) [ 8000/20000]  base_lr: 6.3147e-05 lr: 6.3147e-05  eta: 2:49:54  time: 1.1588  data_time: 0.0236  memory: 11217  loss: 54.3930  decode.loss_cls: 0.2422  decode.loss_mask: 1.8755  decode.loss_dice: 1.6970  decode.d0.loss_cls: 0.4211  decode.d0.loss_mask: 1.8442  decode.d0.loss_dice: 1.6835  decode.d1.loss_cls: 0.2979  decode.d1.loss_mask: 1.8375  decode.d1.loss_dice: 1.6974  decode.d2.loss_cls: 0.2342  decode.d2.loss_mask: 1.8800  decode.d2.loss_dice: 1.7028  decode.d3.loss_cls: 0.2394  decode.d3.loss_mask: 1.8842  decode.d3.loss_dice: 1.7020  decode.d4.loss_cls: 0.2968  decode.d4.loss_mask: 1.8511  decode.d4.loss_dice: 1.7304  decode.d5.loss_cls: 0.2740  decode.d5.loss_mask: 1.8495  decode.d5.loss_dice: 1.7591  decode.d6.loss_cls: 0.2510  decode.d6.loss_mask: 1.8255  decode.d6.loss_dice: 1.7530  decode.d7.loss_cls: 0.2286  decode.d7.loss_mask: 1.8592  decode.d7.loss_dice: 1.7358  decode.d8.loss_cls: 0.2463  decode.d8.loss_mask: 1.8384  decode.d8.loss_dice: 1.7000  mix_decode.loss_cls: 0.2361  mix_decode.loss_mask: 0.6206  mix_decode.loss_dice: 0.7468  mix_decode.d0.loss_cls: 0.1898  mix_decode.d0.loss_mask: 0.6381  mix_decode.d0.loss_dice: 0.8185  mix_decode.d1.loss_cls: 0.1944  mix_decode.d1.loss_mask: 0.6179  mix_decode.d1.loss_dice: 0.7473  mix_decode.d2.loss_cls: 0.1820  mix_decode.d2.loss_mask: 0.6281  mix_decode.d2.loss_dice: 0.7616  mix_decode.d3.loss_cls: 0.2130  mix_decode.d3.loss_mask: 0.6173  mix_decode.d3.loss_dice: 0.7500  mix_decode.d4.loss_cls: 0.2512  mix_decode.d4.loss_mask: 0.5949  mix_decode.d4.loss_dice: 0.7485  mix_decode.d5.loss_cls: 0.2146  mix_decode.d5.loss_mask: 0.6043  mix_decode.d5.loss_dice: 0.7748  mix_decode.d6.loss_cls: 0.2445  mix_decode.d6.loss_mask: 0.6050  mix_decode.d6.loss_dice: 0.7548  mix_decode.d7.loss_cls: 0.2402  mix_decode.d7.loss_mask: 0.6166  mix_decode.d7.loss_dice: 0.7628  mix_decode.d8.loss_cls: 0.2235  mix_decode.d8.loss_mask: 0.6006  mix_decode.d8.loss_dice: 0.7577
2025/03/29 13:58:45 - mmengine - INFO - Saving checkpoint at 8000 iterations
2025/03/29 13:58:51 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:05:58  time: 0.0911  data_time: 0.0018  memory: 3083  
2025/03/29 13:58:55 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:55  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 13:59:00 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:49  time: 0.0911  data_time: 0.0017  memory: 3083  
2025/03/29 13:59:04 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:44  time: 0.0907  data_time: 0.0017  memory: 3083  
2025/03/29 13:59:09 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:39  time: 0.0909  data_time: 0.0018  memory: 3083  
2025/03/29 13:59:14 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:35  time: 0.0915  data_time: 0.0019  memory: 3083  
2025/03/29 13:59:18 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:30  time: 0.0910  data_time: 0.0018  memory: 3083  
2025/03/29 13:59:23 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:26  time: 0.0912  data_time: 0.0018  memory: 3083  
2025/03/29 13:59:27 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:21  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 13:59:32 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:16  time: 0.0912  data_time: 0.0017  memory: 3083  
2025/03/29 13:59:37 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:12  time: 0.0919  data_time: 0.0017  memory: 3083  
2025/03/29 13:59:41 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:08  time: 0.0911  data_time: 0.0017  memory: 3083  
2025/03/29 13:59:46 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:03  time: 0.0912  data_time: 0.0018  memory: 3083  
2025/03/29 13:59:50 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:58  time: 0.0923  data_time: 0.0019  memory: 3083  
2025/03/29 13:59:55 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:54  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:00 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:50  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:04 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:45  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:09 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:41  time: 0.0921  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:13 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:36  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:18 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:32  time: 0.0920  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:23 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:27  time: 0.0918  data_time: 0.0017  memory: 3083  
2025/03/29 14:00:27 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:23  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:32 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:18  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:36 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:14  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:41 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:09  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:46 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:04  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:50 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:04:00  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:55 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:55  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:00:59 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:04 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:46  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:09 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:13 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:37  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:18 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:22 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:27  time: 0.0914  data_time: 0.0017  memory: 3083  
2025/03/29 14:01:27 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:23  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:31 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:36 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:14  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:41 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:45 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:05  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:50 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:54 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:01:59 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:51  time: 0.0912  data_time: 0.0016  memory: 3083  
2025/03/29 14:02:04 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:08 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:42  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:13 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0914  data_time: 0.0017  memory: 3083  
2025/03/29 14:02:17 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:22 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:26 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0914  data_time: 0.0017  memory: 3083  
2025/03/29 14:02:31 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:19  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:36 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:40 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0920  data_time: 0.0019  memory: 3083  
2025/03/29 14:02:45 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:49 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0913  data_time: 0.0017  memory: 3083  
2025/03/29 14:02:54 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:02:59 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:03 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:47  time: 0.0918  data_time: 0.0019  memory: 3083  
2025/03/29 14:03:08 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:13 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:17 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:22 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:26 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0919  data_time: 0.0019  memory: 3083  
2025/03/29 14:03:31 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:36 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:15  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:40 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0921  data_time: 0.0017  memory: 3083  
2025/03/29 14:03:45 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:49 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:54 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:03:59 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:03 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:08 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:12 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0918  data_time: 0.0019  memory: 3083  
2025/03/29 14:04:17 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0944  data_time: 0.0020  memory: 3083  
2025/03/29 14:04:22 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:26 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:31 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:20  time: 0.0922  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:35 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:40 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:45 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:49 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:04:51 - mmengine - INFO - per class results:
2025/03/29 14:04:51 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 50.14 | 70.74 |
|   building   | 62.69 | 75.11 |
|     road     | 37.22 | 38.69 |
|    water     | 64.04 | 68.98 |
|    barren    |  8.55 | 32.25 |
|    forest    | 20.23 | 21.26 |
| agricultural | 61.93 | 70.72 |
+--------------+-------+-------+
2025/03/29 14:04:51 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 65.9700  mIoU: 43.5400  mAcc: 53.9700  data_time: 0.0018  time: 0.0918
2025/03/29 14:04:51 - mmengine - INFO - The previous best checkpoint /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0/best_mIoU_iter_6000.pth is removed
2025/03/29 14:04:52 - mmengine - INFO - The best checkpoint with 43.5400 mIoU at 8000 iter is saved to best_mIoU_iter_8000.pth.
2025/03/29 14:05:50 - mmengine - INFO - Iter(train) [ 8050/20000]  base_lr: 6.2911e-05 lr: 6.2911e-05  eta: 2:49:37  time: 1.1508  data_time: 0.0228  memory: 11224  loss: 62.3410  decode.loss_cls: 0.4950  decode.loss_mask: 1.9110  decode.loss_dice: 1.7962  decode.d0.loss_cls: 0.6428  decode.d0.loss_mask: 1.9057  decode.d0.loss_dice: 1.7451  decode.d1.loss_cls: 0.5380  decode.d1.loss_mask: 1.9714  decode.d1.loss_dice: 1.7697  decode.d2.loss_cls: 0.6474  decode.d2.loss_mask: 1.8911  decode.d2.loss_dice: 1.6743  decode.d3.loss_cls: 0.5380  decode.d3.loss_mask: 1.9190  decode.d3.loss_dice: 1.7254  decode.d4.loss_cls: 0.5414  decode.d4.loss_mask: 1.9818  decode.d4.loss_dice: 1.7415  decode.d5.loss_cls: 0.5119  decode.d5.loss_mask: 1.9627  decode.d5.loss_dice: 1.7515  decode.d6.loss_cls: 0.5451  decode.d6.loss_mask: 1.9377  decode.d6.loss_dice: 1.7755  decode.d7.loss_cls: 0.4972  decode.d7.loss_mask: 2.1174  decode.d7.loss_dice: 1.8705  decode.d8.loss_cls: 0.5796  decode.d8.loss_mask: 1.9567  decode.d8.loss_dice: 1.7832  mix_decode.loss_cls: 0.3195  mix_decode.loss_mask: 0.7377  mix_decode.loss_dice: 0.8739  mix_decode.d0.loss_cls: 0.2753  mix_decode.d0.loss_mask: 0.7725  mix_decode.d0.loss_dice: 0.9828  mix_decode.d1.loss_cls: 0.3094  mix_decode.d1.loss_mask: 0.7495  mix_decode.d1.loss_dice: 0.9036  mix_decode.d2.loss_cls: 0.3014  mix_decode.d2.loss_mask: 0.7464  mix_decode.d2.loss_dice: 0.9135  mix_decode.d3.loss_cls: 0.3395  mix_decode.d3.loss_mask: 0.7489  mix_decode.d3.loss_dice: 0.9204  mix_decode.d4.loss_cls: 0.2840  mix_decode.d4.loss_mask: 0.7593  mix_decode.d4.loss_dice: 0.9070  mix_decode.d5.loss_cls: 0.3525  mix_decode.d5.loss_mask: 0.7167  mix_decode.d5.loss_dice: 0.8609  mix_decode.d6.loss_cls: 0.3340  mix_decode.d6.loss_mask: 0.7074  mix_decode.d6.loss_dice: 0.8708  mix_decode.d7.loss_cls: 0.3327  mix_decode.d7.loss_mask: 0.7379  mix_decode.d7.loss_dice: 0.9076  mix_decode.d8.loss_cls: 0.3230  mix_decode.d8.loss_mask: 0.7369  mix_decode.d8.loss_dice: 0.8921
2025/03/29 14:06:48 - mmengine - INFO - Iter(train) [ 8100/20000]  base_lr: 6.2674e-05 lr: 6.2674e-05  eta: 2:49:16  time: 1.1514  data_time: 0.0224  memory: 11219  loss: 58.5208  decode.loss_cls: 0.6458  decode.loss_mask: 1.6980  decode.loss_dice: 1.6803  decode.d0.loss_cls: 0.6960  decode.d0.loss_mask: 1.6734  decode.d0.loss_dice: 1.7290  decode.d1.loss_cls: 0.6068  decode.d1.loss_mask: 1.6786  decode.d1.loss_dice: 1.6463  decode.d2.loss_cls: 0.6145  decode.d2.loss_mask: 1.6763  decode.d2.loss_dice: 1.6775  decode.d3.loss_cls: 0.6739  decode.d3.loss_mask: 1.6884  decode.d3.loss_dice: 1.6339  decode.d4.loss_cls: 0.6628  decode.d4.loss_mask: 1.7314  decode.d4.loss_dice: 1.7040  decode.d5.loss_cls: 0.6988  decode.d5.loss_mask: 1.6907  decode.d5.loss_dice: 1.6577  decode.d6.loss_cls: 0.6575  decode.d6.loss_mask: 1.6943  decode.d6.loss_dice: 1.7198  decode.d7.loss_cls: 0.6402  decode.d7.loss_mask: 1.6714  decode.d7.loss_dice: 1.6765  decode.d8.loss_cls: 0.7099  decode.d8.loss_mask: 1.6823  decode.d8.loss_dice: 1.6902  mix_decode.loss_cls: 0.2726  mix_decode.loss_mask: 0.6688  mix_decode.loss_dice: 0.8162  mix_decode.d0.loss_cls: 0.3326  mix_decode.d0.loss_mask: 0.6813  mix_decode.d0.loss_dice: 0.8508  mix_decode.d1.loss_cls: 0.3444  mix_decode.d1.loss_mask: 0.6724  mix_decode.d1.loss_dice: 0.7530  mix_decode.d2.loss_cls: 0.3526  mix_decode.d2.loss_mask: 0.6633  mix_decode.d2.loss_dice: 0.7856  mix_decode.d3.loss_cls: 0.3400  mix_decode.d3.loss_mask: 0.6684  mix_decode.d3.loss_dice: 0.7845  mix_decode.d4.loss_cls: 0.3660  mix_decode.d4.loss_mask: 0.6852  mix_decode.d4.loss_dice: 0.7944  mix_decode.d5.loss_cls: 0.3701  mix_decode.d5.loss_mask: 0.6785  mix_decode.d5.loss_dice: 0.8158  mix_decode.d6.loss_cls: 0.3992  mix_decode.d6.loss_mask: 0.6564  mix_decode.d6.loss_dice: 0.7714  mix_decode.d7.loss_cls: 0.3919  mix_decode.d7.loss_mask: 0.6741  mix_decode.d7.loss_dice: 0.8219  mix_decode.d8.loss_cls: 0.3689  mix_decode.d8.loss_mask: 0.6524  mix_decode.d8.loss_dice: 0.7816
2025/03/29 14:07:46 - mmengine - INFO - Iter(train) [ 8150/20000]  base_lr: 6.2437e-05 lr: 6.2437e-05  eta: 2:48:55  time: 1.1508  data_time: 0.0225  memory: 11211  loss: 49.4635  decode.loss_cls: 0.3784  decode.loss_mask: 1.5387  decode.loss_dice: 1.4382  decode.d0.loss_cls: 0.5688  decode.d0.loss_mask: 1.5383  decode.d0.loss_dice: 1.4659  decode.d1.loss_cls: 0.5736  decode.d1.loss_mask: 1.4932  decode.d1.loss_dice: 1.4308  decode.d2.loss_cls: 0.5227  decode.d2.loss_mask: 1.5155  decode.d2.loss_dice: 1.4084  decode.d3.loss_cls: 0.4784  decode.d3.loss_mask: 1.4917  decode.d3.loss_dice: 1.3673  decode.d4.loss_cls: 0.4902  decode.d4.loss_mask: 1.4990  decode.d4.loss_dice: 1.4146  decode.d5.loss_cls: 0.4345  decode.d5.loss_mask: 1.6337  decode.d5.loss_dice: 1.4610  decode.d6.loss_cls: 0.4784  decode.d6.loss_mask: 1.4927  decode.d6.loss_dice: 1.4106  decode.d7.loss_cls: 0.5444  decode.d7.loss_mask: 1.4956  decode.d7.loss_dice: 1.4018  decode.d8.loss_cls: 0.5029  decode.d8.loss_mask: 1.4886  decode.d8.loss_dice: 1.4192  mix_decode.loss_cls: 0.2220  mix_decode.loss_mask: 0.5852  mix_decode.loss_dice: 0.7194  mix_decode.d0.loss_cls: 0.2387  mix_decode.d0.loss_mask: 0.5617  mix_decode.d0.loss_dice: 0.7504  mix_decode.d1.loss_cls: 0.2325  mix_decode.d1.loss_mask: 0.5475  mix_decode.d1.loss_dice: 0.7014  mix_decode.d2.loss_cls: 0.2292  mix_decode.d2.loss_mask: 0.5556  mix_decode.d2.loss_dice: 0.7055  mix_decode.d3.loss_cls: 0.2498  mix_decode.d3.loss_mask: 0.5400  mix_decode.d3.loss_dice: 0.6727  mix_decode.d4.loss_cls: 0.2401  mix_decode.d4.loss_mask: 0.5633  mix_decode.d4.loss_dice: 0.7190  mix_decode.d5.loss_cls: 0.2061  mix_decode.d5.loss_mask: 0.5687  mix_decode.d5.loss_dice: 0.7057  mix_decode.d6.loss_cls: 0.2355  mix_decode.d6.loss_mask: 0.5457  mix_decode.d6.loss_dice: 0.7081  mix_decode.d7.loss_cls: 0.2683  mix_decode.d7.loss_mask: 0.5637  mix_decode.d7.loss_dice: 0.6967  mix_decode.d8.loss_cls: 0.2534  mix_decode.d8.loss_mask: 0.5976  mix_decode.d8.loss_dice: 0.7026
2025/03/29 14:08:43 - mmengine - INFO - Iter(train) [ 8200/20000]  base_lr: 6.2199e-05 lr: 6.2199e-05  eta: 2:48:34  time: 1.1523  data_time: 0.0230  memory: 11211  loss: 57.7878  decode.loss_cls: 0.2316  decode.loss_mask: 1.8736  decode.loss_dice: 1.8240  decode.d0.loss_cls: 0.3029  decode.d0.loss_mask: 1.9167  decode.d0.loss_dice: 1.8650  decode.d1.loss_cls: 0.1891  decode.d1.loss_mask: 1.9042  decode.d1.loss_dice: 1.8576  decode.d2.loss_cls: 0.2239  decode.d2.loss_mask: 1.8790  decode.d2.loss_dice: 1.8737  decode.d3.loss_cls: 0.2312  decode.d3.loss_mask: 1.9002  decode.d3.loss_dice: 1.8443  decode.d4.loss_cls: 0.2485  decode.d4.loss_mask: 1.9293  decode.d4.loss_dice: 1.8363  decode.d5.loss_cls: 0.2511  decode.d5.loss_mask: 1.9078  decode.d5.loss_dice: 1.8256  decode.d6.loss_cls: 0.2439  decode.d6.loss_mask: 1.9095  decode.d6.loss_dice: 1.8404  decode.d7.loss_cls: 0.2727  decode.d7.loss_mask: 1.8887  decode.d7.loss_dice: 1.8405  decode.d8.loss_cls: 0.2686  decode.d8.loss_mask: 1.9107  decode.d8.loss_dice: 1.8056  mix_decode.loss_cls: 0.2483  mix_decode.loss_mask: 0.6992  mix_decode.loss_dice: 0.8188  mix_decode.d0.loss_cls: 0.2735  mix_decode.d0.loss_mask: 0.6872  mix_decode.d0.loss_dice: 0.9104  mix_decode.d1.loss_cls: 0.2378  mix_decode.d1.loss_mask: 0.7261  mix_decode.d1.loss_dice: 0.8085  mix_decode.d2.loss_cls: 0.2517  mix_decode.d2.loss_mask: 0.7097  mix_decode.d2.loss_dice: 0.8352  mix_decode.d3.loss_cls: 0.2695  mix_decode.d3.loss_mask: 0.7072  mix_decode.d3.loss_dice: 0.8171  mix_decode.d4.loss_cls: 0.3016  mix_decode.d4.loss_mask: 0.6760  mix_decode.d4.loss_dice: 0.8127  mix_decode.d5.loss_cls: 0.3071  mix_decode.d5.loss_mask: 0.6860  mix_decode.d5.loss_dice: 0.8232  mix_decode.d6.loss_cls: 0.2822  mix_decode.d6.loss_mask: 0.6970  mix_decode.d6.loss_dice: 0.8159  mix_decode.d7.loss_cls: 0.2992  mix_decode.d7.loss_mask: 0.6518  mix_decode.d7.loss_dice: 0.7859  mix_decode.d8.loss_cls: 0.2413  mix_decode.d8.loss_mask: 0.7040  mix_decode.d8.loss_dice: 0.8073
2025/03/29 14:09:41 - mmengine - INFO - Iter(train) [ 8250/20000]  base_lr: 6.1962e-05 lr: 6.1962e-05  eta: 2:48:12  time: 1.1557  data_time: 0.0227  memory: 11212  loss: 53.5546  decode.loss_cls: 0.3400  decode.loss_mask: 1.7045  decode.loss_dice: 1.5795  decode.d0.loss_cls: 0.4044  decode.d0.loss_mask: 1.7436  decode.d0.loss_dice: 1.7390  decode.d1.loss_cls: 0.3624  decode.d1.loss_mask: 1.6877  decode.d1.loss_dice: 1.5452  decode.d2.loss_cls: 0.3250  decode.d2.loss_mask: 1.7697  decode.d2.loss_dice: 1.5584  decode.d3.loss_cls: 0.3328  decode.d3.loss_mask: 1.7201  decode.d3.loss_dice: 1.5530  decode.d4.loss_cls: 0.3311  decode.d4.loss_mask: 1.7368  decode.d4.loss_dice: 1.6277  decode.d5.loss_cls: 0.3609  decode.d5.loss_mask: 1.7272  decode.d5.loss_dice: 1.6094  decode.d6.loss_cls: 0.3230  decode.d6.loss_mask: 1.7484  decode.d6.loss_dice: 1.6276  decode.d7.loss_cls: 0.3368  decode.d7.loss_mask: 1.7283  decode.d7.loss_dice: 1.6242  decode.d8.loss_cls: 0.3314  decode.d8.loss_mask: 1.7056  decode.d8.loss_dice: 1.6185  mix_decode.loss_cls: 0.2774  mix_decode.loss_mask: 0.6698  mix_decode.loss_dice: 0.7187  mix_decode.d0.loss_cls: 0.2814  mix_decode.d0.loss_mask: 0.6676  mix_decode.d0.loss_dice: 0.7860  mix_decode.d1.loss_cls: 0.2203  mix_decode.d1.loss_mask: 0.6821  mix_decode.d1.loss_dice: 0.7311  mix_decode.d2.loss_cls: 0.1980  mix_decode.d2.loss_mask: 0.7139  mix_decode.d2.loss_dice: 0.7377  mix_decode.d3.loss_cls: 0.2295  mix_decode.d3.loss_mask: 0.6649  mix_decode.d3.loss_dice: 0.7162  mix_decode.d4.loss_cls: 0.2421  mix_decode.d4.loss_mask: 0.6841  mix_decode.d4.loss_dice: 0.7462  mix_decode.d5.loss_cls: 0.2869  mix_decode.d5.loss_mask: 0.6819  mix_decode.d5.loss_dice: 0.7395  mix_decode.d6.loss_cls: 0.2572  mix_decode.d6.loss_mask: 0.6860  mix_decode.d6.loss_dice: 0.7453  mix_decode.d7.loss_cls: 0.2905  mix_decode.d7.loss_mask: 0.6921  mix_decode.d7.loss_dice: 0.7489  mix_decode.d8.loss_cls: 0.2481  mix_decode.d8.loss_mask: 0.6793  mix_decode.d8.loss_dice: 0.7299
2025/03/29 14:10:39 - mmengine - INFO - Iter(train) [ 8300/20000]  base_lr: 6.1725e-05 lr: 6.1725e-05  eta: 2:47:50  time: 1.1509  data_time: 0.0224  memory: 11210  loss: 61.0153  decode.loss_cls: 0.2476  decode.loss_mask: 2.0496  decode.loss_dice: 1.8172  decode.d0.loss_cls: 0.5325  decode.d0.loss_mask: 2.0575  decode.d0.loss_dice: 1.7735  decode.d1.loss_cls: 0.3001  decode.d1.loss_mask: 2.0106  decode.d1.loss_dice: 1.7591  decode.d2.loss_cls: 0.2912  decode.d2.loss_mask: 1.9899  decode.d2.loss_dice: 1.7434  decode.d3.loss_cls: 0.2905  decode.d3.loss_mask: 2.0141  decode.d3.loss_dice: 1.7574  decode.d4.loss_cls: 0.3128  decode.d4.loss_mask: 2.0257  decode.d4.loss_dice: 1.7569  decode.d5.loss_cls: 0.2846  decode.d5.loss_mask: 2.0386  decode.d5.loss_dice: 1.7837  decode.d6.loss_cls: 0.2489  decode.d6.loss_mask: 2.0637  decode.d6.loss_dice: 1.7916  decode.d7.loss_cls: 0.2419  decode.d7.loss_mask: 2.0445  decode.d7.loss_dice: 1.7784  decode.d8.loss_cls: 0.2947  decode.d8.loss_mask: 2.0262  decode.d8.loss_dice: 1.8102  mix_decode.loss_cls: 0.3074  mix_decode.loss_mask: 0.7485  mix_decode.loss_dice: 0.9264  mix_decode.d0.loss_cls: 0.3942  mix_decode.d0.loss_mask: 0.7289  mix_decode.d0.loss_dice: 0.9158  mix_decode.d1.loss_cls: 0.3437  mix_decode.d1.loss_mask: 0.7413  mix_decode.d1.loss_dice: 0.8668  mix_decode.d2.loss_cls: 0.3136  mix_decode.d2.loss_mask: 0.7331  mix_decode.d2.loss_dice: 0.8752  mix_decode.d3.loss_cls: 0.3620  mix_decode.d3.loss_mask: 0.7548  mix_decode.d3.loss_dice: 0.8873  mix_decode.d4.loss_cls: 0.3771  mix_decode.d4.loss_mask: 0.7553  mix_decode.d4.loss_dice: 0.8888  mix_decode.d5.loss_cls: 0.3275  mix_decode.d5.loss_mask: 0.7609  mix_decode.d5.loss_dice: 0.8830  mix_decode.d6.loss_cls: 0.3610  mix_decode.d6.loss_mask: 0.7340  mix_decode.d6.loss_dice: 0.8794  mix_decode.d7.loss_cls: 0.3314  mix_decode.d7.loss_mask: 0.7696  mix_decode.d7.loss_dice: 0.8992  mix_decode.d8.loss_cls: 0.3547  mix_decode.d8.loss_mask: 0.7640  mix_decode.d8.loss_dice: 0.8941
2025/03/29 14:11:36 - mmengine - INFO - Iter(train) [ 8350/20000]  base_lr: 6.1487e-05 lr: 6.1487e-05  eta: 2:47:27  time: 1.1592  data_time: 0.0233  memory: 11215  loss: 57.6484  decode.loss_cls: 0.3910  decode.loss_mask: 1.8289  decode.loss_dice: 1.7526  decode.d0.loss_cls: 0.4204  decode.d0.loss_mask: 1.8401  decode.d0.loss_dice: 1.7534  decode.d1.loss_cls: 0.4419  decode.d1.loss_mask: 1.8128  decode.d1.loss_dice: 1.7224  decode.d2.loss_cls: 0.3555  decode.d2.loss_mask: 1.8643  decode.d2.loss_dice: 1.7283  decode.d3.loss_cls: 0.3088  decode.d3.loss_mask: 1.8449  decode.d3.loss_dice: 1.7396  decode.d4.loss_cls: 0.3819  decode.d4.loss_mask: 1.8299  decode.d4.loss_dice: 1.7571  decode.d5.loss_cls: 0.3364  decode.d5.loss_mask: 1.8425  decode.d5.loss_dice: 1.7494  decode.d6.loss_cls: 0.3832  decode.d6.loss_mask: 1.8460  decode.d6.loss_dice: 1.7564  decode.d7.loss_cls: 0.3667  decode.d7.loss_mask: 1.8415  decode.d7.loss_dice: 1.7713  decode.d8.loss_cls: 0.3958  decode.d8.loss_mask: 1.8163  decode.d8.loss_dice: 1.7783  mix_decode.loss_cls: 0.3307  mix_decode.loss_mask: 0.6839  mix_decode.loss_dice: 0.7620  mix_decode.d0.loss_cls: 0.3611  mix_decode.d0.loss_mask: 0.6722  mix_decode.d0.loss_dice: 0.7824  mix_decode.d1.loss_cls: 0.3323  mix_decode.d1.loss_mask: 0.6762  mix_decode.d1.loss_dice: 0.7501  mix_decode.d2.loss_cls: 0.3583  mix_decode.d2.loss_mask: 0.6924  mix_decode.d2.loss_dice: 0.7425  mix_decode.d3.loss_cls: 0.3267  mix_decode.d3.loss_mask: 0.7133  mix_decode.d3.loss_dice: 0.7944  mix_decode.d4.loss_cls: 0.3500  mix_decode.d4.loss_mask: 0.7019  mix_decode.d4.loss_dice: 0.7710  mix_decode.d5.loss_cls: 0.3196  mix_decode.d5.loss_mask: 0.6863  mix_decode.d5.loss_dice: 0.7831  mix_decode.d6.loss_cls: 0.3116  mix_decode.d6.loss_mask: 0.6862  mix_decode.d6.loss_dice: 0.7876  mix_decode.d7.loss_cls: 0.3432  mix_decode.d7.loss_mask: 0.6692  mix_decode.d7.loss_dice: 0.7814  mix_decode.d8.loss_cls: 0.3473  mix_decode.d8.loss_mask: 0.6938  mix_decode.d8.loss_dice: 0.7802
2025/03/29 14:12:34 - mmengine - INFO - Iter(train) [ 8400/20000]  base_lr: 6.1250e-05 lr: 6.1250e-05  eta: 2:47:04  time: 1.1516  data_time: 0.0224  memory: 11219  loss: 61.3453  decode.loss_cls: 0.2085  decode.loss_mask: 2.2750  decode.loss_dice: 1.8447  decode.d0.loss_cls: 0.3869  decode.d0.loss_mask: 2.2884  decode.d0.loss_dice: 1.8584  decode.d1.loss_cls: 0.2829  decode.d1.loss_mask: 2.2548  decode.d1.loss_dice: 1.8488  decode.d2.loss_cls: 0.2015  decode.d2.loss_mask: 2.2848  decode.d2.loss_dice: 1.8573  decode.d3.loss_cls: 0.2094  decode.d3.loss_mask: 2.2655  decode.d3.loss_dice: 1.8836  decode.d4.loss_cls: 0.1887  decode.d4.loss_mask: 2.3050  decode.d4.loss_dice: 1.8588  decode.d5.loss_cls: 0.1744  decode.d5.loss_mask: 2.3005  decode.d5.loss_dice: 1.8710  decode.d6.loss_cls: 0.2045  decode.d6.loss_mask: 2.2694  decode.d6.loss_dice: 1.8469  decode.d7.loss_cls: 0.2468  decode.d7.loss_mask: 2.3112  decode.d7.loss_dice: 1.8833  decode.d8.loss_cls: 0.2245  decode.d8.loss_mask: 2.2934  decode.d8.loss_dice: 1.8453  mix_decode.loss_cls: 0.2597  mix_decode.loss_mask: 0.6882  mix_decode.loss_dice: 0.7188  mix_decode.d0.loss_cls: 0.2545  mix_decode.d0.loss_mask: 0.7035  mix_decode.d0.loss_dice: 0.8518  mix_decode.d1.loss_cls: 0.2718  mix_decode.d1.loss_mask: 0.6999  mix_decode.d1.loss_dice: 0.7592  mix_decode.d2.loss_cls: 0.2623  mix_decode.d2.loss_mask: 0.6991  mix_decode.d2.loss_dice: 0.7406  mix_decode.d3.loss_cls: 0.2732  mix_decode.d3.loss_mask: 0.7223  mix_decode.d3.loss_dice: 0.7696  mix_decode.d4.loss_cls: 0.2985  mix_decode.d4.loss_mask: 0.6936  mix_decode.d4.loss_dice: 0.7278  mix_decode.d5.loss_cls: 0.2873  mix_decode.d5.loss_mask: 0.7057  mix_decode.d5.loss_dice: 0.7514  mix_decode.d6.loss_cls: 0.2938  mix_decode.d6.loss_mask: 0.7570  mix_decode.d6.loss_dice: 0.7975  mix_decode.d7.loss_cls: 0.3068  mix_decode.d7.loss_mask: 0.6885  mix_decode.d7.loss_dice: 0.8151  mix_decode.d8.loss_cls: 0.2848  mix_decode.d8.loss_mask: 0.7146  mix_decode.d8.loss_dice: 0.7743
2025/03/29 14:13:32 - mmengine - INFO - Iter(train) [ 8450/20000]  base_lr: 6.1012e-05 lr: 6.1012e-05  eta: 2:46:41  time: 1.1454  data_time: 0.0226  memory: 11221  loss: 50.0855  decode.loss_cls: 0.4603  decode.loss_mask: 1.5176  decode.loss_dice: 1.6005  decode.d0.loss_cls: 0.5296  decode.d0.loss_mask: 1.5539  decode.d0.loss_dice: 1.6975  decode.d1.loss_cls: 0.4876  decode.d1.loss_mask: 1.4854  decode.d1.loss_dice: 1.5847  decode.d2.loss_cls: 0.4645  decode.d2.loss_mask: 1.4782  decode.d2.loss_dice: 1.5921  decode.d3.loss_cls: 0.5046  decode.d3.loss_mask: 1.4912  decode.d3.loss_dice: 1.5831  decode.d4.loss_cls: 0.5230  decode.d4.loss_mask: 1.4806  decode.d4.loss_dice: 1.5747  decode.d5.loss_cls: 0.4943  decode.d5.loss_mask: 1.4969  decode.d5.loss_dice: 1.6273  decode.d6.loss_cls: 0.4907  decode.d6.loss_mask: 1.5105  decode.d6.loss_dice: 1.5755  decode.d7.loss_cls: 0.5043  decode.d7.loss_mask: 1.4871  decode.d7.loss_dice: 1.5502  decode.d8.loss_cls: 0.4770  decode.d8.loss_mask: 1.5069  decode.d8.loss_dice: 1.5706  mix_decode.loss_cls: 0.1982  mix_decode.loss_mask: 0.5843  mix_decode.loss_dice: 0.6668  mix_decode.d0.loss_cls: 0.2311  mix_decode.d0.loss_mask: 0.5730  mix_decode.d0.loss_dice: 0.6551  mix_decode.d1.loss_cls: 0.1868  mix_decode.d1.loss_mask: 0.5765  mix_decode.d1.loss_dice: 0.6404  mix_decode.d2.loss_cls: 0.2000  mix_decode.d2.loss_mask: 0.5670  mix_decode.d2.loss_dice: 0.6263  mix_decode.d3.loss_cls: 0.1992  mix_decode.d3.loss_mask: 0.5730  mix_decode.d3.loss_dice: 0.6358  mix_decode.d4.loss_cls: 0.2156  mix_decode.d4.loss_mask: 0.5649  mix_decode.d4.loss_dice: 0.6416  mix_decode.d5.loss_cls: 0.1959  mix_decode.d5.loss_mask: 0.5739  mix_decode.d5.loss_dice: 0.6494  mix_decode.d6.loss_cls: 0.2004  mix_decode.d6.loss_mask: 0.5739  mix_decode.d6.loss_dice: 0.6513  mix_decode.d7.loss_cls: 0.1860  mix_decode.d7.loss_mask: 0.5752  mix_decode.d7.loss_dice: 0.6410  mix_decode.d8.loss_cls: 0.2054  mix_decode.d8.loss_mask: 0.5670  mix_decode.d8.loss_dice: 0.6300
2025/03/29 14:14:29 - mmengine - INFO - Iter(train) [ 8500/20000]  base_lr: 6.0774e-05 lr: 6.0774e-05  eta: 2:46:17  time: 1.1509  data_time: 0.0221  memory: 11209  loss: 58.9545  decode.loss_cls: 0.3926  decode.loss_mask: 1.8626  decode.loss_dice: 1.8350  decode.d0.loss_cls: 0.5400  decode.d0.loss_mask: 1.8736  decode.d0.loss_dice: 1.8798  decode.d1.loss_cls: 0.5278  decode.d1.loss_mask: 1.7753  decode.d1.loss_dice: 1.7989  decode.d2.loss_cls: 0.4652  decode.d2.loss_mask: 1.8259  decode.d2.loss_dice: 1.8484  decode.d3.loss_cls: 0.4028  decode.d3.loss_mask: 1.7881  decode.d3.loss_dice: 1.8033  decode.d4.loss_cls: 0.5065  decode.d4.loss_mask: 1.8216  decode.d4.loss_dice: 1.8311  decode.d5.loss_cls: 0.5279  decode.d5.loss_mask: 1.8138  decode.d5.loss_dice: 1.8605  decode.d6.loss_cls: 0.5646  decode.d6.loss_mask: 1.7361  decode.d6.loss_dice: 1.7876  decode.d7.loss_cls: 0.5767  decode.d7.loss_mask: 1.7778  decode.d7.loss_dice: 1.8200  decode.d8.loss_cls: 0.4959  decode.d8.loss_mask: 1.8366  decode.d8.loss_dice: 1.8255  mix_decode.loss_cls: 0.2776  mix_decode.loss_mask: 0.5788  mix_decode.loss_dice: 0.8645  mix_decode.d0.loss_cls: 0.3023  mix_decode.d0.loss_mask: 0.5849  mix_decode.d0.loss_dice: 0.8978  mix_decode.d1.loss_cls: 0.3076  mix_decode.d1.loss_mask: 0.6016  mix_decode.d1.loss_dice: 0.8284  mix_decode.d2.loss_cls: 0.3182  mix_decode.d2.loss_mask: 0.5714  mix_decode.d2.loss_dice: 0.8361  mix_decode.d3.loss_cls: 0.3102  mix_decode.d3.loss_mask: 0.5716  mix_decode.d3.loss_dice: 0.8587  mix_decode.d4.loss_cls: 0.3264  mix_decode.d4.loss_mask: 0.5612  mix_decode.d4.loss_dice: 0.8446  mix_decode.d5.loss_cls: 0.3064  mix_decode.d5.loss_mask: 0.5785  mix_decode.d5.loss_dice: 0.8932  mix_decode.d6.loss_cls: 0.3069  mix_decode.d6.loss_mask: 0.5910  mix_decode.d6.loss_dice: 0.8938  mix_decode.d7.loss_cls: 0.3506  mix_decode.d7.loss_mask: 0.5795  mix_decode.d7.loss_dice: 0.8585  mix_decode.d8.loss_cls: 0.3277  mix_decode.d8.loss_mask: 0.5773  mix_decode.d8.loss_dice: 0.8481
2025/03/29 14:15:27 - mmengine - INFO - Iter(train) [ 8550/20000]  base_lr: 6.0537e-05 lr: 6.0537e-05  eta: 2:45:52  time: 1.1518  data_time: 0.0218  memory: 11212  loss: 59.0570  decode.loss_cls: 0.7925  decode.loss_mask: 1.7737  decode.loss_dice: 1.7180  decode.d0.loss_cls: 0.6887  decode.d0.loss_mask: 1.7754  decode.d0.loss_dice: 1.7973  decode.d1.loss_cls: 0.5708  decode.d1.loss_mask: 1.7296  decode.d1.loss_dice: 1.7003  decode.d2.loss_cls: 0.6143  decode.d2.loss_mask: 1.7553  decode.d2.loss_dice: 1.7291  decode.d3.loss_cls: 0.6658  decode.d3.loss_mask: 1.7554  decode.d3.loss_dice: 1.7343  decode.d4.loss_cls: 0.6560  decode.d4.loss_mask: 1.7817  decode.d4.loss_dice: 1.6826  decode.d5.loss_cls: 0.6292  decode.d5.loss_mask: 1.7328  decode.d5.loss_dice: 1.7274  decode.d6.loss_cls: 0.7400  decode.d6.loss_mask: 1.7851  decode.d6.loss_dice: 1.7976  decode.d7.loss_cls: 0.6372  decode.d7.loss_mask: 1.7774  decode.d7.loss_dice: 1.7229  decode.d8.loss_cls: 0.6956  decode.d8.loss_mask: 1.8030  decode.d8.loss_dice: 1.7840  mix_decode.loss_cls: 0.3327  mix_decode.loss_mask: 0.5823  mix_decode.loss_dice: 0.8379  mix_decode.d0.loss_cls: 0.2763  mix_decode.d0.loss_mask: 0.5782  mix_decode.d0.loss_dice: 0.8586  mix_decode.d1.loss_cls: 0.2983  mix_decode.d1.loss_mask: 0.5998  mix_decode.d1.loss_dice: 0.7904  mix_decode.d2.loss_cls: 0.3125  mix_decode.d2.loss_mask: 0.5943  mix_decode.d2.loss_dice: 0.7915  mix_decode.d3.loss_cls: 0.3191  mix_decode.d3.loss_mask: 0.5926  mix_decode.d3.loss_dice: 0.8147  mix_decode.d4.loss_cls: 0.3510  mix_decode.d4.loss_mask: 0.5701  mix_decode.d4.loss_dice: 0.8011  mix_decode.d5.loss_cls: 0.3750  mix_decode.d5.loss_mask: 0.5764  mix_decode.d5.loss_dice: 0.8084  mix_decode.d6.loss_cls: 0.3544  mix_decode.d6.loss_mask: 0.5680  mix_decode.d6.loss_dice: 0.8072  mix_decode.d7.loss_cls: 0.3665  mix_decode.d7.loss_mask: 0.5730  mix_decode.d7.loss_dice: 0.8041  mix_decode.d8.loss_cls: 0.3411  mix_decode.d8.loss_mask: 0.5947  mix_decode.d8.loss_dice: 0.8340
2025/03/29 14:16:25 - mmengine - INFO - Iter(train) [ 8600/20000]  base_lr: 6.0299e-05 lr: 6.0299e-05  eta: 2:45:28  time: 1.1495  data_time: 0.0222  memory: 11212  loss: 59.5783  decode.loss_cls: 0.2123  decode.loss_mask: 2.1684  decode.loss_dice: 1.9782  decode.d0.loss_cls: 0.4371  decode.d0.loss_mask: 2.1166  decode.d0.loss_dice: 1.9333  decode.d1.loss_cls: 0.2471  decode.d1.loss_mask: 2.1216  decode.d1.loss_dice: 1.9476  decode.d2.loss_cls: 0.2143  decode.d2.loss_mask: 2.1383  decode.d2.loss_dice: 1.9589  decode.d3.loss_cls: 0.2208  decode.d3.loss_mask: 2.1114  decode.d3.loss_dice: 1.9816  decode.d4.loss_cls: 0.2084  decode.d4.loss_mask: 2.1172  decode.d4.loss_dice: 1.9547  decode.d5.loss_cls: 0.2415  decode.d5.loss_mask: 2.1389  decode.d5.loss_dice: 1.9561  decode.d6.loss_cls: 0.2346  decode.d6.loss_mask: 2.1377  decode.d6.loss_dice: 1.9460  decode.d7.loss_cls: 0.2876  decode.d7.loss_mask: 2.1322  decode.d7.loss_dice: 1.9318  decode.d8.loss_cls: 0.2272  decode.d8.loss_mask: 2.1602  decode.d8.loss_dice: 1.9657  mix_decode.loss_cls: 0.2121  mix_decode.loss_mask: 0.6839  mix_decode.loss_dice: 0.7387  mix_decode.d0.loss_cls: 0.2358  mix_decode.d0.loss_mask: 0.6737  mix_decode.d0.loss_dice: 0.7437  mix_decode.d1.loss_cls: 0.2392  mix_decode.d1.loss_mask: 0.6611  mix_decode.d1.loss_dice: 0.7073  mix_decode.d2.loss_cls: 0.2353  mix_decode.d2.loss_mask: 0.6455  mix_decode.d2.loss_dice: 0.6976  mix_decode.d3.loss_cls: 0.2348  mix_decode.d3.loss_mask: 0.6534  mix_decode.d3.loss_dice: 0.7369  mix_decode.d4.loss_cls: 0.2139  mix_decode.d4.loss_mask: 0.6815  mix_decode.d4.loss_dice: 0.7220  mix_decode.d5.loss_cls: 0.1946  mix_decode.d5.loss_mask: 0.6873  mix_decode.d5.loss_dice: 0.7286  mix_decode.d6.loss_cls: 0.2326  mix_decode.d6.loss_mask: 0.6732  mix_decode.d6.loss_dice: 0.7108  mix_decode.d7.loss_cls: 0.2231  mix_decode.d7.loss_mask: 0.6675  mix_decode.d7.loss_dice: 0.7157  mix_decode.d8.loss_cls: 0.1839  mix_decode.d8.loss_mask: 0.6849  mix_decode.d8.loss_dice: 0.7323
2025/03/29 14:17:22 - mmengine - INFO - Iter(train) [ 8650/20000]  base_lr: 6.0060e-05 lr: 6.0060e-05  eta: 2:45:03  time: 1.1507  data_time: 0.0217  memory: 11208  loss: 56.5104  decode.loss_cls: 0.2886  decode.loss_mask: 1.9398  decode.loss_dice: 1.6312  decode.d0.loss_cls: 0.5277  decode.d0.loss_mask: 1.8790  decode.d0.loss_dice: 1.6078  decode.d1.loss_cls: 0.3155  decode.d1.loss_mask: 1.9280  decode.d1.loss_dice: 1.5776  decode.d2.loss_cls: 0.2852  decode.d2.loss_mask: 1.9599  decode.d2.loss_dice: 1.6247  decode.d3.loss_cls: 0.2726  decode.d3.loss_mask: 1.8914  decode.d3.loss_dice: 1.6501  decode.d4.loss_cls: 0.3363  decode.d4.loss_mask: 1.8824  decode.d4.loss_dice: 1.5786  decode.d5.loss_cls: 0.3226  decode.d5.loss_mask: 1.9460  decode.d5.loss_dice: 1.5921  decode.d6.loss_cls: 0.3115  decode.d6.loss_mask: 1.8894  decode.d6.loss_dice: 1.6325  decode.d7.loss_cls: 0.3162  decode.d7.loss_mask: 1.9237  decode.d7.loss_dice: 1.6062  decode.d8.loss_cls: 0.3012  decode.d8.loss_mask: 1.9355  decode.d8.loss_dice: 1.6465  mix_decode.loss_cls: 0.2850  mix_decode.loss_mask: 0.7329  mix_decode.loss_dice: 0.7352  mix_decode.d0.loss_cls: 0.3176  mix_decode.d0.loss_mask: 0.7488  mix_decode.d0.loss_dice: 0.7826  mix_decode.d1.loss_cls: 0.2714  mix_decode.d1.loss_mask: 0.7427  mix_decode.d1.loss_dice: 0.7595  mix_decode.d2.loss_cls: 0.3311  mix_decode.d2.loss_mask: 0.7583  mix_decode.d2.loss_dice: 0.7309  mix_decode.d3.loss_cls: 0.3162  mix_decode.d3.loss_mask: 0.7556  mix_decode.d3.loss_dice: 0.7129  mix_decode.d4.loss_cls: 0.3103  mix_decode.d4.loss_mask: 0.7642  mix_decode.d4.loss_dice: 0.7092  mix_decode.d5.loss_cls: 0.3281  mix_decode.d5.loss_mask: 0.7380  mix_decode.d5.loss_dice: 0.7337  mix_decode.d6.loss_cls: 0.3061  mix_decode.d6.loss_mask: 0.7260  mix_decode.d6.loss_dice: 0.7381  mix_decode.d7.loss_cls: 0.3197  mix_decode.d7.loss_mask: 0.7362  mix_decode.d7.loss_dice: 0.7292  mix_decode.d8.loss_cls: 0.3135  mix_decode.d8.loss_mask: 0.7516  mix_decode.d8.loss_dice: 0.7262
2025/03/29 14:18:20 - mmengine - INFO - Iter(train) [ 8700/20000]  base_lr: 5.9822e-05 lr: 5.9822e-05  eta: 2:44:37  time: 1.1560  data_time: 0.0223  memory: 11215  loss: 61.9718  decode.loss_cls: 0.4126  decode.loss_mask: 2.1328  decode.loss_dice: 1.8180  decode.d0.loss_cls: 0.4890  decode.d0.loss_mask: 2.2586  decode.d0.loss_dice: 1.8923  decode.d1.loss_cls: 0.4801  decode.d1.loss_mask: 2.1045  decode.d1.loss_dice: 1.7637  decode.d2.loss_cls: 0.4368  decode.d2.loss_mask: 2.1554  decode.d2.loss_dice: 1.8076  decode.d3.loss_cls: 0.4922  decode.d3.loss_mask: 2.0688  decode.d3.loss_dice: 1.7782  decode.d4.loss_cls: 0.4608  decode.d4.loss_mask: 2.1758  decode.d4.loss_dice: 1.8552  decode.d5.loss_cls: 0.5056  decode.d5.loss_mask: 2.1096  decode.d5.loss_dice: 1.8160  decode.d6.loss_cls: 0.3840  decode.d6.loss_mask: 2.1621  decode.d6.loss_dice: 1.8494  decode.d7.loss_cls: 0.4150  decode.d7.loss_mask: 2.1312  decode.d7.loss_dice: 1.8225  decode.d8.loss_cls: 0.3974  decode.d8.loss_mask: 2.1592  decode.d8.loss_dice: 1.8474  mix_decode.loss_cls: 0.2993  mix_decode.loss_mask: 0.6293  mix_decode.loss_dice: 0.7881  mix_decode.d0.loss_cls: 0.3130  mix_decode.d0.loss_mask: 0.6507  mix_decode.d0.loss_dice: 0.8319  mix_decode.d1.loss_cls: 0.3301  mix_decode.d1.loss_mask: 0.6405  mix_decode.d1.loss_dice: 0.8139  mix_decode.d2.loss_cls: 0.3272  mix_decode.d2.loss_mask: 0.6479  mix_decode.d2.loss_dice: 0.7991  mix_decode.d3.loss_cls: 0.2673  mix_decode.d3.loss_mask: 0.6791  mix_decode.d3.loss_dice: 0.8249  mix_decode.d4.loss_cls: 0.4249  mix_decode.d4.loss_mask: 0.6380  mix_decode.d4.loss_dice: 0.7928  mix_decode.d5.loss_cls: 0.3524  mix_decode.d5.loss_mask: 0.6387  mix_decode.d5.loss_dice: 0.7893  mix_decode.d6.loss_cls: 0.2540  mix_decode.d6.loss_mask: 0.6466  mix_decode.d6.loss_dice: 0.8384  mix_decode.d7.loss_cls: 0.3204  mix_decode.d7.loss_mask: 0.6716  mix_decode.d7.loss_dice: 0.8300  mix_decode.d8.loss_cls: 0.3029  mix_decode.d8.loss_mask: 0.6559  mix_decode.d8.loss_dice: 0.7916
2025/03/29 14:19:18 - mmengine - INFO - Iter(train) [ 8750/20000]  base_lr: 5.9584e-05 lr: 5.9584e-05  eta: 2:44:12  time: 1.1572  data_time: 0.0230  memory: 11213  loss: 61.0021  decode.loss_cls: 0.5123  decode.loss_mask: 1.9022  decode.loss_dice: 1.8230  decode.d0.loss_cls: 0.6109  decode.d0.loss_mask: 1.8852  decode.d0.loss_dice: 1.8474  decode.d1.loss_cls: 0.5376  decode.d1.loss_mask: 1.8438  decode.d1.loss_dice: 1.8264  decode.d2.loss_cls: 0.4618  decode.d2.loss_mask: 1.8613  decode.d2.loss_dice: 1.7744  decode.d3.loss_cls: 0.4297  decode.d3.loss_mask: 1.8996  decode.d3.loss_dice: 1.8406  decode.d4.loss_cls: 0.5473  decode.d4.loss_mask: 1.8983  decode.d4.loss_dice: 1.8411  decode.d5.loss_cls: 0.4743  decode.d5.loss_mask: 1.9164  decode.d5.loss_dice: 1.8066  decode.d6.loss_cls: 0.6149  decode.d6.loss_mask: 1.8569  decode.d6.loss_dice: 1.7751  decode.d7.loss_cls: 0.4859  decode.d7.loss_mask: 1.8718  decode.d7.loss_dice: 1.7997  decode.d8.loss_cls: 0.5578  decode.d8.loss_mask: 1.8891  decode.d8.loss_dice: 1.8025  mix_decode.loss_cls: 0.3912  mix_decode.loss_mask: 0.6878  mix_decode.loss_dice: 0.7686  mix_decode.d0.loss_cls: 0.3864  mix_decode.d0.loss_mask: 0.7178  mix_decode.d0.loss_dice: 0.8298  mix_decode.d1.loss_cls: 0.3783  mix_decode.d1.loss_mask: 0.7194  mix_decode.d1.loss_dice: 0.8347  mix_decode.d2.loss_cls: 0.3455  mix_decode.d2.loss_mask: 0.7134  mix_decode.d2.loss_dice: 0.7815  mix_decode.d3.loss_cls: 0.3625  mix_decode.d3.loss_mask: 0.7086  mix_decode.d3.loss_dice: 0.7955  mix_decode.d4.loss_cls: 0.4093  mix_decode.d4.loss_mask: 0.7278  mix_decode.d4.loss_dice: 0.7816  mix_decode.d5.loss_cls: 0.3521  mix_decode.d5.loss_mask: 0.7253  mix_decode.d5.loss_dice: 0.7876  mix_decode.d6.loss_cls: 0.3569  mix_decode.d6.loss_mask: 0.7044  mix_decode.d6.loss_dice: 0.8025  mix_decode.d7.loss_cls: 0.3319  mix_decode.d7.loss_mask: 0.7219  mix_decode.d7.loss_dice: 0.7872  mix_decode.d8.loss_cls: 0.4241  mix_decode.d8.loss_mask: 0.6997  mix_decode.d8.loss_dice: 0.7748
2025/03/29 14:20:15 - mmengine - INFO - Iter(train) [ 8800/20000]  base_lr: 5.9346e-05 lr: 5.9346e-05  eta: 2:43:45  time: 1.1574  data_time: 0.0231  memory: 11200  loss: 59.8075  decode.loss_cls: 0.3535  decode.loss_mask: 1.9621  decode.loss_dice: 1.8572  decode.d0.loss_cls: 0.3653  decode.d0.loss_mask: 1.9903  decode.d0.loss_dice: 1.9259  decode.d1.loss_cls: 0.3569  decode.d1.loss_mask: 1.9998  decode.d1.loss_dice: 1.8769  decode.d2.loss_cls: 0.3290  decode.d2.loss_mask: 1.9850  decode.d2.loss_dice: 1.9009  decode.d3.loss_cls: 0.3213  decode.d3.loss_mask: 1.9692  decode.d3.loss_dice: 1.8914  decode.d4.loss_cls: 0.3334  decode.d4.loss_mask: 1.9518  decode.d4.loss_dice: 1.8845  decode.d5.loss_cls: 0.3360  decode.d5.loss_mask: 1.9734  decode.d5.loss_dice: 1.9018  decode.d6.loss_cls: 0.2822  decode.d6.loss_mask: 2.0402  decode.d6.loss_dice: 1.9288  decode.d7.loss_cls: 0.2706  decode.d7.loss_mask: 2.0578  decode.d7.loss_dice: 1.9341  decode.d8.loss_cls: 0.3173  decode.d8.loss_mask: 2.0447  decode.d8.loss_dice: 1.9057  mix_decode.loss_cls: 0.1874  mix_decode.loss_mask: 0.7446  mix_decode.loss_dice: 0.7594  mix_decode.d0.loss_cls: 0.2225  mix_decode.d0.loss_mask: 0.7757  mix_decode.d0.loss_dice: 0.8373  mix_decode.d1.loss_cls: 0.1925  mix_decode.d1.loss_mask: 0.7544  mix_decode.d1.loss_dice: 0.7926  mix_decode.d2.loss_cls: 0.1938  mix_decode.d2.loss_mask: 0.7617  mix_decode.d2.loss_dice: 0.7981  mix_decode.d3.loss_cls: 0.2292  mix_decode.d3.loss_mask: 0.7813  mix_decode.d3.loss_dice: 0.7887  mix_decode.d4.loss_cls: 0.2323  mix_decode.d4.loss_mask: 0.7636  mix_decode.d4.loss_dice: 0.7435  mix_decode.d5.loss_cls: 0.2481  mix_decode.d5.loss_mask: 0.7610  mix_decode.d5.loss_dice: 0.7910  mix_decode.d6.loss_cls: 0.2444  mix_decode.d6.loss_mask: 0.7338  mix_decode.d6.loss_dice: 0.7743  mix_decode.d7.loss_cls: 0.2343  mix_decode.d7.loss_mask: 0.7333  mix_decode.d7.loss_dice: 0.7392  mix_decode.d8.loss_cls: 0.2197  mix_decode.d8.loss_mask: 0.7479  mix_decode.d8.loss_dice: 0.7754
2025/03/29 14:21:12 - mmengine - INFO - Iter(train) [ 8850/20000]  base_lr: 5.9107e-05 lr: 5.9107e-05  eta: 2:43:18  time: 1.1475  data_time: 0.0224  memory: 11217  loss: 62.7722  decode.loss_cls: 0.3977  decode.loss_mask: 2.0962  decode.loss_dice: 1.8988  decode.d0.loss_cls: 0.3904  decode.d0.loss_mask: 2.1350  decode.d0.loss_dice: 1.9867  decode.d1.loss_cls: 0.4101  decode.d1.loss_mask: 2.1174  decode.d1.loss_dice: 1.9209  decode.d2.loss_cls: 0.4792  decode.d2.loss_mask: 2.0651  decode.d2.loss_dice: 1.8323  decode.d3.loss_cls: 0.4324  decode.d3.loss_mask: 2.0958  decode.d3.loss_dice: 1.8870  decode.d4.loss_cls: 0.3770  decode.d4.loss_mask: 2.0864  decode.d4.loss_dice: 1.9172  decode.d5.loss_cls: 0.4286  decode.d5.loss_mask: 2.0958  decode.d5.loss_dice: 1.9398  decode.d6.loss_cls: 0.4145  decode.d6.loss_mask: 2.0994  decode.d6.loss_dice: 1.9220  decode.d7.loss_cls: 0.4549  decode.d7.loss_mask: 2.0633  decode.d7.loss_dice: 1.8654  decode.d8.loss_cls: 0.4577  decode.d8.loss_mask: 2.0625  decode.d8.loss_dice: 1.8938  mix_decode.loss_cls: 0.2880  mix_decode.loss_mask: 0.7536  mix_decode.loss_dice: 0.7996  mix_decode.d0.loss_cls: 0.2764  mix_decode.d0.loss_mask: 0.7741  mix_decode.d0.loss_dice: 0.8865  mix_decode.d1.loss_cls: 0.2612  mix_decode.d1.loss_mask: 0.7447  mix_decode.d1.loss_dice: 0.8172  mix_decode.d2.loss_cls: 0.3059  mix_decode.d2.loss_mask: 0.7499  mix_decode.d2.loss_dice: 0.7928  mix_decode.d3.loss_cls: 0.2786  mix_decode.d3.loss_mask: 0.7635  mix_decode.d3.loss_dice: 0.7986  mix_decode.d4.loss_cls: 0.3241  mix_decode.d4.loss_mask: 0.7625  mix_decode.d4.loss_dice: 0.7752  mix_decode.d5.loss_cls: 0.2925  mix_decode.d5.loss_mask: 0.7452  mix_decode.d5.loss_dice: 0.7864  mix_decode.d6.loss_cls: 0.2976  mix_decode.d6.loss_mask: 0.7346  mix_decode.d6.loss_dice: 0.7893  mix_decode.d7.loss_cls: 0.3009  mix_decode.d7.loss_mask: 0.7366  mix_decode.d7.loss_dice: 0.7980  mix_decode.d8.loss_cls: 0.3184  mix_decode.d8.loss_mask: 0.7758  mix_decode.d8.loss_dice: 0.8215
2025/03/29 14:22:10 - mmengine - INFO - Iter(train) [ 8900/20000]  base_lr: 5.8869e-05 lr: 5.8869e-05  eta: 2:42:51  time: 1.1508  data_time: 0.0220  memory: 11214  loss: 58.0781  decode.loss_cls: 0.4399  decode.loss_mask: 1.8518  decode.loss_dice: 1.7860  decode.d0.loss_cls: 0.5297  decode.d0.loss_mask: 1.9361  decode.d0.loss_dice: 1.8444  decode.d1.loss_cls: 0.4149  decode.d1.loss_mask: 1.9803  decode.d1.loss_dice: 1.7918  decode.d2.loss_cls: 0.3732  decode.d2.loss_mask: 1.9301  decode.d2.loss_dice: 1.7879  decode.d3.loss_cls: 0.4359  decode.d3.loss_mask: 1.8705  decode.d3.loss_dice: 1.7285  decode.d4.loss_cls: 0.5047  decode.d4.loss_mask: 1.8684  decode.d4.loss_dice: 1.6889  decode.d5.loss_cls: 0.4120  decode.d5.loss_mask: 1.8638  decode.d5.loss_dice: 1.7661  decode.d6.loss_cls: 0.4275  decode.d6.loss_mask: 1.8932  decode.d6.loss_dice: 1.7760  decode.d7.loss_cls: 0.4715  decode.d7.loss_mask: 1.8674  decode.d7.loss_dice: 1.7477  decode.d8.loss_cls: 0.4002  decode.d8.loss_mask: 1.9189  decode.d8.loss_dice: 1.8253  mix_decode.loss_cls: 0.2088  mix_decode.loss_mask: 0.6654  mix_decode.loss_dice: 0.8057  mix_decode.d0.loss_cls: 0.2245  mix_decode.d0.loss_mask: 0.6586  mix_decode.d0.loss_dice: 0.8564  mix_decode.d1.loss_cls: 0.2435  mix_decode.d1.loss_mask: 0.6372  mix_decode.d1.loss_dice: 0.7951  mix_decode.d2.loss_cls: 0.2373  mix_decode.d2.loss_mask: 0.6492  mix_decode.d2.loss_dice: 0.8040  mix_decode.d3.loss_cls: 0.2136  mix_decode.d3.loss_mask: 0.6473  mix_decode.d3.loss_dice: 0.7864  mix_decode.d4.loss_cls: 0.2100  mix_decode.d4.loss_mask: 0.6782  mix_decode.d4.loss_dice: 0.8018  mix_decode.d5.loss_cls: 0.2325  mix_decode.d5.loss_mask: 0.6649  mix_decode.d5.loss_dice: 0.8330  mix_decode.d6.loss_cls: 0.2210  mix_decode.d6.loss_mask: 0.6612  mix_decode.d6.loss_dice: 0.8030  mix_decode.d7.loss_cls: 0.2519  mix_decode.d7.loss_mask: 0.6599  mix_decode.d7.loss_dice: 0.8110  mix_decode.d8.loss_cls: 0.1825  mix_decode.d8.loss_mask: 0.6871  mix_decode.d8.loss_dice: 0.8147
2025/03/29 14:23:08 - mmengine - INFO - Iter(train) [ 8950/20000]  base_lr: 5.8630e-05 lr: 5.8630e-05  eta: 2:42:24  time: 1.1490  data_time: 0.0224  memory: 11210  loss: 59.3700  decode.loss_cls: 0.5505  decode.loss_mask: 1.9831  decode.loss_dice: 1.7676  decode.d0.loss_cls: 0.7302  decode.d0.loss_mask: 1.9529  decode.d0.loss_dice: 1.8764  decode.d1.loss_cls: 0.5714  decode.d1.loss_mask: 1.9868  decode.d1.loss_dice: 1.8057  decode.d2.loss_cls: 0.5378  decode.d2.loss_mask: 2.0223  decode.d2.loss_dice: 1.8351  decode.d3.loss_cls: 0.4879  decode.d3.loss_mask: 2.1204  decode.d3.loss_dice: 1.7771  decode.d4.loss_cls: 0.5657  decode.d4.loss_mask: 2.0109  decode.d4.loss_dice: 1.8394  decode.d5.loss_cls: 0.6389  decode.d5.loss_mask: 1.9715  decode.d5.loss_dice: 1.7793  decode.d6.loss_cls: 0.4217  decode.d6.loss_mask: 2.1853  decode.d6.loss_dice: 1.8652  decode.d7.loss_cls: 0.5532  decode.d7.loss_mask: 1.9485  decode.d7.loss_dice: 1.7877  decode.d8.loss_cls: 0.5430  decode.d8.loss_mask: 2.0391  decode.d8.loss_dice: 1.8588  mix_decode.loss_cls: 0.1979  mix_decode.loss_mask: 0.6290  mix_decode.loss_dice: 0.7044  mix_decode.d0.loss_cls: 0.2949  mix_decode.d0.loss_mask: 0.6014  mix_decode.d0.loss_dice: 0.6880  mix_decode.d1.loss_cls: 0.1783  mix_decode.d1.loss_mask: 0.6314  mix_decode.d1.loss_dice: 0.6925  mix_decode.d2.loss_cls: 0.2024  mix_decode.d2.loss_mask: 0.6342  mix_decode.d2.loss_dice: 0.6875  mix_decode.d3.loss_cls: 0.1967  mix_decode.d3.loss_mask: 0.6238  mix_decode.d3.loss_dice: 0.6802  mix_decode.d4.loss_cls: 0.2100  mix_decode.d4.loss_mask: 0.6353  mix_decode.d4.loss_dice: 0.7033  mix_decode.d5.loss_cls: 0.2323  mix_decode.d5.loss_mask: 0.6097  mix_decode.d5.loss_dice: 0.6997  mix_decode.d6.loss_cls: 0.2250  mix_decode.d6.loss_mask: 0.6302  mix_decode.d6.loss_dice: 0.6846  mix_decode.d7.loss_cls: 0.2260  mix_decode.d7.loss_mask: 0.6115  mix_decode.d7.loss_dice: 0.6792  mix_decode.d8.loss_cls: 0.2414  mix_decode.d8.loss_mask: 0.6221  mix_decode.d8.loss_dice: 0.7043
2025/03/29 14:24:05 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 14:24:05 - mmengine - INFO - Iter(train) [ 9000/20000]  base_lr: 5.8391e-05 lr: 5.8391e-05  eta: 2:41:57  time: 1.1524  data_time: 0.0225  memory: 11218  loss: 61.7839  decode.loss_cls: 0.3590  decode.loss_mask: 2.1748  decode.loss_dice: 1.7073  decode.d0.loss_cls: 0.3790  decode.d0.loss_mask: 2.2382  decode.d0.loss_dice: 1.7663  decode.d1.loss_cls: 0.4003  decode.d1.loss_mask: 2.1705  decode.d1.loss_dice: 1.7309  decode.d2.loss_cls: 0.3122  decode.d2.loss_mask: 2.1969  decode.d2.loss_dice: 1.7347  decode.d3.loss_cls: 0.3774  decode.d3.loss_mask: 2.1730  decode.d3.loss_dice: 1.7216  decode.d4.loss_cls: 0.4103  decode.d4.loss_mask: 2.1552  decode.d4.loss_dice: 1.7028  decode.d5.loss_cls: 0.3111  decode.d5.loss_mask: 2.2538  decode.d5.loss_dice: 1.7089  decode.d6.loss_cls: 0.3310  decode.d6.loss_mask: 2.2138  decode.d6.loss_dice: 1.7256  decode.d7.loss_cls: 0.4031  decode.d7.loss_mask: 2.1914  decode.d7.loss_dice: 1.7321  decode.d8.loss_cls: 0.3666  decode.d8.loss_mask: 2.1769  decode.d8.loss_dice: 1.7510  mix_decode.loss_cls: 0.3403  mix_decode.loss_mask: 0.7288  mix_decode.loss_dice: 0.8267  mix_decode.d0.loss_cls: 0.3008  mix_decode.d0.loss_mask: 0.6956  mix_decode.d0.loss_dice: 0.8526  mix_decode.d1.loss_cls: 0.3453  mix_decode.d1.loss_mask: 0.7254  mix_decode.d1.loss_dice: 0.8337  mix_decode.d2.loss_cls: 0.3514  mix_decode.d2.loss_mask: 0.7242  mix_decode.d2.loss_dice: 0.8194  mix_decode.d3.loss_cls: 0.3591  mix_decode.d3.loss_mask: 0.6941  mix_decode.d3.loss_dice: 0.8153  mix_decode.d4.loss_cls: 0.3431  mix_decode.d4.loss_mask: 0.7179  mix_decode.d4.loss_dice: 0.8301  mix_decode.d5.loss_cls: 0.3567  mix_decode.d5.loss_mask: 0.7121  mix_decode.d5.loss_dice: 0.8212  mix_decode.d6.loss_cls: 0.3476  mix_decode.d6.loss_mask: 0.7219  mix_decode.d6.loss_dice: 0.8397  mix_decode.d7.loss_cls: 0.4218  mix_decode.d7.loss_mask: 0.7077  mix_decode.d7.loss_dice: 0.8023  mix_decode.d8.loss_cls: 0.3416  mix_decode.d8.loss_mask: 0.7187  mix_decode.d8.loss_dice: 0.8134
2025/03/29 14:25:03 - mmengine - INFO - Iter(train) [ 9050/20000]  base_lr: 5.8152e-05 lr: 5.8152e-05  eta: 2:41:29  time: 1.1668  data_time: 0.0226  memory: 11214  loss: 62.4351  decode.loss_cls: 0.3768  decode.loss_mask: 2.0528  decode.loss_dice: 1.8211  decode.d0.loss_cls: 0.4990  decode.d0.loss_mask: 2.0870  decode.d0.loss_dice: 1.8584  decode.d1.loss_cls: 0.4878  decode.d1.loss_mask: 1.9988  decode.d1.loss_dice: 1.8246  decode.d2.loss_cls: 0.4336  decode.d2.loss_mask: 2.0534  decode.d2.loss_dice: 1.8092  decode.d3.loss_cls: 0.4445  decode.d3.loss_mask: 2.0590  decode.d3.loss_dice: 1.8358  decode.d4.loss_cls: 0.4378  decode.d4.loss_mask: 2.0286  decode.d4.loss_dice: 1.8514  decode.d5.loss_cls: 0.4637  decode.d5.loss_mask: 2.0522  decode.d5.loss_dice: 1.8103  decode.d6.loss_cls: 0.4604  decode.d6.loss_mask: 2.0796  decode.d6.loss_dice: 1.8032  decode.d7.loss_cls: 0.4289  decode.d7.loss_mask: 2.0470  decode.d7.loss_dice: 1.8161  decode.d8.loss_cls: 0.4053  decode.d8.loss_mask: 2.1255  decode.d8.loss_dice: 1.8536  mix_decode.loss_cls: 0.2347  mix_decode.loss_mask: 0.7239  mix_decode.loss_dice: 0.9024  mix_decode.d0.loss_cls: 0.3007  mix_decode.d0.loss_mask: 0.7171  mix_decode.d0.loss_dice: 0.9285  mix_decode.d1.loss_cls: 0.2439  mix_decode.d1.loss_mask: 0.7673  mix_decode.d1.loss_dice: 0.8941  mix_decode.d2.loss_cls: 0.2809  mix_decode.d2.loss_mask: 0.7615  mix_decode.d2.loss_dice: 0.8874  mix_decode.d3.loss_cls: 0.2403  mix_decode.d3.loss_mask: 0.7577  mix_decode.d3.loss_dice: 0.8834  mix_decode.d4.loss_cls: 0.3270  mix_decode.d4.loss_mask: 0.7496  mix_decode.d4.loss_dice: 0.8988  mix_decode.d5.loss_cls: 0.3195  mix_decode.d5.loss_mask: 0.7829  mix_decode.d5.loss_dice: 0.8915  mix_decode.d6.loss_cls: 0.2971  mix_decode.d6.loss_mask: 0.7241  mix_decode.d6.loss_dice: 0.8768  mix_decode.d7.loss_cls: 0.2650  mix_decode.d7.loss_mask: 0.7170  mix_decode.d7.loss_dice: 0.9047  mix_decode.d8.loss_cls: 0.2567  mix_decode.d8.loss_mask: 0.7104  mix_decode.d8.loss_dice: 0.8849
2025/03/29 14:26:01 - mmengine - INFO - Iter(train) [ 9100/20000]  base_lr: 5.7913e-05 lr: 5.7913e-05  eta: 2:41:01  time: 1.1513  data_time: 0.0223  memory: 11208  loss: 59.7710  decode.loss_cls: 0.3191  decode.loss_mask: 2.0563  decode.loss_dice: 1.8777  decode.d0.loss_cls: 0.5162  decode.d0.loss_mask: 2.0286  decode.d0.loss_dice: 1.9063  decode.d1.loss_cls: 0.3789  decode.d1.loss_mask: 2.0606  decode.d1.loss_dice: 1.8558  decode.d2.loss_cls: 0.4097  decode.d2.loss_mask: 2.0022  decode.d2.loss_dice: 1.8522  decode.d3.loss_cls: 0.3146  decode.d3.loss_mask: 2.0556  decode.d3.loss_dice: 1.8757  decode.d4.loss_cls: 0.2745  decode.d4.loss_mask: 2.0456  decode.d4.loss_dice: 1.9128  decode.d5.loss_cls: 0.2968  decode.d5.loss_mask: 2.1025  decode.d5.loss_dice: 1.9195  decode.d6.loss_cls: 0.2848  decode.d6.loss_mask: 2.0594  decode.d6.loss_dice: 1.9303  decode.d7.loss_cls: 0.3368  decode.d7.loss_mask: 2.1054  decode.d7.loss_dice: 1.8977  decode.d8.loss_cls: 0.3483  decode.d8.loss_mask: 2.0259  decode.d8.loss_dice: 1.8975  mix_decode.loss_cls: 0.2343  mix_decode.loss_mask: 0.6101  mix_decode.loss_dice: 0.8101  mix_decode.d0.loss_cls: 0.2710  mix_decode.d0.loss_mask: 0.6328  mix_decode.d0.loss_dice: 0.8290  mix_decode.d1.loss_cls: 0.2783  mix_decode.d1.loss_mask: 0.6360  mix_decode.d1.loss_dice: 0.8137  mix_decode.d2.loss_cls: 0.2493  mix_decode.d2.loss_mask: 0.6302  mix_decode.d2.loss_dice: 0.7923  mix_decode.d3.loss_cls: 0.2357  mix_decode.d3.loss_mask: 0.6390  mix_decode.d3.loss_dice: 0.7940  mix_decode.d4.loss_cls: 0.2810  mix_decode.d4.loss_mask: 0.6198  mix_decode.d4.loss_dice: 0.7841  mix_decode.d5.loss_cls: 0.2605  mix_decode.d5.loss_mask: 0.6120  mix_decode.d5.loss_dice: 0.8165  mix_decode.d6.loss_cls: 0.2708  mix_decode.d6.loss_mask: 0.6028  mix_decode.d6.loss_dice: 0.7768  mix_decode.d7.loss_cls: 0.2508  mix_decode.d7.loss_mask: 0.6167  mix_decode.d7.loss_dice: 0.8049  mix_decode.d8.loss_cls: 0.2388  mix_decode.d8.loss_mask: 0.6268  mix_decode.d8.loss_dice: 0.8055
2025/03/29 14:26:58 - mmengine - INFO - Iter(train) [ 9150/20000]  base_lr: 5.7674e-05 lr: 5.7674e-05  eta: 2:40:32  time: 1.1455  data_time: 0.0223  memory: 11224  loss: 60.8691  decode.loss_cls: 0.4852  decode.loss_mask: 2.0464  decode.loss_dice: 1.8023  decode.d0.loss_cls: 0.5752  decode.d0.loss_mask: 1.9780  decode.d0.loss_dice: 1.8841  decode.d1.loss_cls: 0.6000  decode.d1.loss_mask: 1.9503  decode.d1.loss_dice: 1.7845  decode.d2.loss_cls: 0.6923  decode.d2.loss_mask: 1.9158  decode.d2.loss_dice: 1.7272  decode.d3.loss_cls: 0.6013  decode.d3.loss_mask: 1.9759  decode.d3.loss_dice: 1.8127  decode.d4.loss_cls: 0.5391  decode.d4.loss_mask: 2.0149  decode.d4.loss_dice: 1.8277  decode.d5.loss_cls: 0.5750  decode.d5.loss_mask: 1.9953  decode.d5.loss_dice: 1.8095  decode.d6.loss_cls: 0.4608  decode.d6.loss_mask: 1.9712  decode.d6.loss_dice: 1.8203  decode.d7.loss_cls: 0.5054  decode.d7.loss_mask: 2.0066  decode.d7.loss_dice: 1.8725  decode.d8.loss_cls: 0.4296  decode.d8.loss_mask: 1.9840  decode.d8.loss_dice: 1.8395  mix_decode.loss_cls: 0.2639  mix_decode.loss_mask: 0.6742  mix_decode.loss_dice: 0.7475  mix_decode.d0.loss_cls: 0.2897  mix_decode.d0.loss_mask: 0.7038  mix_decode.d0.loss_dice: 0.8650  mix_decode.d1.loss_cls: 0.3118  mix_decode.d1.loss_mask: 0.6507  mix_decode.d1.loss_dice: 0.7292  mix_decode.d2.loss_cls: 0.2437  mix_decode.d2.loss_mask: 0.7042  mix_decode.d2.loss_dice: 0.7660  mix_decode.d3.loss_cls: 0.2896  mix_decode.d3.loss_mask: 0.6645  mix_decode.d3.loss_dice: 0.7680  mix_decode.d4.loss_cls: 0.2741  mix_decode.d4.loss_mask: 0.6730  mix_decode.d4.loss_dice: 0.7781  mix_decode.d5.loss_cls: 0.2603  mix_decode.d5.loss_mask: 0.6954  mix_decode.d5.loss_dice: 0.8305  mix_decode.d6.loss_cls: 0.2454  mix_decode.d6.loss_mask: 0.6988  mix_decode.d6.loss_dice: 0.7948  mix_decode.d7.loss_cls: 0.2715  mix_decode.d7.loss_mask: 0.6932  mix_decode.d7.loss_dice: 0.7690  mix_decode.d8.loss_cls: 0.2768  mix_decode.d8.loss_mask: 0.6877  mix_decode.d8.loss_dice: 0.7663
2025/03/29 14:27:56 - mmengine - INFO - Iter(train) [ 9200/20000]  base_lr: 5.7435e-05 lr: 5.7435e-05  eta: 2:40:03  time: 1.1496  data_time: 0.0228  memory: 11213  loss: 58.1680  decode.loss_cls: 0.3193  decode.loss_mask: 1.9306  decode.loss_dice: 1.6720  decode.d0.loss_cls: 0.4031  decode.d0.loss_mask: 1.9531  decode.d0.loss_dice: 1.6533  decode.d1.loss_cls: 0.3621  decode.d1.loss_mask: 1.9929  decode.d1.loss_dice: 1.6913  decode.d2.loss_cls: 0.3685  decode.d2.loss_mask: 1.9674  decode.d2.loss_dice: 1.6947  decode.d3.loss_cls: 0.3677  decode.d3.loss_mask: 1.9242  decode.d3.loss_dice: 1.6608  decode.d4.loss_cls: 0.3475  decode.d4.loss_mask: 1.9203  decode.d4.loss_dice: 1.6524  decode.d5.loss_cls: 0.3604  decode.d5.loss_mask: 1.9229  decode.d5.loss_dice: 1.6815  decode.d6.loss_cls: 0.3230  decode.d6.loss_mask: 1.9663  decode.d6.loss_dice: 1.6793  decode.d7.loss_cls: 0.3365  decode.d7.loss_mask: 1.9534  decode.d7.loss_dice: 1.6604  decode.d8.loss_cls: 0.3740  decode.d8.loss_mask: 1.9025  decode.d8.loss_dice: 1.6526  mix_decode.loss_cls: 0.3042  mix_decode.loss_mask: 0.7071  mix_decode.loss_dice: 0.8208  mix_decode.d0.loss_cls: 0.2882  mix_decode.d0.loss_mask: 0.6912  mix_decode.d0.loss_dice: 0.8984  mix_decode.d1.loss_cls: 0.3192  mix_decode.d1.loss_mask: 0.6996  mix_decode.d1.loss_dice: 0.8444  mix_decode.d2.loss_cls: 0.3042  mix_decode.d2.loss_mask: 0.6790  mix_decode.d2.loss_dice: 0.8256  mix_decode.d3.loss_cls: 0.2797  mix_decode.d3.loss_mask: 0.6797  mix_decode.d3.loss_dice: 0.8399  mix_decode.d4.loss_cls: 0.2999  mix_decode.d4.loss_mask: 0.6899  mix_decode.d4.loss_dice: 0.8438  mix_decode.d5.loss_cls: 0.3297  mix_decode.d5.loss_mask: 0.7024  mix_decode.d5.loss_dice: 0.8638  mix_decode.d6.loss_cls: 0.2636  mix_decode.d6.loss_mask: 0.7280  mix_decode.d6.loss_dice: 0.8613  mix_decode.d7.loss_cls: 0.3518  mix_decode.d7.loss_mask: 0.6861  mix_decode.d7.loss_dice: 0.8193  mix_decode.d8.loss_cls: 0.3246  mix_decode.d8.loss_mask: 0.6950  mix_decode.d8.loss_dice: 0.8343
2025/03/29 14:28:53 - mmengine - INFO - Iter(train) [ 9250/20000]  base_lr: 5.7195e-05 lr: 5.7195e-05  eta: 2:39:34  time: 1.1496  data_time: 0.0228  memory: 11214  loss: 59.2289  decode.loss_cls: 0.4116  decode.loss_mask: 2.0282  decode.loss_dice: 1.7306  decode.d0.loss_cls: 0.4878  decode.d0.loss_mask: 2.0232  decode.d0.loss_dice: 1.8273  decode.d1.loss_cls: 0.4395  decode.d1.loss_mask: 2.0091  decode.d1.loss_dice: 1.7726  decode.d2.loss_cls: 0.5152  decode.d2.loss_mask: 2.0203  decode.d2.loss_dice: 1.7355  decode.d3.loss_cls: 0.4705  decode.d3.loss_mask: 2.0180  decode.d3.loss_dice: 1.7574  decode.d4.loss_cls: 0.4175  decode.d4.loss_mask: 2.0495  decode.d4.loss_dice: 1.7610  decode.d5.loss_cls: 0.3886  decode.d5.loss_mask: 2.0680  decode.d5.loss_dice: 1.8368  decode.d6.loss_cls: 0.4788  decode.d6.loss_mask: 2.0156  decode.d6.loss_dice: 1.7904  decode.d7.loss_cls: 0.4834  decode.d7.loss_mask: 1.9945  decode.d7.loss_dice: 1.7721  decode.d8.loss_cls: 0.4429  decode.d8.loss_mask: 2.0031  decode.d8.loss_dice: 1.7560  mix_decode.loss_cls: 0.2249  mix_decode.loss_mask: 0.6684  mix_decode.loss_dice: 0.7536  mix_decode.d0.loss_cls: 0.2527  mix_decode.d0.loss_mask: 0.6754  mix_decode.d0.loss_dice: 0.8311  mix_decode.d1.loss_cls: 0.1998  mix_decode.d1.loss_mask: 0.6749  mix_decode.d1.loss_dice: 0.7790  mix_decode.d2.loss_cls: 0.2026  mix_decode.d2.loss_mask: 0.6699  mix_decode.d2.loss_dice: 0.7737  mix_decode.d3.loss_cls: 0.1971  mix_decode.d3.loss_mask: 0.6679  mix_decode.d3.loss_dice: 0.7691  mix_decode.d4.loss_cls: 0.2129  mix_decode.d4.loss_mask: 0.6796  mix_decode.d4.loss_dice: 0.7665  mix_decode.d5.loss_cls: 0.2321  mix_decode.d5.loss_mask: 0.6685  mix_decode.d5.loss_dice: 0.7921  mix_decode.d6.loss_cls: 0.2091  mix_decode.d6.loss_mask: 0.6807  mix_decode.d6.loss_dice: 0.7778  mix_decode.d7.loss_cls: 0.2596  mix_decode.d7.loss_mask: 0.6477  mix_decode.d7.loss_dice: 0.7726  mix_decode.d8.loss_cls: 0.2432  mix_decode.d8.loss_mask: 0.6712  mix_decode.d8.loss_dice: 0.7704
2025/03/29 14:29:51 - mmengine - INFO - Iter(train) [ 9300/20000]  base_lr: 5.6956e-05 lr: 5.6956e-05  eta: 2:39:04  time: 1.1484  data_time: 0.0222  memory: 11212  loss: 54.6351  decode.loss_cls: 0.2760  decode.loss_mask: 1.8587  decode.loss_dice: 1.5693  decode.d0.loss_cls: 0.4849  decode.d0.loss_mask: 1.7619  decode.d0.loss_dice: 1.5725  decode.d1.loss_cls: 0.3695  decode.d1.loss_mask: 1.7629  decode.d1.loss_dice: 1.6076  decode.d2.loss_cls: 0.3834  decode.d2.loss_mask: 1.7747  decode.d2.loss_dice: 1.6103  decode.d3.loss_cls: 0.3952  decode.d3.loss_mask: 1.7671  decode.d3.loss_dice: 1.6045  decode.d4.loss_cls: 0.3235  decode.d4.loss_mask: 1.7856  decode.d4.loss_dice: 1.6190  decode.d5.loss_cls: 0.2840  decode.d5.loss_mask: 1.7766  decode.d5.loss_dice: 1.6276  decode.d6.loss_cls: 0.3199  decode.d6.loss_mask: 1.7848  decode.d6.loss_dice: 1.6388  decode.d7.loss_cls: 0.3629  decode.d7.loss_mask: 1.7911  decode.d7.loss_dice: 1.6263  decode.d8.loss_cls: 0.3832  decode.d8.loss_mask: 1.8550  decode.d8.loss_dice: 1.6271  mix_decode.loss_cls: 0.2352  mix_decode.loss_mask: 0.5956  mix_decode.loss_dice: 0.8042  mix_decode.d0.loss_cls: 0.2763  mix_decode.d0.loss_mask: 0.6075  mix_decode.d0.loss_dice: 0.8619  mix_decode.d1.loss_cls: 0.2323  mix_decode.d1.loss_mask: 0.6213  mix_decode.d1.loss_dice: 0.8468  mix_decode.d2.loss_cls: 0.2766  mix_decode.d2.loss_mask: 0.5947  mix_decode.d2.loss_dice: 0.8016  mix_decode.d3.loss_cls: 0.2714  mix_decode.d3.loss_mask: 0.6013  mix_decode.d3.loss_dice: 0.8092  mix_decode.d4.loss_cls: 0.2595  mix_decode.d4.loss_mask: 0.6129  mix_decode.d4.loss_dice: 0.8398  mix_decode.d5.loss_cls: 0.2556  mix_decode.d5.loss_mask: 0.6167  mix_decode.d5.loss_dice: 0.8440  mix_decode.d6.loss_cls: 0.2376  mix_decode.d6.loss_mask: 0.6364  mix_decode.d6.loss_dice: 0.8605  mix_decode.d7.loss_cls: 0.3075  mix_decode.d7.loss_mask: 0.6052  mix_decode.d7.loss_dice: 0.8043  mix_decode.d8.loss_cls: 0.2472  mix_decode.d8.loss_mask: 0.6256  mix_decode.d8.loss_dice: 0.8422
2025/03/29 14:30:48 - mmengine - INFO - Iter(train) [ 9350/20000]  base_lr: 5.6716e-05 lr: 5.6716e-05  eta: 2:38:34  time: 1.1545  data_time: 0.0227  memory: 11209  loss: 60.9897  decode.loss_cls: 0.3551  decode.loss_mask: 2.1206  decode.loss_dice: 1.8258  decode.d0.loss_cls: 0.5151  decode.d0.loss_mask: 2.0989  decode.d0.loss_dice: 1.7839  decode.d1.loss_cls: 0.4561  decode.d1.loss_mask: 2.1044  decode.d1.loss_dice: 1.7092  decode.d2.loss_cls: 0.3971  decode.d2.loss_mask: 2.0939  decode.d2.loss_dice: 1.7240  decode.d3.loss_cls: 0.3610  decode.d3.loss_mask: 2.1659  decode.d3.loss_dice: 1.8168  decode.d4.loss_cls: 0.3259  decode.d4.loss_mask: 2.1428  decode.d4.loss_dice: 1.7497  decode.d5.loss_cls: 0.3345  decode.d5.loss_mask: 2.2133  decode.d5.loss_dice: 1.8070  decode.d6.loss_cls: 0.3767  decode.d6.loss_mask: 2.1208  decode.d6.loss_dice: 1.7640  decode.d7.loss_cls: 0.3988  decode.d7.loss_mask: 2.1168  decode.d7.loss_dice: 1.7664  decode.d8.loss_cls: 0.3421  decode.d8.loss_mask: 2.1210  decode.d8.loss_dice: 1.8330  mix_decode.loss_cls: 0.4620  mix_decode.loss_mask: 0.6288  mix_decode.loss_dice: 0.7799  mix_decode.d0.loss_cls: 0.3559  mix_decode.d0.loss_mask: 0.6765  mix_decode.d0.loss_dice: 0.8003  mix_decode.d1.loss_cls: 0.3933  mix_decode.d1.loss_mask: 0.6265  mix_decode.d1.loss_dice: 0.7554  mix_decode.d2.loss_cls: 0.3556  mix_decode.d2.loss_mask: 0.6611  mix_decode.d2.loss_dice: 0.7576  mix_decode.d3.loss_cls: 0.3609  mix_decode.d3.loss_mask: 0.6432  mix_decode.d3.loss_dice: 0.7906  mix_decode.d4.loss_cls: 0.3650  mix_decode.d4.loss_mask: 0.6463  mix_decode.d4.loss_dice: 0.7776  mix_decode.d5.loss_cls: 0.3695  mix_decode.d5.loss_mask: 0.6568  mix_decode.d5.loss_dice: 0.7883  mix_decode.d6.loss_cls: 0.3528  mix_decode.d6.loss_mask: 0.6247  mix_decode.d6.loss_dice: 0.7709  mix_decode.d7.loss_cls: 0.4069  mix_decode.d7.loss_mask: 0.6622  mix_decode.d7.loss_dice: 0.7600  mix_decode.d8.loss_cls: 0.3860  mix_decode.d8.loss_mask: 0.6612  mix_decode.d8.loss_dice: 0.7733
2025/03/29 14:31:46 - mmengine - INFO - Iter(train) [ 9400/20000]  base_lr: 5.6477e-05 lr: 5.6477e-05  eta: 2:38:04  time: 1.1506  data_time: 0.0224  memory: 11215  loss: 61.8258  decode.loss_cls: 0.3230  decode.loss_mask: 2.0181  decode.loss_dice: 2.0137  decode.d0.loss_cls: 0.3249  decode.d0.loss_mask: 2.0660  decode.d0.loss_dice: 2.0540  decode.d1.loss_cls: 0.3155  decode.d1.loss_mask: 2.0153  decode.d1.loss_dice: 2.0251  decode.d2.loss_cls: 0.4143  decode.d2.loss_mask: 2.0137  decode.d2.loss_dice: 1.9884  decode.d3.loss_cls: 0.3443  decode.d3.loss_mask: 2.0227  decode.d3.loss_dice: 1.9860  decode.d4.loss_cls: 0.3379  decode.d4.loss_mask: 2.0008  decode.d4.loss_dice: 1.9985  decode.d5.loss_cls: 0.3435  decode.d5.loss_mask: 1.9826  decode.d5.loss_dice: 1.9999  decode.d6.loss_cls: 0.3836  decode.d6.loss_mask: 2.0063  decode.d6.loss_dice: 1.9840  decode.d7.loss_cls: 0.3344  decode.d7.loss_mask: 2.0297  decode.d7.loss_dice: 1.9867  decode.d8.loss_cls: 0.4314  decode.d8.loss_mask: 2.0016  decode.d8.loss_dice: 1.9984  mix_decode.loss_cls: 0.3450  mix_decode.loss_mask: 0.6011  mix_decode.loss_dice: 0.8539  mix_decode.d0.loss_cls: 0.2612  mix_decode.d0.loss_mask: 0.6211  mix_decode.d0.loss_dice: 0.9169  mix_decode.d1.loss_cls: 0.3207  mix_decode.d1.loss_mask: 0.6084  mix_decode.d1.loss_dice: 0.8657  mix_decode.d2.loss_cls: 0.3104  mix_decode.d2.loss_mask: 0.6189  mix_decode.d2.loss_dice: 0.8432  mix_decode.d3.loss_cls: 0.3180  mix_decode.d3.loss_mask: 0.6131  mix_decode.d3.loss_dice: 0.8842  mix_decode.d4.loss_cls: 0.3588  mix_decode.d4.loss_mask: 0.6114  mix_decode.d4.loss_dice: 0.8614  mix_decode.d5.loss_cls: 0.3240  mix_decode.d5.loss_mask: 0.6295  mix_decode.d5.loss_dice: 0.8604  mix_decode.d6.loss_cls: 0.3624  mix_decode.d6.loss_mask: 0.6122  mix_decode.d6.loss_dice: 0.8532  mix_decode.d7.loss_cls: 0.3798  mix_decode.d7.loss_mask: 0.5922  mix_decode.d7.loss_dice: 0.8518  mix_decode.d8.loss_cls: 0.3566  mix_decode.d8.loss_mask: 0.6037  mix_decode.d8.loss_dice: 0.8427
2025/03/29 14:32:43 - mmengine - INFO - Iter(train) [ 9450/20000]  base_lr: 5.6237e-05 lr: 5.6237e-05  eta: 2:37:34  time: 1.1449  data_time: 0.0227  memory: 11221  loss: 58.4031  decode.loss_cls: 0.4073  decode.loss_mask: 1.9933  decode.loss_dice: 1.7001  decode.d0.loss_cls: 0.4927  decode.d0.loss_mask: 1.9560  decode.d0.loss_dice: 1.6972  decode.d1.loss_cls: 0.3261  decode.d1.loss_mask: 1.9782  decode.d1.loss_dice: 1.7054  decode.d2.loss_cls: 0.3313  decode.d2.loss_mask: 2.0385  decode.d2.loss_dice: 1.7329  decode.d3.loss_cls: 0.3765  decode.d3.loss_mask: 1.9929  decode.d3.loss_dice: 1.6852  decode.d4.loss_cls: 0.3755  decode.d4.loss_mask: 2.0832  decode.d4.loss_dice: 1.7192  decode.d5.loss_cls: 0.3946  decode.d5.loss_mask: 2.0307  decode.d5.loss_dice: 1.7145  decode.d6.loss_cls: 0.3781  decode.d6.loss_mask: 1.9613  decode.d6.loss_dice: 1.7055  decode.d7.loss_cls: 0.3986  decode.d7.loss_mask: 1.9873  decode.d7.loss_dice: 1.6782  decode.d8.loss_cls: 0.3870  decode.d8.loss_mask: 2.0139  decode.d8.loss_dice: 1.6872  mix_decode.loss_cls: 0.2510  mix_decode.loss_mask: 0.6872  mix_decode.loss_dice: 0.7710  mix_decode.d0.loss_cls: 0.3289  mix_decode.d0.loss_mask: 0.6724  mix_decode.d0.loss_dice: 0.8230  mix_decode.d1.loss_cls: 0.2850  mix_decode.d1.loss_mask: 0.6783  mix_decode.d1.loss_dice: 0.7961  mix_decode.d2.loss_cls: 0.2722  mix_decode.d2.loss_mask: 0.6800  mix_decode.d2.loss_dice: 0.7827  mix_decode.d3.loss_cls: 0.3153  mix_decode.d3.loss_mask: 0.6711  mix_decode.d3.loss_dice: 0.7632  mix_decode.d4.loss_cls: 0.2701  mix_decode.d4.loss_mask: 0.6877  mix_decode.d4.loss_dice: 0.7749  mix_decode.d5.loss_cls: 0.2174  mix_decode.d5.loss_mask: 0.6956  mix_decode.d5.loss_dice: 0.7972  mix_decode.d6.loss_cls: 0.2576  mix_decode.d6.loss_mask: 0.6969  mix_decode.d6.loss_dice: 0.7899  mix_decode.d7.loss_cls: 0.2757  mix_decode.d7.loss_mask: 0.6997  mix_decode.d7.loss_dice: 0.7833  mix_decode.d8.loss_cls: 0.2577  mix_decode.d8.loss_mask: 0.7012  mix_decode.d8.loss_dice: 0.7925
2025/03/29 14:33:41 - mmengine - INFO - Iter(train) [ 9500/20000]  base_lr: 5.5997e-05 lr: 5.5997e-05  eta: 2:37:03  time: 1.1490  data_time: 0.0221  memory: 11229  loss: 58.7500  decode.loss_cls: 0.4046  decode.loss_mask: 1.8229  decode.loss_dice: 1.7720  decode.d0.loss_cls: 0.4749  decode.d0.loss_mask: 1.8634  decode.d0.loss_dice: 1.8024  decode.d1.loss_cls: 0.4860  decode.d1.loss_mask: 1.8110  decode.d1.loss_dice: 1.7542  decode.d2.loss_cls: 0.4266  decode.d2.loss_mask: 1.7829  decode.d2.loss_dice: 1.7374  decode.d3.loss_cls: 0.4185  decode.d3.loss_mask: 1.7627  decode.d3.loss_dice: 1.7283  decode.d4.loss_cls: 0.4485  decode.d4.loss_mask: 1.8383  decode.d4.loss_dice: 1.7605  decode.d5.loss_cls: 0.4184  decode.d5.loss_mask: 1.8223  decode.d5.loss_dice: 1.7858  decode.d6.loss_cls: 0.3512  decode.d6.loss_mask: 1.8463  decode.d6.loss_dice: 1.8127  decode.d7.loss_cls: 0.4915  decode.d7.loss_mask: 1.7758  decode.d7.loss_dice: 1.7243  decode.d8.loss_cls: 0.5226  decode.d8.loss_mask: 1.7873  decode.d8.loss_dice: 1.7085  mix_decode.loss_cls: 0.3725  mix_decode.loss_mask: 0.6887  mix_decode.loss_dice: 0.8216  mix_decode.d0.loss_cls: 0.3222  mix_decode.d0.loss_mask: 0.6939  mix_decode.d0.loss_dice: 0.8367  mix_decode.d1.loss_cls: 0.2945  mix_decode.d1.loss_mask: 0.6712  mix_decode.d1.loss_dice: 0.8307  mix_decode.d2.loss_cls: 0.3255  mix_decode.d2.loss_mask: 0.6566  mix_decode.d2.loss_dice: 0.8177  mix_decode.d3.loss_cls: 0.3305  mix_decode.d3.loss_mask: 0.6830  mix_decode.d3.loss_dice: 0.8182  mix_decode.d4.loss_cls: 0.3428  mix_decode.d4.loss_mask: 0.6662  mix_decode.d4.loss_dice: 0.8177  mix_decode.d5.loss_cls: 0.3320  mix_decode.d5.loss_mask: 0.6984  mix_decode.d5.loss_dice: 0.8538  mix_decode.d6.loss_cls: 0.3221  mix_decode.d6.loss_mask: 0.7217  mix_decode.d6.loss_dice: 0.8544  mix_decode.d7.loss_cls: 0.3521  mix_decode.d7.loss_mask: 0.6852  mix_decode.d7.loss_dice: 0.8465  mix_decode.d8.loss_cls: 0.4227  mix_decode.d8.loss_mask: 0.6935  mix_decode.d8.loss_dice: 0.8353
2025/03/29 14:34:39 - mmengine - INFO - Iter(train) [ 9550/20000]  base_lr: 5.5757e-05 lr: 5.5757e-05  eta: 2:36:32  time: 1.1504  data_time: 0.0224  memory: 11227  loss: 59.1243  decode.loss_cls: 0.5178  decode.loss_mask: 1.8792  decode.loss_dice: 1.7031  decode.d0.loss_cls: 0.5470  decode.d0.loss_mask: 1.9116  decode.d0.loss_dice: 1.6777  decode.d1.loss_cls: 0.5482  decode.d1.loss_mask: 1.8118  decode.d1.loss_dice: 1.6957  decode.d2.loss_cls: 0.5334  decode.d2.loss_mask: 1.7955  decode.d2.loss_dice: 1.6821  decode.d3.loss_cls: 0.5910  decode.d3.loss_mask: 1.8613  decode.d3.loss_dice: 1.7021  decode.d4.loss_cls: 0.5115  decode.d4.loss_mask: 1.8633  decode.d4.loss_dice: 1.6776  decode.d5.loss_cls: 0.4817  decode.d5.loss_mask: 1.9018  decode.d5.loss_dice: 1.7085  decode.d6.loss_cls: 0.5437  decode.d6.loss_mask: 1.8568  decode.d6.loss_dice: 1.7200  decode.d7.loss_cls: 0.5021  decode.d7.loss_mask: 1.9103  decode.d7.loss_dice: 1.6902  decode.d8.loss_cls: 0.5430  decode.d8.loss_mask: 1.8915  decode.d8.loss_dice: 1.7108  mix_decode.loss_cls: 0.3585  mix_decode.loss_mask: 0.6130  mix_decode.loss_dice: 0.8658  mix_decode.d0.loss_cls: 0.3303  mix_decode.d0.loss_mask: 0.6381  mix_decode.d0.loss_dice: 0.8896  mix_decode.d1.loss_cls: 0.3234  mix_decode.d1.loss_mask: 0.5871  mix_decode.d1.loss_dice: 0.8179  mix_decode.d2.loss_cls: 0.3181  mix_decode.d2.loss_mask: 0.6276  mix_decode.d2.loss_dice: 0.8145  mix_decode.d3.loss_cls: 0.3853  mix_decode.d3.loss_mask: 0.5813  mix_decode.d3.loss_dice: 0.8270  mix_decode.d4.loss_cls: 0.3411  mix_decode.d4.loss_mask: 0.6478  mix_decode.d4.loss_dice: 0.8446  mix_decode.d5.loss_cls: 0.3285  mix_decode.d5.loss_mask: 0.6413  mix_decode.d5.loss_dice: 0.8558  mix_decode.d6.loss_cls: 0.3853  mix_decode.d6.loss_mask: 0.6316  mix_decode.d6.loss_dice: 0.8414  mix_decode.d7.loss_cls: 0.4279  mix_decode.d7.loss_mask: 0.5966  mix_decode.d7.loss_dice: 0.8285  mix_decode.d8.loss_cls: 0.3442  mix_decode.d8.loss_mask: 0.6159  mix_decode.d8.loss_dice: 0.8464
2025/03/29 14:35:36 - mmengine - INFO - Iter(train) [ 9600/20000]  base_lr: 5.5517e-05 lr: 5.5517e-05  eta: 2:36:01  time: 1.1485  data_time: 0.0227  memory: 11205  loss: 59.9455  decode.loss_cls: 0.2724  decode.loss_mask: 1.9687  decode.loss_dice: 1.8809  decode.d0.loss_cls: 0.3228  decode.d0.loss_mask: 1.9906  decode.d0.loss_dice: 1.9004  decode.d1.loss_cls: 0.2275  decode.d1.loss_mask: 1.9760  decode.d1.loss_dice: 1.8855  decode.d2.loss_cls: 0.3131  decode.d2.loss_mask: 1.9222  decode.d2.loss_dice: 1.8699  decode.d3.loss_cls: 0.2358  decode.d3.loss_mask: 1.9842  decode.d3.loss_dice: 1.8749  decode.d4.loss_cls: 0.2809  decode.d4.loss_mask: 1.9830  decode.d4.loss_dice: 1.8790  decode.d5.loss_cls: 0.2373  decode.d5.loss_mask: 2.0018  decode.d5.loss_dice: 1.9192  decode.d6.loss_cls: 0.2501  decode.d6.loss_mask: 1.9797  decode.d6.loss_dice: 1.8912  decode.d7.loss_cls: 0.2936  decode.d7.loss_mask: 1.9813  decode.d7.loss_dice: 1.8979  decode.d8.loss_cls: 0.2784  decode.d8.loss_mask: 1.9381  decode.d8.loss_dice: 1.8868  mix_decode.loss_cls: 0.2164  mix_decode.loss_mask: 0.7063  mix_decode.loss_dice: 0.9204  mix_decode.d0.loss_cls: 0.2539  mix_decode.d0.loss_mask: 0.6429  mix_decode.d0.loss_dice: 0.9756  mix_decode.d1.loss_cls: 0.2312  mix_decode.d1.loss_mask: 0.6955  mix_decode.d1.loss_dice: 0.9226  mix_decode.d2.loss_cls: 0.2526  mix_decode.d2.loss_mask: 0.6703  mix_decode.d2.loss_dice: 0.9098  mix_decode.d3.loss_cls: 0.2503  mix_decode.d3.loss_mask: 0.6902  mix_decode.d3.loss_dice: 0.9091  mix_decode.d4.loss_cls: 0.2811  mix_decode.d4.loss_mask: 0.6596  mix_decode.d4.loss_dice: 0.9170  mix_decode.d5.loss_cls: 0.2218  mix_decode.d5.loss_mask: 0.7002  mix_decode.d5.loss_dice: 0.9519  mix_decode.d6.loss_cls: 0.2738  mix_decode.d6.loss_mask: 0.6832  mix_decode.d6.loss_dice: 0.9471  mix_decode.d7.loss_cls: 0.3190  mix_decode.d7.loss_mask: 0.6536  mix_decode.d7.loss_dice: 0.9279  mix_decode.d8.loss_cls: 0.2706  mix_decode.d8.loss_mask: 0.6792  mix_decode.d8.loss_dice: 0.8894
2025/03/29 14:36:33 - mmengine - INFO - Iter(train) [ 9650/20000]  base_lr: 5.5276e-05 lr: 5.5276e-05  eta: 2:35:29  time: 1.1530  data_time: 0.0233  memory: 11218  loss: 61.0762  decode.loss_cls: 0.4826  decode.loss_mask: 1.8186  decode.loss_dice: 1.8699  decode.d0.loss_cls: 0.4254  decode.d0.loss_mask: 1.8832  decode.d0.loss_dice: 2.0733  decode.d1.loss_cls: 0.5159  decode.d1.loss_mask: 1.8310  decode.d1.loss_dice: 1.9524  decode.d2.loss_cls: 0.4868  decode.d2.loss_mask: 1.8317  decode.d2.loss_dice: 1.8825  decode.d3.loss_cls: 0.4673  decode.d3.loss_mask: 1.8442  decode.d3.loss_dice: 1.8866  decode.d4.loss_cls: 0.4378  decode.d4.loss_mask: 1.8382  decode.d4.loss_dice: 1.9255  decode.d5.loss_cls: 0.4715  decode.d5.loss_mask: 1.8326  decode.d5.loss_dice: 1.9326  decode.d6.loss_cls: 0.5036  decode.d6.loss_mask: 1.8202  decode.d6.loss_dice: 1.8720  decode.d7.loss_cls: 0.5482  decode.d7.loss_mask: 1.8389  decode.d7.loss_dice: 1.8847  decode.d8.loss_cls: 0.4840  decode.d8.loss_mask: 1.8201  decode.d8.loss_dice: 1.8640  mix_decode.loss_cls: 0.2672  mix_decode.loss_mask: 0.6929  mix_decode.loss_dice: 0.9187  mix_decode.d0.loss_cls: 0.2811  mix_decode.d0.loss_mask: 0.6738  mix_decode.d0.loss_dice: 0.9836  mix_decode.d1.loss_cls: 0.3028  mix_decode.d1.loss_mask: 0.6546  mix_decode.d1.loss_dice: 0.9253  mix_decode.d2.loss_cls: 0.2787  mix_decode.d2.loss_mask: 0.6632  mix_decode.d2.loss_dice: 0.9146  mix_decode.d3.loss_cls: 0.2510  mix_decode.d3.loss_mask: 0.6732  mix_decode.d3.loss_dice: 0.9343  mix_decode.d4.loss_cls: 0.2677  mix_decode.d4.loss_mask: 0.6715  mix_decode.d4.loss_dice: 0.9079  mix_decode.d5.loss_cls: 0.2861  mix_decode.d5.loss_mask: 0.6599  mix_decode.d5.loss_dice: 0.9690  mix_decode.d6.loss_cls: 0.3024  mix_decode.d6.loss_mask: 0.6772  mix_decode.d6.loss_dice: 0.9427  mix_decode.d7.loss_cls: 0.2821  mix_decode.d7.loss_mask: 0.6493  mix_decode.d7.loss_dice: 0.8974  mix_decode.d8.loss_cls: 0.2537  mix_decode.d8.loss_mask: 0.6719  mix_decode.d8.loss_dice: 0.8970
2025/03/29 14:37:31 - mmengine - INFO - Iter(train) [ 9700/20000]  base_lr: 5.5036e-05 lr: 5.5036e-05  eta: 2:34:57  time: 1.1488  data_time: 0.0223  memory: 11216  loss: 57.5194  decode.loss_cls: 0.3994  decode.loss_mask: 1.7165  decode.loss_dice: 1.9106  decode.d0.loss_cls: 0.4968  decode.d0.loss_mask: 1.6898  decode.d0.loss_dice: 1.8987  decode.d1.loss_cls: 0.3402  decode.d1.loss_mask: 1.6827  decode.d1.loss_dice: 1.8981  decode.d2.loss_cls: 0.4378  decode.d2.loss_mask: 1.6855  decode.d2.loss_dice: 1.9400  decode.d3.loss_cls: 0.3897  decode.d3.loss_mask: 1.7198  decode.d3.loss_dice: 1.9302  decode.d4.loss_cls: 0.3651  decode.d4.loss_mask: 1.7235  decode.d4.loss_dice: 1.9048  decode.d5.loss_cls: 0.3639  decode.d5.loss_mask: 1.6964  decode.d5.loss_dice: 1.8892  decode.d6.loss_cls: 0.4470  decode.d6.loss_mask: 1.6330  decode.d6.loss_dice: 1.8947  decode.d7.loss_cls: 0.4224  decode.d7.loss_mask: 1.6748  decode.d7.loss_dice: 1.9229  decode.d8.loss_cls: 0.4109  decode.d8.loss_mask: 1.6936  decode.d8.loss_dice: 1.9080  mix_decode.loss_cls: 0.2538  mix_decode.loss_mask: 0.6135  mix_decode.loss_dice: 0.8375  mix_decode.d0.loss_cls: 0.2713  mix_decode.d0.loss_mask: 0.6052  mix_decode.d0.loss_dice: 0.9045  mix_decode.d1.loss_cls: 0.2565  mix_decode.d1.loss_mask: 0.6192  mix_decode.d1.loss_dice: 0.8365  mix_decode.d2.loss_cls: 0.2779  mix_decode.d2.loss_mask: 0.6000  mix_decode.d2.loss_dice: 0.8405  mix_decode.d3.loss_cls: 0.2888  mix_decode.d3.loss_mask: 0.6064  mix_decode.d3.loss_dice: 0.8268  mix_decode.d4.loss_cls: 0.3111  mix_decode.d4.loss_mask: 0.6153  mix_decode.d4.loss_dice: 0.8324  mix_decode.d5.loss_cls: 0.2778  mix_decode.d5.loss_mask: 0.6372  mix_decode.d5.loss_dice: 0.8583  mix_decode.d6.loss_cls: 0.3077  mix_decode.d6.loss_mask: 0.6250  mix_decode.d6.loss_dice: 0.8592  mix_decode.d7.loss_cls: 0.2769  mix_decode.d7.loss_mask: 0.6229  mix_decode.d7.loss_dice: 0.8551  mix_decode.d8.loss_cls: 0.2640  mix_decode.d8.loss_mask: 0.6110  mix_decode.d8.loss_dice: 0.8413
2025/03/29 14:38:28 - mmengine - INFO - Iter(train) [ 9750/20000]  base_lr: 5.4795e-05 lr: 5.4795e-05  eta: 2:34:25  time: 1.1437  data_time: 0.0224  memory: 11218  loss: 49.3240  decode.loss_cls: 0.1905  decode.loss_mask: 1.7374  decode.loss_dice: 1.4696  decode.d0.loss_cls: 0.3503  decode.d0.loss_mask: 1.7582  decode.d0.loss_dice: 1.4326  decode.d1.loss_cls: 0.2444  decode.d1.loss_mask: 1.7202  decode.d1.loss_dice: 1.4280  decode.d2.loss_cls: 0.2638  decode.d2.loss_mask: 1.7079  decode.d2.loss_dice: 1.4338  decode.d3.loss_cls: 0.2567  decode.d3.loss_mask: 1.7485  decode.d3.loss_dice: 1.4552  decode.d4.loss_cls: 0.2768  decode.d4.loss_mask: 1.6927  decode.d4.loss_dice: 1.4201  decode.d5.loss_cls: 0.3054  decode.d5.loss_mask: 1.7539  decode.d5.loss_dice: 1.4664  decode.d6.loss_cls: 0.2614  decode.d6.loss_mask: 1.7734  decode.d6.loss_dice: 1.4970  decode.d7.loss_cls: 0.2138  decode.d7.loss_mask: 1.7655  decode.d7.loss_dice: 1.4981  decode.d8.loss_cls: 0.1826  decode.d8.loss_mask: 1.7909  decode.d8.loss_dice: 1.5565  mix_decode.loss_cls: 0.1790  mix_decode.loss_mask: 0.5698  mix_decode.loss_dice: 0.7068  mix_decode.d0.loss_cls: 0.2749  mix_decode.d0.loss_mask: 0.5523  mix_decode.d0.loss_dice: 0.7003  mix_decode.d1.loss_cls: 0.2046  mix_decode.d1.loss_mask: 0.5354  mix_decode.d1.loss_dice: 0.6445  mix_decode.d2.loss_cls: 0.2049  mix_decode.d2.loss_mask: 0.5542  mix_decode.d2.loss_dice: 0.6757  mix_decode.d3.loss_cls: 0.2395  mix_decode.d3.loss_mask: 0.5650  mix_decode.d3.loss_dice: 0.6998  mix_decode.d4.loss_cls: 0.2189  mix_decode.d4.loss_mask: 0.5483  mix_decode.d4.loss_dice: 0.6494  mix_decode.d5.loss_cls: 0.2586  mix_decode.d5.loss_mask: 0.5484  mix_decode.d5.loss_dice: 0.6712  mix_decode.d6.loss_cls: 0.2732  mix_decode.d6.loss_mask: 0.5439  mix_decode.d6.loss_dice: 0.6733  mix_decode.d7.loss_cls: 0.2374  mix_decode.d7.loss_mask: 0.5740  mix_decode.d7.loss_dice: 0.6885  mix_decode.d8.loss_cls: 0.2386  mix_decode.d8.loss_mask: 0.5668  mix_decode.d8.loss_dice: 0.6749
2025/03/29 14:39:26 - mmengine - INFO - Iter(train) [ 9800/20000]  base_lr: 5.4555e-05 lr: 5.4555e-05  eta: 2:33:53  time: 1.1497  data_time: 0.0224  memory: 11223  loss: 59.4933  decode.loss_cls: 0.3558  decode.loss_mask: 1.9265  decode.loss_dice: 1.8950  decode.d0.loss_cls: 0.3335  decode.d0.loss_mask: 1.9543  decode.d0.loss_dice: 1.9238  decode.d1.loss_cls: 0.3104  decode.d1.loss_mask: 1.9102  decode.d1.loss_dice: 1.8374  decode.d2.loss_cls: 0.4469  decode.d2.loss_mask: 1.9001  decode.d2.loss_dice: 1.8661  decode.d3.loss_cls: 0.4329  decode.d3.loss_mask: 1.8833  decode.d3.loss_dice: 1.8587  decode.d4.loss_cls: 0.3802  decode.d4.loss_mask: 1.8980  decode.d4.loss_dice: 1.8698  decode.d5.loss_cls: 0.4307  decode.d5.loss_mask: 1.9220  decode.d5.loss_dice: 1.8871  decode.d6.loss_cls: 0.4303  decode.d6.loss_mask: 1.8757  decode.d6.loss_dice: 1.8944  decode.d7.loss_cls: 0.3395  decode.d7.loss_mask: 1.8997  decode.d7.loss_dice: 1.8669  decode.d8.loss_cls: 0.3602  decode.d8.loss_mask: 1.8938  decode.d8.loss_dice: 1.8593  mix_decode.loss_cls: 0.2896  mix_decode.loss_mask: 0.6374  mix_decode.loss_dice: 0.8371  mix_decode.d0.loss_cls: 0.2396  mix_decode.d0.loss_mask: 0.6250  mix_decode.d0.loss_dice: 0.9011  mix_decode.d1.loss_cls: 0.2684  mix_decode.d1.loss_mask: 0.6562  mix_decode.d1.loss_dice: 0.8367  mix_decode.d2.loss_cls: 0.2732  mix_decode.d2.loss_mask: 0.6191  mix_decode.d2.loss_dice: 0.8172  mix_decode.d3.loss_cls: 0.2911  mix_decode.d3.loss_mask: 0.6338  mix_decode.d3.loss_dice: 0.8343  mix_decode.d4.loss_cls: 0.3098  mix_decode.d4.loss_mask: 0.6220  mix_decode.d4.loss_dice: 0.8274  mix_decode.d5.loss_cls: 0.3118  mix_decode.d5.loss_mask: 0.6448  mix_decode.d5.loss_dice: 0.8469  mix_decode.d6.loss_cls: 0.3127  mix_decode.d6.loss_mask: 0.6971  mix_decode.d6.loss_dice: 0.8568  mix_decode.d7.loss_cls: 0.3155  mix_decode.d7.loss_mask: 0.6973  mix_decode.d7.loss_dice: 0.8423  mix_decode.d8.loss_cls: 0.2853  mix_decode.d8.loss_mask: 0.6863  mix_decode.d8.loss_dice: 0.8352
2025/03/29 14:40:24 - mmengine - INFO - Iter(train) [ 9850/20000]  base_lr: 5.4314e-05 lr: 5.4314e-05  eta: 2:33:20  time: 1.1559  data_time: 0.0240  memory: 11214  loss: 63.1325  decode.loss_cls: 0.2455  decode.loss_mask: 2.1898  decode.loss_dice: 1.8757  decode.d0.loss_cls: 0.4542  decode.d0.loss_mask: 2.1205  decode.d0.loss_dice: 1.7999  decode.d1.loss_cls: 0.4127  decode.d1.loss_mask: 2.0899  decode.d1.loss_dice: 1.7894  decode.d2.loss_cls: 0.3406  decode.d2.loss_mask: 2.0958  decode.d2.loss_dice: 1.8079  decode.d3.loss_cls: 0.2954  decode.d3.loss_mask: 2.2029  decode.d3.loss_dice: 1.8995  decode.d4.loss_cls: 0.3139  decode.d4.loss_mask: 2.1334  decode.d4.loss_dice: 1.8788  decode.d5.loss_cls: 0.2947  decode.d5.loss_mask: 2.1855  decode.d5.loss_dice: 1.8699  decode.d6.loss_cls: 0.3131  decode.d6.loss_mask: 2.1739  decode.d6.loss_dice: 1.8850  decode.d7.loss_cls: 0.2918  decode.d7.loss_mask: 2.1450  decode.d7.loss_dice: 1.8612  decode.d8.loss_cls: 0.3082  decode.d8.loss_mask: 2.1566  decode.d8.loss_dice: 1.8821  mix_decode.loss_cls: 0.2708  mix_decode.loss_mask: 0.8348  mix_decode.loss_dice: 0.8947  mix_decode.d0.loss_cls: 0.2423  mix_decode.d0.loss_mask: 0.8480  mix_decode.d0.loss_dice: 0.9295  mix_decode.d1.loss_cls: 0.2267  mix_decode.d1.loss_mask: 0.8047  mix_decode.d1.loss_dice: 0.8795  mix_decode.d2.loss_cls: 0.2438  mix_decode.d2.loss_mask: 0.8461  mix_decode.d2.loss_dice: 0.9098  mix_decode.d3.loss_cls: 0.2770  mix_decode.d3.loss_mask: 0.8075  mix_decode.d3.loss_dice: 0.9101  mix_decode.d4.loss_cls: 0.2766  mix_decode.d4.loss_mask: 0.8392  mix_decode.d4.loss_dice: 0.8989  mix_decode.d5.loss_cls: 0.2393  mix_decode.d5.loss_mask: 0.8592  mix_decode.d5.loss_dice: 0.9259  mix_decode.d6.loss_cls: 0.2187  mix_decode.d6.loss_mask: 0.8397  mix_decode.d6.loss_dice: 0.8997  mix_decode.d7.loss_cls: 0.2169  mix_decode.d7.loss_mask: 0.8378  mix_decode.d7.loss_dice: 0.8935  mix_decode.d8.loss_cls: 0.2185  mix_decode.d8.loss_mask: 0.8459  mix_decode.d8.loss_dice: 0.8845
2025/03/29 14:41:21 - mmengine - INFO - Iter(train) [ 9900/20000]  base_lr: 5.4073e-05 lr: 5.4073e-05  eta: 2:32:47  time: 1.1503  data_time: 0.0223  memory: 11214  loss: 57.2543  decode.loss_cls: 0.3884  decode.loss_mask: 1.9255  decode.loss_dice: 1.6527  decode.d0.loss_cls: 0.3594  decode.d0.loss_mask: 2.0243  decode.d0.loss_dice: 1.7567  decode.d1.loss_cls: 0.4520  decode.d1.loss_mask: 1.9026  decode.d1.loss_dice: 1.6138  decode.d2.loss_cls: 0.4624  decode.d2.loss_mask: 1.9335  decode.d2.loss_dice: 1.6197  decode.d3.loss_cls: 0.4635  decode.d3.loss_mask: 1.9425  decode.d3.loss_dice: 1.6567  decode.d4.loss_cls: 0.3358  decode.d4.loss_mask: 1.9672  decode.d4.loss_dice: 1.6710  decode.d5.loss_cls: 0.4860  decode.d5.loss_mask: 1.9016  decode.d5.loss_dice: 1.6146  decode.d6.loss_cls: 0.4410  decode.d6.loss_mask: 1.9289  decode.d6.loss_dice: 1.6417  decode.d7.loss_cls: 0.3614  decode.d7.loss_mask: 1.9178  decode.d7.loss_dice: 1.6875  decode.d8.loss_cls: 0.3782  decode.d8.loss_mask: 1.9509  decode.d8.loss_dice: 1.6560  mix_decode.loss_cls: 0.2153  mix_decode.loss_mask: 0.7154  mix_decode.loss_dice: 0.7702  mix_decode.d0.loss_cls: 0.2824  mix_decode.d0.loss_mask: 0.7073  mix_decode.d0.loss_dice: 0.7756  mix_decode.d1.loss_cls: 0.2545  mix_decode.d1.loss_mask: 0.6990  mix_decode.d1.loss_dice: 0.7350  mix_decode.d2.loss_cls: 0.2624  mix_decode.d2.loss_mask: 0.7029  mix_decode.d2.loss_dice: 0.7209  mix_decode.d3.loss_cls: 0.2410  mix_decode.d3.loss_mask: 0.6994  mix_decode.d3.loss_dice: 0.7538  mix_decode.d4.loss_cls: 0.2369  mix_decode.d4.loss_mask: 0.7232  mix_decode.d4.loss_dice: 0.7743  mix_decode.d5.loss_cls: 0.2360  mix_decode.d5.loss_mask: 0.7348  mix_decode.d5.loss_dice: 0.7542  mix_decode.d6.loss_cls: 0.2379  mix_decode.d6.loss_mask: 0.7105  mix_decode.d6.loss_dice: 0.7838  mix_decode.d7.loss_cls: 0.2476  mix_decode.d7.loss_mask: 0.7190  mix_decode.d7.loss_dice: 0.7500  mix_decode.d8.loss_cls: 0.2549  mix_decode.d8.loss_mask: 0.7012  mix_decode.d8.loss_dice: 0.7617
2025/03/29 14:42:19 - mmengine - INFO - Iter(train) [ 9950/20000]  base_lr: 5.3832e-05 lr: 5.3832e-05  eta: 2:32:14  time: 1.1563  data_time: 0.0226  memory: 11213  loss: 58.1246  decode.loss_cls: 0.2889  decode.loss_mask: 1.9677  decode.loss_dice: 1.7924  decode.d0.loss_cls: 0.3462  decode.d0.loss_mask: 1.9738  decode.d0.loss_dice: 1.8019  decode.d1.loss_cls: 0.3121  decode.d1.loss_mask: 1.9227  decode.d1.loss_dice: 1.7764  decode.d2.loss_cls: 0.2498  decode.d2.loss_mask: 1.9475  decode.d2.loss_dice: 1.7975  decode.d3.loss_cls: 0.2479  decode.d3.loss_mask: 1.9743  decode.d3.loss_dice: 1.7884  decode.d4.loss_cls: 0.2746  decode.d4.loss_mask: 1.9804  decode.d4.loss_dice: 1.8225  decode.d5.loss_cls: 0.3277  decode.d5.loss_mask: 1.9633  decode.d5.loss_dice: 1.7789  decode.d6.loss_cls: 0.2847  decode.d6.loss_mask: 1.9479  decode.d6.loss_dice: 1.7771  decode.d7.loss_cls: 0.3036  decode.d7.loss_mask: 1.9634  decode.d7.loss_dice: 1.7968  decode.d8.loss_cls: 0.2896  decode.d8.loss_mask: 1.9587  decode.d8.loss_dice: 1.8055  mix_decode.loss_cls: 0.3069  mix_decode.loss_mask: 0.6959  mix_decode.loss_dice: 0.7801  mix_decode.d0.loss_cls: 0.2257  mix_decode.d0.loss_mask: 0.7008  mix_decode.d0.loss_dice: 0.8621  mix_decode.d1.loss_cls: 0.3150  mix_decode.d1.loss_mask: 0.6617  mix_decode.d1.loss_dice: 0.7669  mix_decode.d2.loss_cls: 0.2779  mix_decode.d2.loss_mask: 0.6888  mix_decode.d2.loss_dice: 0.7933  mix_decode.d3.loss_cls: 0.2864  mix_decode.d3.loss_mask: 0.6851  mix_decode.d3.loss_dice: 0.7823  mix_decode.d4.loss_cls: 0.3038  mix_decode.d4.loss_mask: 0.6609  mix_decode.d4.loss_dice: 0.7917  mix_decode.d5.loss_cls: 0.2436  mix_decode.d5.loss_mask: 0.7077  mix_decode.d5.loss_dice: 0.7934  mix_decode.d6.loss_cls: 0.3037  mix_decode.d6.loss_mask: 0.7026  mix_decode.d6.loss_dice: 0.7811  mix_decode.d7.loss_cls: 0.3628  mix_decode.d7.loss_mask: 0.6620  mix_decode.d7.loss_dice: 0.7526  mix_decode.d8.loss_cls: 0.3288  mix_decode.d8.loss_mask: 0.6845  mix_decode.d8.loss_dice: 0.7544
2025/03/29 14:43:16 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 14:43:16 - mmengine - INFO - Iter(train) [10000/20000]  base_lr: 5.3591e-05 lr: 5.3591e-05  eta: 2:31:41  time: 1.1553  data_time: 0.0229  memory: 11217  loss: 61.8385  decode.loss_cls: 0.4455  decode.loss_mask: 1.9469  decode.loss_dice: 1.9750  decode.d0.loss_cls: 0.2837  decode.d0.loss_mask: 2.0441  decode.d0.loss_dice: 2.0769  decode.d1.loss_cls: 0.3046  decode.d1.loss_mask: 1.9975  decode.d1.loss_dice: 2.0445  decode.d2.loss_cls: 0.3163  decode.d2.loss_mask: 1.9761  decode.d2.loss_dice: 2.0054  decode.d3.loss_cls: 0.3312  decode.d3.loss_mask: 1.9847  decode.d3.loss_dice: 2.0062  decode.d4.loss_cls: 0.3532  decode.d4.loss_mask: 1.9293  decode.d4.loss_dice: 1.9684  decode.d5.loss_cls: 0.4091  decode.d5.loss_mask: 2.0059  decode.d5.loss_dice: 2.0053  decode.d6.loss_cls: 0.3847  decode.d6.loss_mask: 1.9636  decode.d6.loss_dice: 1.9719  decode.d7.loss_cls: 0.3384  decode.d7.loss_mask: 1.9486  decode.d7.loss_dice: 1.9753  decode.d8.loss_cls: 0.3374  decode.d8.loss_mask: 1.9489  decode.d8.loss_dice: 2.0162  mix_decode.loss_cls: 0.3315  mix_decode.loss_mask: 0.6534  mix_decode.loss_dice: 0.8335  mix_decode.d0.loss_cls: 0.3618  mix_decode.d0.loss_mask: 0.6265  mix_decode.d0.loss_dice: 0.9250  mix_decode.d1.loss_cls: 0.3306  mix_decode.d1.loss_mask: 0.6156  mix_decode.d1.loss_dice: 0.8620  mix_decode.d2.loss_cls: 0.3534  mix_decode.d2.loss_mask: 0.6383  mix_decode.d2.loss_dice: 0.8306  mix_decode.d3.loss_cls: 0.3039  mix_decode.d3.loss_mask: 0.6636  mix_decode.d3.loss_dice: 0.8603  mix_decode.d4.loss_cls: 0.3537  mix_decode.d4.loss_mask: 0.6680  mix_decode.d4.loss_dice: 0.8441  mix_decode.d5.loss_cls: 0.3936  mix_decode.d5.loss_mask: 0.6523  mix_decode.d5.loss_dice: 0.8565  mix_decode.d6.loss_cls: 0.3637  mix_decode.d6.loss_mask: 0.6341  mix_decode.d6.loss_dice: 0.8777  mix_decode.d7.loss_cls: 0.3609  mix_decode.d7.loss_mask: 0.6280  mix_decode.d7.loss_dice: 0.8442  mix_decode.d8.loss_cls: 0.3719  mix_decode.d8.loss_mask: 0.6467  mix_decode.d8.loss_dice: 0.8583
2025/03/29 14:43:16 - mmengine - INFO - Saving checkpoint at 10000 iterations
2025/03/29 14:43:22 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:06:01  time: 0.0914  data_time: 0.0017  memory: 3083  
2025/03/29 14:43:27 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:55  time: 0.0916  data_time: 0.0017  memory: 3083  
2025/03/29 14:43:31 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:50  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:43:36 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:45  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:43:40 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:41  time: 0.0912  data_time: 0.0018  memory: 3083  
2025/03/29 14:43:45 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:36  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 14:43:50 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:32  time: 0.0919  data_time: 0.0019  memory: 3083  
2025/03/29 14:43:54 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:27  time: 0.0910  data_time: 0.0017  memory: 3083  
2025/03/29 14:43:59 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:22  time: 0.0912  data_time: 0.0017  memory: 3083  
2025/03/29 14:44:03 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:18  time: 0.0913  data_time: 0.0017  memory: 3083  
2025/03/29 14:44:08 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:13  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:13 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:08  time: 0.0936  data_time: 0.0020  memory: 3083  
2025/03/29 14:44:17 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:04  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:22 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:59  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:26 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:55  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:31 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:50  time: 0.0913  data_time: 0.0017  memory: 3083  
2025/03/29 14:44:35 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:45  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:40 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:41  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:45 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:36  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:49 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:32  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:54 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:27  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:44:58 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:22  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:03 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:18  time: 0.0913  data_time: 0.0017  memory: 3083  
2025/03/29 14:45:08 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:13  time: 0.0914  data_time: 0.0017  memory: 3083  
2025/03/29 14:45:12 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:09  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:17 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:04  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:21 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:59  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:26 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:55  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:31 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:35 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:46  time: 0.0919  data_time: 0.0019  memory: 3083  
2025/03/29 14:45:40 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0918  data_time: 0.0017  memory: 3083  
2025/03/29 14:45:44 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:37  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:45:49 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0917  data_time: 0.0017  memory: 3083  
2025/03/29 14:45:54 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:28  time: 0.0917  data_time: 0.0017  memory: 3083  
2025/03/29 14:45:58 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:23  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:03 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:07 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:14  time: 0.0920  data_time: 0.0019  memory: 3083  
2025/03/29 14:46:12 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0920  data_time: 0.0019  memory: 3083  
2025/03/29 14:46:17 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:05  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:21 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0919  data_time: 0.0017  memory: 3083  
2025/03/29 14:46:26 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0917  data_time: 0.0017  memory: 3083  
2025/03/29 14:46:30 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:51  time: 0.0920  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:35 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0937  data_time: 0.0020  memory: 3083  
2025/03/29 14:46:40 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:42  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:44 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0917  data_time: 0.0019  memory: 3083  
2025/03/29 14:46:49 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:33  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:53 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:46:58 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:03 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:19  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:07 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0934  data_time: 0.0019  memory: 3083  
2025/03/29 14:47:12 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:10  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:16 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:21 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0917  data_time: 0.0019  memory: 3083  
2025/03/29 14:47:26 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:30 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:35 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:47  time: 0.0921  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:39 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:44 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:38  time: 0.0923  data_time: 0.0019  memory: 3083  
2025/03/29 14:47:49 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:53 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:47:58 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0920  data_time: 0.0017  memory: 3083  
2025/03/29 14:48:02 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:07 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:15  time: 0.0920  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:12 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:16 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0915  data_time: 0.0017  memory: 3083  
2025/03/29 14:48:21 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:26 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:30 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:35 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:39 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0922  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:44 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0922  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:49 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0920  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:53 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0921  data_time: 0.0018  memory: 3083  
2025/03/29 14:48:58 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0921  data_time: 0.0018  memory: 3083  
2025/03/29 14:49:02 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:20  time: 0.0921  data_time: 0.0018  memory: 3083  
2025/03/29 14:49:07 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0920  data_time: 0.0018  memory: 3083  
2025/03/29 14:49:12 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0920  data_time: 0.0018  memory: 3083  
2025/03/29 14:49:16 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0927  data_time: 0.0019  memory: 3083  
2025/03/29 14:49:21 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0916  data_time: 0.0017  memory: 3083  
2025/03/29 14:49:23 - mmengine - INFO - per class results:
2025/03/29 14:49:23 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 49.95 | 63.08 |
|   building   | 48.34 | 67.83 |
|     road     | 45.35 | 50.19 |
|    water     | 66.62 | 78.12 |
|    barren    | 13.67 | 38.26 |
|    forest    | 39.32 | 51.89 |
| agricultural | 63.32 |  80.5 |
+--------------+-------+-------+
2025/03/29 14:49:23 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 68.5200  mIoU: 46.6500  mAcc: 61.4100  data_time: 0.0018  time: 0.0919
2025/03/29 14:49:23 - mmengine - INFO - The previous best checkpoint /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0/best_mIoU_iter_8000.pth is removed
2025/03/29 14:49:23 - mmengine - INFO - The best checkpoint with 46.6500 mIoU at 10000 iter is saved to best_mIoU_iter_10000.pth.
2025/03/29 14:50:22 - mmengine - INFO - Iter(train) [10050/20000]  base_lr: 5.3350e-05 lr: 5.3350e-05  eta: 2:31:10  time: 1.1516  data_time: 0.0229  memory: 11215  loss: 55.6083  decode.loss_cls: 0.3492  decode.loss_mask: 1.7223  decode.loss_dice: 1.7945  decode.d0.loss_cls: 0.4747  decode.d0.loss_mask: 1.7623  decode.d0.loss_dice: 1.8687  decode.d1.loss_cls: 0.3331  decode.d1.loss_mask: 1.7552  decode.d1.loss_dice: 1.8104  decode.d2.loss_cls: 0.3463  decode.d2.loss_mask: 1.7861  decode.d2.loss_dice: 1.7810  decode.d3.loss_cls: 0.4120  decode.d3.loss_mask: 1.7531  decode.d3.loss_dice: 1.7597  decode.d4.loss_cls: 0.3188  decode.d4.loss_mask: 1.7553  decode.d4.loss_dice: 1.7944  decode.d5.loss_cls: 0.3739  decode.d5.loss_mask: 1.7548  decode.d5.loss_dice: 1.7924  decode.d6.loss_cls: 0.3971  decode.d6.loss_mask: 1.7872  decode.d6.loss_dice: 1.7922  decode.d7.loss_cls: 0.3653  decode.d7.loss_mask: 1.7345  decode.d7.loss_dice: 1.7696  decode.d8.loss_cls: 0.3442  decode.d8.loss_mask: 1.7507  decode.d8.loss_dice: 1.7986  mix_decode.loss_cls: 0.2989  mix_decode.loss_mask: 0.6326  mix_decode.loss_dice: 0.7324  mix_decode.d0.loss_cls: 0.2634  mix_decode.d0.loss_mask: 0.6334  mix_decode.d0.loss_dice: 0.7920  mix_decode.d1.loss_cls: 0.2674  mix_decode.d1.loss_mask: 0.6075  mix_decode.d1.loss_dice: 0.7314  mix_decode.d2.loss_cls: 0.3101  mix_decode.d2.loss_mask: 0.6026  mix_decode.d2.loss_dice: 0.7232  mix_decode.d3.loss_cls: 0.2795  mix_decode.d3.loss_mask: 0.6256  mix_decode.d3.loss_dice: 0.7210  mix_decode.d4.loss_cls: 0.2658  mix_decode.d4.loss_mask: 0.6276  mix_decode.d4.loss_dice: 0.7491  mix_decode.d5.loss_cls: 0.2629  mix_decode.d5.loss_mask: 0.6069  mix_decode.d5.loss_dice: 0.7426  mix_decode.d6.loss_cls: 0.2877  mix_decode.d6.loss_mask: 0.6257  mix_decode.d6.loss_dice: 0.7390  mix_decode.d7.loss_cls: 0.2666  mix_decode.d7.loss_mask: 0.6235  mix_decode.d7.loss_dice: 0.7199  mix_decode.d8.loss_cls: 0.2506  mix_decode.d8.loss_mask: 0.6231  mix_decode.d8.loss_dice: 0.7585
2025/03/29 14:51:20 - mmengine - INFO - Iter(train) [10100/20000]  base_lr: 5.3109e-05 lr: 5.3109e-05  eta: 2:30:36  time: 1.1521  data_time: 0.0226  memory: 11224  loss: 55.6935  decode.loss_cls: 0.2809  decode.loss_mask: 1.8525  decode.loss_dice: 1.7459  decode.d0.loss_cls: 0.3706  decode.d0.loss_mask: 1.9478  decode.d0.loss_dice: 1.7762  decode.d1.loss_cls: 0.1983  decode.d1.loss_mask: 1.9012  decode.d1.loss_dice: 1.7543  decode.d2.loss_cls: 0.2758  decode.d2.loss_mask: 1.8586  decode.d2.loss_dice: 1.7186  decode.d3.loss_cls: 0.2058  decode.d3.loss_mask: 1.8712  decode.d3.loss_dice: 1.7355  decode.d4.loss_cls: 0.2345  decode.d4.loss_mask: 1.8584  decode.d4.loss_dice: 1.7234  decode.d5.loss_cls: 0.1623  decode.d5.loss_mask: 1.9757  decode.d5.loss_dice: 1.7722  decode.d6.loss_cls: 0.2210  decode.d6.loss_mask: 1.9596  decode.d6.loss_dice: 1.7488  decode.d7.loss_cls: 0.2310  decode.d7.loss_mask: 1.9519  decode.d7.loss_dice: 1.7166  decode.d8.loss_cls: 0.2482  decode.d8.loss_mask: 1.9092  decode.d8.loss_dice: 1.7129  mix_decode.loss_cls: 0.2069  mix_decode.loss_mask: 0.6549  mix_decode.loss_dice: 0.7948  mix_decode.d0.loss_cls: 0.2324  mix_decode.d0.loss_mask: 0.6191  mix_decode.d0.loss_dice: 0.8176  mix_decode.d1.loss_cls: 0.2476  mix_decode.d1.loss_mask: 0.6553  mix_decode.d1.loss_dice: 0.8147  mix_decode.d2.loss_cls: 0.2189  mix_decode.d2.loss_mask: 0.6521  mix_decode.d2.loss_dice: 0.8016  mix_decode.d3.loss_cls: 0.2291  mix_decode.d3.loss_mask: 0.6259  mix_decode.d3.loss_dice: 0.7372  mix_decode.d4.loss_cls: 0.2567  mix_decode.d4.loss_mask: 0.6352  mix_decode.d4.loss_dice: 0.7552  mix_decode.d5.loss_cls: 0.2853  mix_decode.d5.loss_mask: 0.6241  mix_decode.d5.loss_dice: 0.7755  mix_decode.d6.loss_cls: 0.3024  mix_decode.d6.loss_mask: 0.6460  mix_decode.d6.loss_dice: 0.7861  mix_decode.d7.loss_cls: 0.3314  mix_decode.d7.loss_mask: 0.6156  mix_decode.d7.loss_dice: 0.7722  mix_decode.d8.loss_cls: 0.2606  mix_decode.d8.loss_mask: 0.6281  mix_decode.d8.loss_dice: 0.7921
2025/03/29 14:52:17 - mmengine - INFO - Iter(train) [10150/20000]  base_lr: 5.2867e-05 lr: 5.2867e-05  eta: 2:30:02  time: 1.1477  data_time: 0.0229  memory: 11213  loss: 60.3482  decode.loss_cls: 0.4661  decode.loss_mask: 2.0671  decode.loss_dice: 1.8228  decode.d0.loss_cls: 0.5006  decode.d0.loss_mask: 2.1516  decode.d0.loss_dice: 1.9722  decode.d1.loss_cls: 0.5284  decode.d1.loss_mask: 2.0627  decode.d1.loss_dice: 1.8354  decode.d2.loss_cls: 0.5360  decode.d2.loss_mask: 2.0459  decode.d2.loss_dice: 1.8219  decode.d3.loss_cls: 0.4851  decode.d3.loss_mask: 2.0782  decode.d3.loss_dice: 1.8562  decode.d4.loss_cls: 0.4262  decode.d4.loss_mask: 2.0933  decode.d4.loss_dice: 1.8627  decode.d5.loss_cls: 0.4806  decode.d5.loss_mask: 2.0798  decode.d5.loss_dice: 1.8200  decode.d6.loss_cls: 0.4693  decode.d6.loss_mask: 2.0737  decode.d6.loss_dice: 1.8185  decode.d7.loss_cls: 0.4324  decode.d7.loss_mask: 2.1152  decode.d7.loss_dice: 1.8815  decode.d8.loss_cls: 0.4388  decode.d8.loss_mask: 2.1004  decode.d8.loss_dice: 1.8455  mix_decode.loss_cls: 0.2605  mix_decode.loss_mask: 0.6038  mix_decode.loss_dice: 0.7156  mix_decode.d0.loss_cls: 0.3093  mix_decode.d0.loss_mask: 0.6105  mix_decode.d0.loss_dice: 0.7803  mix_decode.d1.loss_cls: 0.2609  mix_decode.d1.loss_mask: 0.6161  mix_decode.d1.loss_dice: 0.7240  mix_decode.d2.loss_cls: 0.2724  mix_decode.d2.loss_mask: 0.6361  mix_decode.d2.loss_dice: 0.7180  mix_decode.d3.loss_cls: 0.2811  mix_decode.d3.loss_mask: 0.6078  mix_decode.d3.loss_dice: 0.7033  mix_decode.d4.loss_cls: 0.2474  mix_decode.d4.loss_mask: 0.6333  mix_decode.d4.loss_dice: 0.7107  mix_decode.d5.loss_cls: 0.2891  mix_decode.d5.loss_mask: 0.6103  mix_decode.d5.loss_dice: 0.7047  mix_decode.d6.loss_cls: 0.2686  mix_decode.d6.loss_mask: 0.6393  mix_decode.d6.loss_dice: 0.7373  mix_decode.d7.loss_cls: 0.2854  mix_decode.d7.loss_mask: 0.6351  mix_decode.d7.loss_dice: 0.7005  mix_decode.d8.loss_cls: 0.2517  mix_decode.d8.loss_mask: 0.6286  mix_decode.d8.loss_dice: 0.7386
2025/03/29 14:53:15 - mmengine - INFO - Iter(train) [10200/20000]  base_lr: 5.2625e-05 lr: 5.2625e-05  eta: 2:29:27  time: 1.1513  data_time: 0.0226  memory: 11220  loss: 65.0853  decode.loss_cls: 0.5657  decode.loss_mask: 2.1019  decode.loss_dice: 1.9351  decode.d0.loss_cls: 0.6884  decode.d0.loss_mask: 2.0330  decode.d0.loss_dice: 1.9921  decode.d1.loss_cls: 0.6506  decode.d1.loss_mask: 2.0510  decode.d1.loss_dice: 1.9700  decode.d2.loss_cls: 0.6312  decode.d2.loss_mask: 2.0555  decode.d2.loss_dice: 1.9556  decode.d3.loss_cls: 0.5967  decode.d3.loss_mask: 2.0863  decode.d3.loss_dice: 1.9848  decode.d4.loss_cls: 0.6097  decode.d4.loss_mask: 2.0646  decode.d4.loss_dice: 1.9448  decode.d5.loss_cls: 0.6197  decode.d5.loss_mask: 2.0772  decode.d5.loss_dice: 1.9846  decode.d6.loss_cls: 0.6115  decode.d6.loss_mask: 2.1067  decode.d6.loss_dice: 1.9972  decode.d7.loss_cls: 0.6594  decode.d7.loss_mask: 2.0876  decode.d7.loss_dice: 1.9385  decode.d8.loss_cls: 0.5952  decode.d8.loss_mask: 2.1215  decode.d8.loss_dice: 1.9984  mix_decode.loss_cls: 0.3000  mix_decode.loss_mask: 0.7727  mix_decode.loss_dice: 0.7472  mix_decode.d0.loss_cls: 0.3491  mix_decode.d0.loss_mask: 0.7581  mix_decode.d0.loss_dice: 0.7771  mix_decode.d1.loss_cls: 0.3562  mix_decode.d1.loss_mask: 0.7524  mix_decode.d1.loss_dice: 0.7446  mix_decode.d2.loss_cls: 0.3303  mix_decode.d2.loss_mask: 0.7724  mix_decode.d2.loss_dice: 0.7644  mix_decode.d3.loss_cls: 0.2759  mix_decode.d3.loss_mask: 0.7626  mix_decode.d3.loss_dice: 0.7544  mix_decode.d4.loss_cls: 0.3404  mix_decode.d4.loss_mask: 0.7747  mix_decode.d4.loss_dice: 0.7368  mix_decode.d5.loss_cls: 0.3083  mix_decode.d5.loss_mask: 0.7574  mix_decode.d5.loss_dice: 0.7222  mix_decode.d6.loss_cls: 0.2999  mix_decode.d6.loss_mask: 0.7723  mix_decode.d6.loss_dice: 0.7278  mix_decode.d7.loss_cls: 0.3477  mix_decode.d7.loss_mask: 0.7692  mix_decode.d7.loss_dice: 0.7596  mix_decode.d8.loss_cls: 0.2574  mix_decode.d8.loss_mask: 0.7830  mix_decode.d8.loss_dice: 0.7968
2025/03/29 14:54:12 - mmengine - INFO - Iter(train) [10250/20000]  base_lr: 5.2384e-05 lr: 5.2384e-05  eta: 2:28:53  time: 1.1494  data_time: 0.0227  memory: 11212  loss: 56.2071  decode.loss_cls: 0.3465  decode.loss_mask: 1.9047  decode.loss_dice: 1.8432  decode.d0.loss_cls: 0.3681  decode.d0.loss_mask: 1.9201  decode.d0.loss_dice: 1.8788  decode.d1.loss_cls: 0.3543  decode.d1.loss_mask: 1.9129  decode.d1.loss_dice: 1.8741  decode.d2.loss_cls: 0.3086  decode.d2.loss_mask: 1.9051  decode.d2.loss_dice: 1.8610  decode.d3.loss_cls: 0.2937  decode.d3.loss_mask: 1.9142  decode.d3.loss_dice: 1.8829  decode.d4.loss_cls: 0.2833  decode.d4.loss_mask: 1.9342  decode.d4.loss_dice: 1.8678  decode.d5.loss_cls: 0.2728  decode.d5.loss_mask: 1.9353  decode.d5.loss_dice: 1.9235  decode.d6.loss_cls: 0.2767  decode.d6.loss_mask: 1.9192  decode.d6.loss_dice: 1.8924  decode.d7.loss_cls: 0.3189  decode.d7.loss_mask: 1.9022  decode.d7.loss_dice: 1.8594  decode.d8.loss_cls: 0.3114  decode.d8.loss_mask: 1.9385  decode.d8.loss_dice: 1.9106  mix_decode.loss_cls: 0.2284  mix_decode.loss_mask: 0.6028  mix_decode.loss_dice: 0.7083  mix_decode.d0.loss_cls: 0.2275  mix_decode.d0.loss_mask: 0.5961  mix_decode.d0.loss_dice: 0.7026  mix_decode.d1.loss_cls: 0.2096  mix_decode.d1.loss_mask: 0.5943  mix_decode.d1.loss_dice: 0.6747  mix_decode.d2.loss_cls: 0.2133  mix_decode.d2.loss_mask: 0.5946  mix_decode.d2.loss_dice: 0.6632  mix_decode.d3.loss_cls: 0.2129  mix_decode.d3.loss_mask: 0.6033  mix_decode.d3.loss_dice: 0.6816  mix_decode.d4.loss_cls: 0.2199  mix_decode.d4.loss_mask: 0.6248  mix_decode.d4.loss_dice: 0.6706  mix_decode.d5.loss_cls: 0.2246  mix_decode.d5.loss_mask: 0.6001  mix_decode.d5.loss_dice: 0.6931  mix_decode.d6.loss_cls: 0.2229  mix_decode.d6.loss_mask: 0.5817  mix_decode.d6.loss_dice: 0.6813  mix_decode.d7.loss_cls: 0.2423  mix_decode.d7.loss_mask: 0.6187  mix_decode.d7.loss_dice: 0.6939  mix_decode.d8.loss_cls: 0.1844  mix_decode.d8.loss_mask: 0.6190  mix_decode.d8.loss_dice: 0.7020
2025/03/29 14:55:10 - mmengine - INFO - Iter(train) [10300/20000]  base_lr: 5.2142e-05 lr: 5.2142e-05  eta: 2:28:18  time: 1.1484  data_time: 0.0226  memory: 11219  loss: 64.1216  decode.loss_cls: 0.4461  decode.loss_mask: 2.2256  decode.loss_dice: 1.9539  decode.d0.loss_cls: 0.4374  decode.d0.loss_mask: 2.2771  decode.d0.loss_dice: 1.9740  decode.d1.loss_cls: 0.5186  decode.d1.loss_mask: 2.1573  decode.d1.loss_dice: 1.9247  decode.d2.loss_cls: 0.4709  decode.d2.loss_mask: 2.2119  decode.d2.loss_dice: 1.9477  decode.d3.loss_cls: 0.3747  decode.d3.loss_mask: 2.2794  decode.d3.loss_dice: 2.0077  decode.d4.loss_cls: 0.4378  decode.d4.loss_mask: 2.2085  decode.d4.loss_dice: 1.9666  decode.d5.loss_cls: 0.4039  decode.d5.loss_mask: 2.2630  decode.d5.loss_dice: 2.0039  decode.d6.loss_cls: 0.3783  decode.d6.loss_mask: 2.2661  decode.d6.loss_dice: 1.9921  decode.d7.loss_cls: 0.3785  decode.d7.loss_mask: 2.2911  decode.d7.loss_dice: 1.9688  decode.d8.loss_cls: 0.3945  decode.d8.loss_mask: 2.2408  decode.d8.loss_dice: 1.9676  mix_decode.loss_cls: 0.2538  mix_decode.loss_mask: 0.6840  mix_decode.loss_dice: 0.7723  mix_decode.d0.loss_cls: 0.3186  mix_decode.d0.loss_mask: 0.6774  mix_decode.d0.loss_dice: 0.8472  mix_decode.d1.loss_cls: 0.2739  mix_decode.d1.loss_mask: 0.6689  mix_decode.d1.loss_dice: 0.8076  mix_decode.d2.loss_cls: 0.2799  mix_decode.d2.loss_mask: 0.6818  mix_decode.d2.loss_dice: 0.7773  mix_decode.d3.loss_cls: 0.2825  mix_decode.d3.loss_mask: 0.6672  mix_decode.d3.loss_dice: 0.7794  mix_decode.d4.loss_cls: 0.3154  mix_decode.d4.loss_mask: 0.6506  mix_decode.d4.loss_dice: 0.7870  mix_decode.d5.loss_cls: 0.3259  mix_decode.d5.loss_mask: 0.6780  mix_decode.d5.loss_dice: 0.8142  mix_decode.d6.loss_cls: 0.3245  mix_decode.d6.loss_mask: 0.6714  mix_decode.d6.loss_dice: 0.8130  mix_decode.d7.loss_cls: 0.3243  mix_decode.d7.loss_mask: 0.6652  mix_decode.d7.loss_dice: 0.7951  mix_decode.d8.loss_cls: 0.3387  mix_decode.d8.loss_mask: 0.6884  mix_decode.d8.loss_dice: 0.7893
2025/03/29 14:56:07 - mmengine - INFO - Iter(train) [10350/20000]  base_lr: 5.1900e-05 lr: 5.1900e-05  eta: 2:27:43  time: 1.1446  data_time: 0.0223  memory: 11217  loss: 54.3866  decode.loss_cls: 0.1874  decode.loss_mask: 1.8899  decode.loss_dice: 1.7272  decode.d0.loss_cls: 0.3470  decode.d0.loss_mask: 1.9274  decode.d0.loss_dice: 1.7092  decode.d1.loss_cls: 0.1756  decode.d1.loss_mask: 1.9399  decode.d1.loss_dice: 1.7476  decode.d2.loss_cls: 0.1526  decode.d2.loss_mask: 1.9579  decode.d2.loss_dice: 1.7300  decode.d3.loss_cls: 0.1759  decode.d3.loss_mask: 1.9273  decode.d3.loss_dice: 1.7183  decode.d4.loss_cls: 0.1921  decode.d4.loss_mask: 1.9162  decode.d4.loss_dice: 1.7175  decode.d5.loss_cls: 0.1646  decode.d5.loss_mask: 1.9507  decode.d5.loss_dice: 1.7510  decode.d6.loss_cls: 0.1740  decode.d6.loss_mask: 1.9386  decode.d6.loss_dice: 1.7392  decode.d7.loss_cls: 0.1954  decode.d7.loss_mask: 1.8981  decode.d7.loss_dice: 1.7341  decode.d8.loss_cls: 0.1993  decode.d8.loss_mask: 1.9144  decode.d8.loss_dice: 1.7066  mix_decode.loss_cls: 0.2063  mix_decode.loss_mask: 0.5938  mix_decode.loss_dice: 0.7580  mix_decode.d0.loss_cls: 0.2345  mix_decode.d0.loss_mask: 0.5954  mix_decode.d0.loss_dice: 0.8398  mix_decode.d1.loss_cls: 0.2433  mix_decode.d1.loss_mask: 0.5668  mix_decode.d1.loss_dice: 0.7390  mix_decode.d2.loss_cls: 0.2236  mix_decode.d2.loss_mask: 0.5992  mix_decode.d2.loss_dice: 0.7561  mix_decode.d3.loss_cls: 0.2172  mix_decode.d3.loss_mask: 0.5814  mix_decode.d3.loss_dice: 0.7555  mix_decode.d4.loss_cls: 0.2382  mix_decode.d4.loss_mask: 0.6024  mix_decode.d4.loss_dice: 0.7778  mix_decode.d5.loss_cls: 0.2706  mix_decode.d5.loss_mask: 0.6038  mix_decode.d5.loss_dice: 0.7708  mix_decode.d6.loss_cls: 0.2291  mix_decode.d6.loss_mask: 0.5642  mix_decode.d6.loss_dice: 0.8053  mix_decode.d7.loss_cls: 0.2213  mix_decode.d7.loss_mask: 0.5771  mix_decode.d7.loss_dice: 0.7831  mix_decode.d8.loss_cls: 0.2036  mix_decode.d8.loss_mask: 0.5592  mix_decode.d8.loss_dice: 0.7653
2025/03/29 14:57:05 - mmengine - INFO - Iter(train) [10400/20000]  base_lr: 5.1658e-05 lr: 5.1658e-05  eta: 2:27:08  time: 1.1534  data_time: 0.0230  memory: 11214  loss: 48.4823  decode.loss_cls: 0.3357  decode.loss_mask: 1.5150  decode.loss_dice: 1.4106  decode.d0.loss_cls: 0.5122  decode.d0.loss_mask: 1.5953  decode.d0.loss_dice: 1.4584  decode.d1.loss_cls: 0.3184  decode.d1.loss_mask: 1.5348  decode.d1.loss_dice: 1.4104  decode.d2.loss_cls: 0.3013  decode.d2.loss_mask: 1.5676  decode.d2.loss_dice: 1.4428  decode.d3.loss_cls: 0.2992  decode.d3.loss_mask: 1.5570  decode.d3.loss_dice: 1.4761  decode.d4.loss_cls: 0.3542  decode.d4.loss_mask: 1.5562  decode.d4.loss_dice: 1.4172  decode.d5.loss_cls: 0.3211  decode.d5.loss_mask: 1.6290  decode.d5.loss_dice: 1.4262  decode.d6.loss_cls: 0.3255  decode.d6.loss_mask: 1.5350  decode.d6.loss_dice: 1.4188  decode.d7.loss_cls: 0.3917  decode.d7.loss_mask: 1.4700  decode.d7.loss_dice: 1.3827  decode.d8.loss_cls: 0.3383  decode.d8.loss_mask: 1.4885  decode.d8.loss_dice: 1.4050  mix_decode.loss_cls: 0.2426  mix_decode.loss_mask: 0.5276  mix_decode.loss_dice: 0.7786  mix_decode.d0.loss_cls: 0.2853  mix_decode.d0.loss_mask: 0.5054  mix_decode.d0.loss_dice: 0.7713  mix_decode.d1.loss_cls: 0.2111  mix_decode.d1.loss_mask: 0.5186  mix_decode.d1.loss_dice: 0.7715  mix_decode.d2.loss_cls: 0.2204  mix_decode.d2.loss_mask: 0.5384  mix_decode.d2.loss_dice: 0.7605  mix_decode.d3.loss_cls: 0.2189  mix_decode.d3.loss_mask: 0.5072  mix_decode.d3.loss_dice: 0.7564  mix_decode.d4.loss_cls: 0.2793  mix_decode.d4.loss_mask: 0.4979  mix_decode.d4.loss_dice: 0.7568  mix_decode.d5.loss_cls: 0.2628  mix_decode.d5.loss_mask: 0.5064  mix_decode.d5.loss_dice: 0.7684  mix_decode.d6.loss_cls: 0.2313  mix_decode.d6.loss_mask: 0.5130  mix_decode.d6.loss_dice: 0.7662  mix_decode.d7.loss_cls: 0.2537  mix_decode.d7.loss_mask: 0.5095  mix_decode.d7.loss_dice: 0.7859  mix_decode.d8.loss_cls: 0.2669  mix_decode.d8.loss_mask: 0.5009  mix_decode.d8.loss_dice: 0.7751
2025/03/29 14:58:03 - mmengine - INFO - Iter(train) [10450/20000]  base_lr: 5.1416e-05 lr: 5.1416e-05  eta: 2:26:33  time: 1.1621  data_time: 0.0238  memory: 11210  loss: 62.3008  decode.loss_cls: 0.3993  decode.loss_mask: 2.1114  decode.loss_dice: 1.9510  decode.d0.loss_cls: 0.4861  decode.d0.loss_mask: 2.1548  decode.d0.loss_dice: 2.0127  decode.d1.loss_cls: 0.3936  decode.d1.loss_mask: 2.1354  decode.d1.loss_dice: 1.9443  decode.d2.loss_cls: 0.3831  decode.d2.loss_mask: 2.1290  decode.d2.loss_dice: 1.9451  decode.d3.loss_cls: 0.4296  decode.d3.loss_mask: 2.0902  decode.d3.loss_dice: 1.9426  decode.d4.loss_cls: 0.4011  decode.d4.loss_mask: 2.0969  decode.d4.loss_dice: 1.9674  decode.d5.loss_cls: 0.4371  decode.d5.loss_mask: 2.0789  decode.d5.loss_dice: 1.9109  decode.d6.loss_cls: 0.4002  decode.d6.loss_mask: 2.0661  decode.d6.loss_dice: 1.8976  decode.d7.loss_cls: 0.4092  decode.d7.loss_mask: 2.1145  decode.d7.loss_dice: 1.9665  decode.d8.loss_cls: 0.4125  decode.d8.loss_mask: 2.1104  decode.d8.loss_dice: 1.9441  mix_decode.loss_cls: 0.1699  mix_decode.loss_mask: 0.7833  mix_decode.loss_dice: 0.7908  mix_decode.d0.loss_cls: 0.2278  mix_decode.d0.loss_mask: 0.7482  mix_decode.d0.loss_dice: 0.8254  mix_decode.d1.loss_cls: 0.2092  mix_decode.d1.loss_mask: 0.7365  mix_decode.d1.loss_dice: 0.7875  mix_decode.d2.loss_cls: 0.1810  mix_decode.d2.loss_mask: 0.7715  mix_decode.d2.loss_dice: 0.8224  mix_decode.d3.loss_cls: 0.1976  mix_decode.d3.loss_mask: 0.7599  mix_decode.d3.loss_dice: 0.7987  mix_decode.d4.loss_cls: 0.1871  mix_decode.d4.loss_mask: 0.7399  mix_decode.d4.loss_dice: 0.7957  mix_decode.d5.loss_cls: 0.2153  mix_decode.d5.loss_mask: 0.7440  mix_decode.d5.loss_dice: 0.8030  mix_decode.d6.loss_cls: 0.2230  mix_decode.d6.loss_mask: 0.7513  mix_decode.d6.loss_dice: 0.7971  mix_decode.d7.loss_cls: 0.1939  mix_decode.d7.loss_mask: 0.7423  mix_decode.d7.loss_dice: 0.8028  mix_decode.d8.loss_cls: 0.1993  mix_decode.d8.loss_mask: 0.7590  mix_decode.d8.loss_dice: 0.8157
2025/03/29 14:59:01 - mmengine - INFO - Iter(train) [10500/20000]  base_lr: 5.1173e-05 lr: 5.1173e-05  eta: 2:25:57  time: 1.1520  data_time: 0.0225  memory: 11216  loss: 60.8026  decode.loss_cls: 0.5589  decode.loss_mask: 1.9091  decode.loss_dice: 2.0434  decode.d0.loss_cls: 0.5656  decode.d0.loss_mask: 1.9063  decode.d0.loss_dice: 1.9988  decode.d1.loss_cls: 0.5287  decode.d1.loss_mask: 1.8570  decode.d1.loss_dice: 1.9992  decode.d2.loss_cls: 0.6623  decode.d2.loss_mask: 1.8335  decode.d2.loss_dice: 1.9289  decode.d3.loss_cls: 0.5338  decode.d3.loss_mask: 1.9068  decode.d3.loss_dice: 2.0102  decode.d4.loss_cls: 0.5710  decode.d4.loss_mask: 1.8696  decode.d4.loss_dice: 2.0219  decode.d5.loss_cls: 0.5634  decode.d5.loss_mask: 1.8683  decode.d5.loss_dice: 1.9991  decode.d6.loss_cls: 0.5789  decode.d6.loss_mask: 1.8764  decode.d6.loss_dice: 2.0046  decode.d7.loss_cls: 0.4773  decode.d7.loss_mask: 1.9248  decode.d7.loss_dice: 2.0178  decode.d8.loss_cls: 0.6389  decode.d8.loss_mask: 1.8408  decode.d8.loss_dice: 2.0094  mix_decode.loss_cls: 0.2217  mix_decode.loss_mask: 0.6768  mix_decode.loss_dice: 0.7477  mix_decode.d0.loss_cls: 0.2994  mix_decode.d0.loss_mask: 0.6316  mix_decode.d0.loss_dice: 0.7608  mix_decode.d1.loss_cls: 0.2777  mix_decode.d1.loss_mask: 0.6033  mix_decode.d1.loss_dice: 0.6691  mix_decode.d2.loss_cls: 0.2536  mix_decode.d2.loss_mask: 0.6136  mix_decode.d2.loss_dice: 0.6799  mix_decode.d3.loss_cls: 0.2631  mix_decode.d3.loss_mask: 0.6314  mix_decode.d3.loss_dice: 0.6971  mix_decode.d4.loss_cls: 0.2869  mix_decode.d4.loss_mask: 0.6252  mix_decode.d4.loss_dice: 0.7060  mix_decode.d5.loss_cls: 0.2581  mix_decode.d5.loss_mask: 0.6456  mix_decode.d5.loss_dice: 0.7306  mix_decode.d6.loss_cls: 0.2808  mix_decode.d6.loss_mask: 0.6632  mix_decode.d6.loss_dice: 0.7104  mix_decode.d7.loss_cls: 0.2534  mix_decode.d7.loss_mask: 0.6998  mix_decode.d7.loss_dice: 0.7362  mix_decode.d8.loss_cls: 0.2769  mix_decode.d8.loss_mask: 0.6764  mix_decode.d8.loss_dice: 0.7218
2025/03/29 14:59:59 - mmengine - INFO - Iter(train) [10550/20000]  base_lr: 5.0931e-05 lr: 5.0931e-05  eta: 2:25:22  time: 1.1623  data_time: 0.0239  memory: 11209  loss: 58.8050  decode.loss_cls: 0.4147  decode.loss_mask: 1.9319  decode.loss_dice: 1.6466  decode.d0.loss_cls: 0.4927  decode.d0.loss_mask: 1.9684  decode.d0.loss_dice: 1.7207  decode.d1.loss_cls: 0.4396  decode.d1.loss_mask: 1.8740  decode.d1.loss_dice: 1.6582  decode.d2.loss_cls: 0.3906  decode.d2.loss_mask: 1.8915  decode.d2.loss_dice: 1.6797  decode.d3.loss_cls: 0.4362  decode.d3.loss_mask: 1.8916  decode.d3.loss_dice: 1.6188  decode.d4.loss_cls: 0.4055  decode.d4.loss_mask: 1.9243  decode.d4.loss_dice: 1.6937  decode.d5.loss_cls: 0.4290  decode.d5.loss_mask: 1.8881  decode.d5.loss_dice: 1.5863  decode.d6.loss_cls: 0.4290  decode.d6.loss_mask: 1.8969  decode.d6.loss_dice: 1.6481  decode.d7.loss_cls: 0.4878  decode.d7.loss_mask: 1.8887  decode.d7.loss_dice: 1.6514  decode.d8.loss_cls: 0.4728  decode.d8.loss_mask: 1.9135  decode.d8.loss_dice: 1.6345  mix_decode.loss_cls: 0.3134  mix_decode.loss_mask: 0.6858  mix_decode.loss_dice: 0.8548  mix_decode.d0.loss_cls: 0.3148  mix_decode.d0.loss_mask: 0.6898  mix_decode.d0.loss_dice: 0.8856  mix_decode.d1.loss_cls: 0.3593  mix_decode.d1.loss_mask: 0.6504  mix_decode.d1.loss_dice: 0.8470  mix_decode.d2.loss_cls: 0.3578  mix_decode.d2.loss_mask: 0.6882  mix_decode.d2.loss_dice: 0.8236  mix_decode.d3.loss_cls: 0.3562  mix_decode.d3.loss_mask: 0.6945  mix_decode.d3.loss_dice: 0.8589  mix_decode.d4.loss_cls: 0.3431  mix_decode.d4.loss_mask: 0.6864  mix_decode.d4.loss_dice: 0.8703  mix_decode.d5.loss_cls: 0.3599  mix_decode.d5.loss_mask: 0.6432  mix_decode.d5.loss_dice: 0.8209  mix_decode.d6.loss_cls: 0.3742  mix_decode.d6.loss_mask: 0.6637  mix_decode.d6.loss_dice: 0.8583  mix_decode.d7.loss_cls: 0.3489  mix_decode.d7.loss_mask: 0.7058  mix_decode.d7.loss_dice: 0.8883  mix_decode.d8.loss_cls: 0.3026  mix_decode.d8.loss_mask: 0.6848  mix_decode.d8.loss_dice: 0.8702
2025/03/29 15:00:57 - mmengine - INFO - Iter(train) [10600/20000]  base_lr: 5.0688e-05 lr: 5.0688e-05  eta: 2:24:46  time: 1.1557  data_time: 0.0236  memory: 11208  loss: 58.4319  decode.loss_cls: 0.3473  decode.loss_mask: 2.0809  decode.loss_dice: 1.8681  decode.d0.loss_cls: 0.3898  decode.d0.loss_mask: 2.1717  decode.d0.loss_dice: 1.8861  decode.d1.loss_cls: 0.2724  decode.d1.loss_mask: 2.1377  decode.d1.loss_dice: 1.9394  decode.d2.loss_cls: 0.3539  decode.d2.loss_mask: 2.0381  decode.d2.loss_dice: 1.8412  decode.d3.loss_cls: 0.2581  decode.d3.loss_mask: 2.1181  decode.d3.loss_dice: 1.9255  decode.d4.loss_cls: 0.2973  decode.d4.loss_mask: 2.1015  decode.d4.loss_dice: 1.8968  decode.d5.loss_cls: 0.4165  decode.d5.loss_mask: 2.1245  decode.d5.loss_dice: 1.8897  decode.d6.loss_cls: 0.3579  decode.d6.loss_mask: 2.1183  decode.d6.loss_dice: 1.9516  decode.d7.loss_cls: 0.3315  decode.d7.loss_mask: 2.1572  decode.d7.loss_dice: 1.9173  decode.d8.loss_cls: 0.3454  decode.d8.loss_mask: 2.1177  decode.d8.loss_dice: 1.9304  mix_decode.loss_cls: 0.1785  mix_decode.loss_mask: 0.5919  mix_decode.loss_dice: 0.7025  mix_decode.d0.loss_cls: 0.2514  mix_decode.d0.loss_mask: 0.5798  mix_decode.d0.loss_dice: 0.7605  mix_decode.d1.loss_cls: 0.1792  mix_decode.d1.loss_mask: 0.5546  mix_decode.d1.loss_dice: 0.7124  mix_decode.d2.loss_cls: 0.1846  mix_decode.d2.loss_mask: 0.5768  mix_decode.d2.loss_dice: 0.7101  mix_decode.d3.loss_cls: 0.1965  mix_decode.d3.loss_mask: 0.5703  mix_decode.d3.loss_dice: 0.7050  mix_decode.d4.loss_cls: 0.1784  mix_decode.d4.loss_mask: 0.5693  mix_decode.d4.loss_dice: 0.7180  mix_decode.d5.loss_cls: 0.2097  mix_decode.d5.loss_mask: 0.5875  mix_decode.d5.loss_dice: 0.7278  mix_decode.d6.loss_cls: 0.1820  mix_decode.d6.loss_mask: 0.5724  mix_decode.d6.loss_dice: 0.7123  mix_decode.d7.loss_cls: 0.1884  mix_decode.d7.loss_mask: 0.5835  mix_decode.d7.loss_dice: 0.7008  mix_decode.d8.loss_cls: 0.1814  mix_decode.d8.loss_mask: 0.5803  mix_decode.d8.loss_dice: 0.7043
2025/03/29 15:01:54 - mmengine - INFO - Iter(train) [10650/20000]  base_lr: 5.0446e-05 lr: 5.0446e-05  eta: 2:24:10  time: 1.1708  data_time: 0.0248  memory: 11227  loss: 59.0263  decode.loss_cls: 0.5449  decode.loss_mask: 1.7850  decode.loss_dice: 1.8348  decode.d0.loss_cls: 0.6891  decode.d0.loss_mask: 1.7260  decode.d0.loss_dice: 1.8738  decode.d1.loss_cls: 0.6000  decode.d1.loss_mask: 1.7825  decode.d1.loss_dice: 1.8437  decode.d2.loss_cls: 0.5436  decode.d2.loss_mask: 1.8347  decode.d2.loss_dice: 1.8310  decode.d3.loss_cls: 0.5473  decode.d3.loss_mask: 1.7523  decode.d3.loss_dice: 1.8653  decode.d4.loss_cls: 0.6351  decode.d4.loss_mask: 1.7462  decode.d4.loss_dice: 1.8254  decode.d5.loss_cls: 0.5585  decode.d5.loss_mask: 1.8246  decode.d5.loss_dice: 1.8761  decode.d6.loss_cls: 0.5880  decode.d6.loss_mask: 1.7823  decode.d6.loss_dice: 1.8773  decode.d7.loss_cls: 0.5373  decode.d7.loss_mask: 1.7553  decode.d7.loss_dice: 1.8956  decode.d8.loss_cls: 0.5018  decode.d8.loss_mask: 1.7724  decode.d8.loss_dice: 1.8613  mix_decode.loss_cls: 0.2904  mix_decode.loss_mask: 0.6332  mix_decode.loss_dice: 0.7563  mix_decode.d0.loss_cls: 0.3528  mix_decode.d0.loss_mask: 0.6315  mix_decode.d0.loss_dice: 0.8065  mix_decode.d1.loss_cls: 0.2851  mix_decode.d1.loss_mask: 0.6355  mix_decode.d1.loss_dice: 0.7549  mix_decode.d2.loss_cls: 0.3095  mix_decode.d2.loss_mask: 0.6386  mix_decode.d2.loss_dice: 0.7311  mix_decode.d3.loss_cls: 0.3254  mix_decode.d3.loss_mask: 0.6297  mix_decode.d3.loss_dice: 0.7172  mix_decode.d4.loss_cls: 0.3100  mix_decode.d4.loss_mask: 0.6359  mix_decode.d4.loss_dice: 0.7478  mix_decode.d5.loss_cls: 0.3151  mix_decode.d5.loss_mask: 0.6094  mix_decode.d5.loss_dice: 0.7511  mix_decode.d6.loss_cls: 0.2823  mix_decode.d6.loss_mask: 0.6097  mix_decode.d6.loss_dice: 0.7862  mix_decode.d7.loss_cls: 0.2956  mix_decode.d7.loss_mask: 0.6386  mix_decode.d7.loss_dice: 0.7646  mix_decode.d8.loss_cls: 0.3119  mix_decode.d8.loss_mask: 0.6290  mix_decode.d8.loss_dice: 0.7502
2025/03/29 15:02:52 - mmengine - INFO - Iter(train) [10700/20000]  base_lr: 5.0203e-05 lr: 5.0203e-05  eta: 2:23:34  time: 1.1562  data_time: 0.0232  memory: 11210  loss: 59.4155  decode.loss_cls: 0.2669  decode.loss_mask: 1.9606  decode.loss_dice: 1.7887  decode.d0.loss_cls: 0.3925  decode.d0.loss_mask: 1.9353  decode.d0.loss_dice: 1.8034  decode.d1.loss_cls: 0.2573  decode.d1.loss_mask: 1.9215  decode.d1.loss_dice: 1.7352  decode.d2.loss_cls: 0.3135  decode.d2.loss_mask: 1.8941  decode.d2.loss_dice: 1.7201  decode.d3.loss_cls: 0.2858  decode.d3.loss_mask: 1.9329  decode.d3.loss_dice: 1.7512  decode.d4.loss_cls: 0.2959  decode.d4.loss_mask: 1.9148  decode.d4.loss_dice: 1.7302  decode.d5.loss_cls: 0.2610  decode.d5.loss_mask: 1.9676  decode.d5.loss_dice: 1.8120  decode.d6.loss_cls: 0.2959  decode.d6.loss_mask: 1.9110  decode.d6.loss_dice: 1.7336  decode.d7.loss_cls: 0.3047  decode.d7.loss_mask: 1.9581  decode.d7.loss_dice: 1.8077  decode.d8.loss_cls: 0.2770  decode.d8.loss_mask: 1.9828  decode.d8.loss_dice: 1.7811  mix_decode.loss_cls: 0.2780  mix_decode.loss_mask: 0.8291  mix_decode.loss_dice: 0.8179  mix_decode.d0.loss_cls: 0.3270  mix_decode.d0.loss_mask: 0.8135  mix_decode.d0.loss_dice: 0.8600  mix_decode.d1.loss_cls: 0.3139  mix_decode.d1.loss_mask: 0.7946  mix_decode.d1.loss_dice: 0.7768  mix_decode.d2.loss_cls: 0.3512  mix_decode.d2.loss_mask: 0.8218  mix_decode.d2.loss_dice: 0.7950  mix_decode.d3.loss_cls: 0.3105  mix_decode.d3.loss_mask: 0.8297  mix_decode.d3.loss_dice: 0.8112  mix_decode.d4.loss_cls: 0.3423  mix_decode.d4.loss_mask: 0.7972  mix_decode.d4.loss_dice: 0.7885  mix_decode.d5.loss_cls: 0.3150  mix_decode.d5.loss_mask: 0.8031  mix_decode.d5.loss_dice: 0.8238  mix_decode.d6.loss_cls: 0.3240  mix_decode.d6.loss_mask: 0.8095  mix_decode.d6.loss_dice: 0.8159  mix_decode.d7.loss_cls: 0.3357  mix_decode.d7.loss_mask: 0.8057  mix_decode.d7.loss_dice: 0.8111  mix_decode.d8.loss_cls: 0.2882  mix_decode.d8.loss_mask: 0.8170  mix_decode.d8.loss_dice: 0.8156
2025/03/29 15:03:50 - mmengine - INFO - Iter(train) [10750/20000]  base_lr: 4.9960e-05 lr: 4.9960e-05  eta: 2:22:57  time: 1.1497  data_time: 0.0224  memory: 11225  loss: 61.9469  decode.loss_cls: 0.4078  decode.loss_mask: 1.9947  decode.loss_dice: 1.8714  decode.d0.loss_cls: 0.5924  decode.d0.loss_mask: 1.9938  decode.d0.loss_dice: 1.8759  decode.d1.loss_cls: 0.5325  decode.d1.loss_mask: 1.9597  decode.d1.loss_dice: 1.8480  decode.d2.loss_cls: 0.5952  decode.d2.loss_mask: 1.9460  decode.d2.loss_dice: 1.9009  decode.d3.loss_cls: 0.4282  decode.d3.loss_mask: 1.9994  decode.d3.loss_dice: 1.8798  decode.d4.loss_cls: 0.3787  decode.d4.loss_mask: 2.0470  decode.d4.loss_dice: 1.8902  decode.d5.loss_cls: 0.4756  decode.d5.loss_mask: 2.0092  decode.d5.loss_dice: 1.8838  decode.d6.loss_cls: 0.5103  decode.d6.loss_mask: 2.0051  decode.d6.loss_dice: 1.8295  decode.d7.loss_cls: 0.5172  decode.d7.loss_mask: 1.9555  decode.d7.loss_dice: 1.8114  decode.d8.loss_cls: 0.4911  decode.d8.loss_mask: 1.9449  decode.d8.loss_dice: 1.8368  mix_decode.loss_cls: 0.3067  mix_decode.loss_mask: 0.6686  mix_decode.loss_dice: 0.8344  mix_decode.d0.loss_cls: 0.3551  mix_decode.d0.loss_mask: 0.6782  mix_decode.d0.loss_dice: 0.8741  mix_decode.d1.loss_cls: 0.3313  mix_decode.d1.loss_mask: 0.6564  mix_decode.d1.loss_dice: 0.8161  mix_decode.d2.loss_cls: 0.3221  mix_decode.d2.loss_mask: 0.6600  mix_decode.d2.loss_dice: 0.8387  mix_decode.d3.loss_cls: 0.3409  mix_decode.d3.loss_mask: 0.6476  mix_decode.d3.loss_dice: 0.8372  mix_decode.d4.loss_cls: 0.3380  mix_decode.d4.loss_mask: 0.7090  mix_decode.d4.loss_dice: 0.8580  mix_decode.d5.loss_cls: 0.3419  mix_decode.d5.loss_mask: 0.7061  mix_decode.d5.loss_dice: 0.8361  mix_decode.d6.loss_cls: 0.3434  mix_decode.d6.loss_mask: 0.6887  mix_decode.d6.loss_dice: 0.8200  mix_decode.d7.loss_cls: 0.3821  mix_decode.d7.loss_mask: 0.6686  mix_decode.d7.loss_dice: 0.8373  mix_decode.d8.loss_cls: 0.3088  mix_decode.d8.loss_mask: 0.6926  mix_decode.d8.loss_dice: 0.8370
2025/03/29 15:04:48 - mmengine - INFO - Iter(train) [10800/20000]  base_lr: 4.9717e-05 lr: 4.9717e-05  eta: 2:22:21  time: 1.1573  data_time: 0.0235  memory: 11213  loss: 58.7731  decode.loss_cls: 0.2841  decode.loss_mask: 1.9525  decode.loss_dice: 1.8061  decode.d0.loss_cls: 0.4175  decode.d0.loss_mask: 1.9931  decode.d0.loss_dice: 1.8215  decode.d1.loss_cls: 0.2815  decode.d1.loss_mask: 1.9686  decode.d1.loss_dice: 1.8360  decode.d2.loss_cls: 0.2678  decode.d2.loss_mask: 1.9589  decode.d2.loss_dice: 1.8239  decode.d3.loss_cls: 0.2607  decode.d3.loss_mask: 1.9677  decode.d3.loss_dice: 1.8286  decode.d4.loss_cls: 0.2598  decode.d4.loss_mask: 1.9743  decode.d4.loss_dice: 1.8126  decode.d5.loss_cls: 0.3272  decode.d5.loss_mask: 1.9625  decode.d5.loss_dice: 1.8050  decode.d6.loss_cls: 0.2815  decode.d6.loss_mask: 1.9584  decode.d6.loss_dice: 1.8151  decode.d7.loss_cls: 0.2693  decode.d7.loss_mask: 1.9602  decode.d7.loss_dice: 1.8138  decode.d8.loss_cls: 0.3437  decode.d8.loss_mask: 1.9543  decode.d8.loss_dice: 1.8181  mix_decode.loss_cls: 0.2257  mix_decode.loss_mask: 0.7172  mix_decode.loss_dice: 0.8160  mix_decode.d0.loss_cls: 0.2637  mix_decode.d0.loss_mask: 0.7291  mix_decode.d0.loss_dice: 0.8616  mix_decode.d1.loss_cls: 0.2356  mix_decode.d1.loss_mask: 0.7279  mix_decode.d1.loss_dice: 0.8095  mix_decode.d2.loss_cls: 0.2377  mix_decode.d2.loss_mask: 0.7288  mix_decode.d2.loss_dice: 0.8328  mix_decode.d3.loss_cls: 0.2174  mix_decode.d3.loss_mask: 0.7177  mix_decode.d3.loss_dice: 0.8393  mix_decode.d4.loss_cls: 0.2594  mix_decode.d4.loss_mask: 0.7184  mix_decode.d4.loss_dice: 0.8437  mix_decode.d5.loss_cls: 0.2704  mix_decode.d5.loss_mask: 0.6955  mix_decode.d5.loss_dice: 0.8359  mix_decode.d6.loss_cls: 0.1842  mix_decode.d6.loss_mask: 0.7340  mix_decode.d6.loss_dice: 0.8455  mix_decode.d7.loss_cls: 0.2159  mix_decode.d7.loss_mask: 0.7283  mix_decode.d7.loss_dice: 0.8525  mix_decode.d8.loss_cls: 0.2339  mix_decode.d8.loss_mask: 0.7236  mix_decode.d8.loss_dice: 0.8475
2025/03/29 15:05:46 - mmengine - INFO - Iter(train) [10850/20000]  base_lr: 4.9473e-05 lr: 4.9473e-05  eta: 2:21:44  time: 1.1510  data_time: 0.0226  memory: 11212  loss: 46.2234  decode.loss_cls: 0.3113  decode.loss_mask: 1.6099  decode.loss_dice: 1.3428  decode.d0.loss_cls: 0.4631  decode.d0.loss_mask: 1.6488  decode.d0.loss_dice: 1.3047  decode.d1.loss_cls: 0.2426  decode.d1.loss_mask: 1.6056  decode.d1.loss_dice: 1.3458  decode.d2.loss_cls: 0.2858  decode.d2.loss_mask: 1.6492  decode.d2.loss_dice: 1.3522  decode.d3.loss_cls: 0.3506  decode.d3.loss_mask: 1.6416  decode.d3.loss_dice: 1.3272  decode.d4.loss_cls: 0.3127  decode.d4.loss_mask: 1.6263  decode.d4.loss_dice: 1.3555  decode.d5.loss_cls: 0.3522  decode.d5.loss_mask: 1.6321  decode.d5.loss_dice: 1.3300  decode.d6.loss_cls: 0.3630  decode.d6.loss_mask: 1.5963  decode.d6.loss_dice: 1.2931  decode.d7.loss_cls: 0.3516  decode.d7.loss_mask: 1.6313  decode.d7.loss_dice: 1.3341  decode.d8.loss_cls: 0.3495  decode.d8.loss_mask: 1.6259  decode.d8.loss_dice: 1.3185  mix_decode.loss_cls: 0.1591  mix_decode.loss_mask: 0.5326  mix_decode.loss_dice: 0.6033  mix_decode.d0.loss_cls: 0.2091  mix_decode.d0.loss_mask: 0.5284  mix_decode.d0.loss_dice: 0.6234  mix_decode.d1.loss_cls: 0.1602  mix_decode.d1.loss_mask: 0.5415  mix_decode.d1.loss_dice: 0.6060  mix_decode.d2.loss_cls: 0.1240  mix_decode.d2.loss_mask: 0.5825  mix_decode.d2.loss_dice: 0.6216  mix_decode.d3.loss_cls: 0.1648  mix_decode.d3.loss_mask: 0.5355  mix_decode.d3.loss_dice: 0.6147  mix_decode.d4.loss_cls: 0.1415  mix_decode.d4.loss_mask: 0.5921  mix_decode.d4.loss_dice: 0.6122  mix_decode.d5.loss_cls: 0.1728  mix_decode.d5.loss_mask: 0.5498  mix_decode.d5.loss_dice: 0.6200  mix_decode.d6.loss_cls: 0.1818  mix_decode.d6.loss_mask: 0.5466  mix_decode.d6.loss_dice: 0.6182  mix_decode.d7.loss_cls: 0.1641  mix_decode.d7.loss_mask: 0.5378  mix_decode.d7.loss_dice: 0.6187  mix_decode.d8.loss_cls: 0.1396  mix_decode.d8.loss_mask: 0.5457  mix_decode.d8.loss_dice: 0.6224
2025/03/29 15:06:44 - mmengine - INFO - Iter(train) [10900/20000]  base_lr: 4.9230e-05 lr: 4.9230e-05  eta: 2:21:07  time: 1.1558  data_time: 0.0228  memory: 11209  loss: 54.9156  decode.loss_cls: 0.2093  decode.loss_mask: 1.9169  decode.loss_dice: 1.7859  decode.d0.loss_cls: 0.2961  decode.d0.loss_mask: 1.9230  decode.d0.loss_dice: 1.8676  decode.d1.loss_cls: 0.2287  decode.d1.loss_mask: 1.8703  decode.d1.loss_dice: 1.7734  decode.d2.loss_cls: 0.2226  decode.d2.loss_mask: 1.8787  decode.d2.loss_dice: 1.7726  decode.d3.loss_cls: 0.1811  decode.d3.loss_mask: 1.9045  decode.d3.loss_dice: 1.7825  decode.d4.loss_cls: 0.2045  decode.d4.loss_mask: 1.8736  decode.d4.loss_dice: 1.8142  decode.d5.loss_cls: 0.2283  decode.d5.loss_mask: 1.8684  decode.d5.loss_dice: 1.7911  decode.d6.loss_cls: 0.2319  decode.d6.loss_mask: 1.8578  decode.d6.loss_dice: 1.8148  decode.d7.loss_cls: 0.1710  decode.d7.loss_mask: 1.9092  decode.d7.loss_dice: 1.8047  decode.d8.loss_cls: 0.2395  decode.d8.loss_mask: 1.9067  decode.d8.loss_dice: 1.7680  mix_decode.loss_cls: 0.2128  mix_decode.loss_mask: 0.6321  mix_decode.loss_dice: 0.7445  mix_decode.d0.loss_cls: 0.2195  mix_decode.d0.loss_mask: 0.6299  mix_decode.d0.loss_dice: 0.7754  mix_decode.d1.loss_cls: 0.1669  mix_decode.d1.loss_mask: 0.6468  mix_decode.d1.loss_dice: 0.7394  mix_decode.d2.loss_cls: 0.1689  mix_decode.d2.loss_mask: 0.6598  mix_decode.d2.loss_dice: 0.7386  mix_decode.d3.loss_cls: 0.1871  mix_decode.d3.loss_mask: 0.6369  mix_decode.d3.loss_dice: 0.7248  mix_decode.d4.loss_cls: 0.1993  mix_decode.d4.loss_mask: 0.6331  mix_decode.d4.loss_dice: 0.7419  mix_decode.d5.loss_cls: 0.1835  mix_decode.d5.loss_mask: 0.6365  mix_decode.d5.loss_dice: 0.7450  mix_decode.d6.loss_cls: 0.2015  mix_decode.d6.loss_mask: 0.6456  mix_decode.d6.loss_dice: 0.7554  mix_decode.d7.loss_cls: 0.2305  mix_decode.d7.loss_mask: 0.6359  mix_decode.d7.loss_dice: 0.7561  mix_decode.d8.loss_cls: 0.1921  mix_decode.d8.loss_mask: 0.6344  mix_decode.d8.loss_dice: 0.7443
2025/03/29 15:07:42 - mmengine - INFO - Iter(train) [10950/20000]  base_lr: 4.8986e-05 lr: 4.8986e-05  eta: 2:20:31  time: 1.1639  data_time: 0.0241  memory: 11215  loss: 58.0659  decode.loss_cls: 0.2666  decode.loss_mask: 1.9876  decode.loss_dice: 1.6538  decode.d0.loss_cls: 0.4826  decode.d0.loss_mask: 1.9956  decode.d0.loss_dice: 1.6955  decode.d1.loss_cls: 0.3742  decode.d1.loss_mask: 1.9829  decode.d1.loss_dice: 1.6409  decode.d2.loss_cls: 0.3831  decode.d2.loss_mask: 1.9623  decode.d2.loss_dice: 1.6090  decode.d3.loss_cls: 0.3053  decode.d3.loss_mask: 1.9797  decode.d3.loss_dice: 1.6400  decode.d4.loss_cls: 0.3383  decode.d4.loss_mask: 1.9959  decode.d4.loss_dice: 1.6389  decode.d5.loss_cls: 0.3241  decode.d5.loss_mask: 1.9786  decode.d5.loss_dice: 1.6419  decode.d6.loss_cls: 0.3146  decode.d6.loss_mask: 2.0173  decode.d6.loss_dice: 1.6551  decode.d7.loss_cls: 0.3166  decode.d7.loss_mask: 2.0119  decode.d7.loss_dice: 1.6658  decode.d8.loss_cls: 0.3154  decode.d8.loss_mask: 2.0224  decode.d8.loss_dice: 1.6122  mix_decode.loss_cls: 0.2777  mix_decode.loss_mask: 0.7264  mix_decode.loss_dice: 0.8010  mix_decode.d0.loss_cls: 0.2842  mix_decode.d0.loss_mask: 0.6997  mix_decode.d0.loss_dice: 0.8415  mix_decode.d1.loss_cls: 0.3046  mix_decode.d1.loss_mask: 0.6944  mix_decode.d1.loss_dice: 0.7911  mix_decode.d2.loss_cls: 0.3163  mix_decode.d2.loss_mask: 0.6962  mix_decode.d2.loss_dice: 0.7939  mix_decode.d3.loss_cls: 0.2898  mix_decode.d3.loss_mask: 0.7083  mix_decode.d3.loss_dice: 0.8227  mix_decode.d4.loss_cls: 0.3253  mix_decode.d4.loss_mask: 0.7608  mix_decode.d4.loss_dice: 0.8381  mix_decode.d5.loss_cls: 0.2378  mix_decode.d5.loss_mask: 0.7287  mix_decode.d5.loss_dice: 0.8266  mix_decode.d6.loss_cls: 0.2960  mix_decode.d6.loss_mask: 0.7219  mix_decode.d6.loss_dice: 0.8170  mix_decode.d7.loss_cls: 0.3376  mix_decode.d7.loss_mask: 0.7155  mix_decode.d7.loss_dice: 0.8194  mix_decode.d8.loss_cls: 0.2940  mix_decode.d8.loss_mask: 0.7039  mix_decode.d8.loss_dice: 0.7872
2025/03/29 15:08:40 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 15:08:40 - mmengine - INFO - Iter(train) [11000/20000]  base_lr: 4.8743e-05 lr: 4.8743e-05  eta: 2:19:53  time: 1.1500  data_time: 0.0227  memory: 11223  loss: 60.5327  decode.loss_cls: 0.4269  decode.loss_mask: 2.0452  decode.loss_dice: 1.8069  decode.d0.loss_cls: 0.5036  decode.d0.loss_mask: 2.0103  decode.d0.loss_dice: 1.8340  decode.d1.loss_cls: 0.4345  decode.d1.loss_mask: 2.0552  decode.d1.loss_dice: 1.8383  decode.d2.loss_cls: 0.4270  decode.d2.loss_mask: 2.0320  decode.d2.loss_dice: 1.8011  decode.d3.loss_cls: 0.3940  decode.d3.loss_mask: 2.0595  decode.d3.loss_dice: 1.8135  decode.d4.loss_cls: 0.4439  decode.d4.loss_mask: 2.0302  decode.d4.loss_dice: 1.7771  decode.d5.loss_cls: 0.4189  decode.d5.loss_mask: 2.0648  decode.d5.loss_dice: 1.8685  decode.d6.loss_cls: 0.4182  decode.d6.loss_mask: 2.0343  decode.d6.loss_dice: 1.8136  decode.d7.loss_cls: 0.4235  decode.d7.loss_mask: 2.0553  decode.d7.loss_dice: 1.8326  decode.d8.loss_cls: 0.4179  decode.d8.loss_mask: 2.0535  decode.d8.loss_dice: 1.7872  mix_decode.loss_cls: 0.2348  mix_decode.loss_mask: 0.7086  mix_decode.loss_dice: 0.7666  mix_decode.d0.loss_cls: 0.2677  mix_decode.d0.loss_mask: 0.7359  mix_decode.d0.loss_dice: 0.8435  mix_decode.d1.loss_cls: 0.2485  mix_decode.d1.loss_mask: 0.6799  mix_decode.d1.loss_dice: 0.7877  mix_decode.d2.loss_cls: 0.2448  mix_decode.d2.loss_mask: 0.6895  mix_decode.d2.loss_dice: 0.7788  mix_decode.d3.loss_cls: 0.2669  mix_decode.d3.loss_mask: 0.7172  mix_decode.d3.loss_dice: 0.7872  mix_decode.d4.loss_cls: 0.2455  mix_decode.d4.loss_mask: 0.7065  mix_decode.d4.loss_dice: 0.7751  mix_decode.d5.loss_cls: 0.2789  mix_decode.d5.loss_mask: 0.6994  mix_decode.d5.loss_dice: 0.8023  mix_decode.d6.loss_cls: 0.2570  mix_decode.d6.loss_mask: 0.7063  mix_decode.d6.loss_dice: 0.7921  mix_decode.d7.loss_cls: 0.2975  mix_decode.d7.loss_mask: 0.7371  mix_decode.d7.loss_dice: 0.7920  mix_decode.d8.loss_cls: 0.2596  mix_decode.d8.loss_mask: 0.7205  mix_decode.d8.loss_dice: 0.7839
2025/03/29 15:09:38 - mmengine - INFO - Iter(train) [11050/20000]  base_lr: 4.8499e-05 lr: 4.8499e-05  eta: 2:19:16  time: 1.1528  data_time: 0.0225  memory: 11209  loss: 57.8646  decode.loss_cls: 0.2712  decode.loss_mask: 1.9915  decode.loss_dice: 1.7564  decode.d0.loss_cls: 0.4081  decode.d0.loss_mask: 2.0203  decode.d0.loss_dice: 1.7153  decode.d1.loss_cls: 0.3267  decode.d1.loss_mask: 2.0170  decode.d1.loss_dice: 1.6745  decode.d2.loss_cls: 0.3492  decode.d2.loss_mask: 2.0264  decode.d2.loss_dice: 1.6775  decode.d3.loss_cls: 0.3523  decode.d3.loss_mask: 1.9817  decode.d3.loss_dice: 1.6710  decode.d4.loss_cls: 0.3288  decode.d4.loss_mask: 2.0169  decode.d4.loss_dice: 1.6839  decode.d5.loss_cls: 0.3563  decode.d5.loss_mask: 1.9708  decode.d5.loss_dice: 1.6860  decode.d6.loss_cls: 0.3201  decode.d6.loss_mask: 1.9974  decode.d6.loss_dice: 1.7384  decode.d7.loss_cls: 0.3339  decode.d7.loss_mask: 2.0099  decode.d7.loss_dice: 1.7469  decode.d8.loss_cls: 0.3141  decode.d8.loss_mask: 2.0121  decode.d8.loss_dice: 1.7222  mix_decode.loss_cls: 0.2601  mix_decode.loss_mask: 0.6901  mix_decode.loss_dice: 0.7677  mix_decode.d0.loss_cls: 0.3351  mix_decode.d0.loss_mask: 0.6667  mix_decode.d0.loss_dice: 0.7780  mix_decode.d1.loss_cls: 0.3109  mix_decode.d1.loss_mask: 0.6497  mix_decode.d1.loss_dice: 0.7319  mix_decode.d2.loss_cls: 0.3951  mix_decode.d2.loss_mask: 0.6191  mix_decode.d2.loss_dice: 0.7518  mix_decode.d3.loss_cls: 0.3215  mix_decode.d3.loss_mask: 0.6394  mix_decode.d3.loss_dice: 0.7592  mix_decode.d4.loss_cls: 0.2840  mix_decode.d4.loss_mask: 0.6208  mix_decode.d4.loss_dice: 0.7487  mix_decode.d5.loss_cls: 0.3181  mix_decode.d5.loss_mask: 0.6586  mix_decode.d5.loss_dice: 0.7765  mix_decode.d6.loss_cls: 0.3271  mix_decode.d6.loss_mask: 0.6644  mix_decode.d6.loss_dice: 0.7915  mix_decode.d7.loss_cls: 0.3258  mix_decode.d7.loss_mask: 0.6629  mix_decode.d7.loss_dice: 0.7934  mix_decode.d8.loss_cls: 0.2929  mix_decode.d8.loss_mask: 0.6670  mix_decode.d8.loss_dice: 0.7798
2025/03/29 15:10:35 - mmengine - INFO - Iter(train) [11100/20000]  base_lr: 4.8255e-05 lr: 4.8255e-05  eta: 2:18:38  time: 1.1633  data_time: 0.0239  memory: 11213  loss: 52.3504  decode.loss_cls: 0.2823  decode.loss_mask: 1.8341  decode.loss_dice: 1.6096  decode.d0.loss_cls: 0.4271  decode.d0.loss_mask: 1.8065  decode.d0.loss_dice: 1.5763  decode.d1.loss_cls: 0.3338  decode.d1.loss_mask: 1.8044  decode.d1.loss_dice: 1.5796  decode.d2.loss_cls: 0.3230  decode.d2.loss_mask: 1.8049  decode.d2.loss_dice: 1.5884  decode.d3.loss_cls: 0.3246  decode.d3.loss_mask: 1.8316  decode.d3.loss_dice: 1.5424  decode.d4.loss_cls: 0.2721  decode.d4.loss_mask: 1.8434  decode.d4.loss_dice: 1.6262  decode.d5.loss_cls: 0.2584  decode.d5.loss_mask: 1.8373  decode.d5.loss_dice: 1.6101  decode.d6.loss_cls: 0.3576  decode.d6.loss_mask: 1.8222  decode.d6.loss_dice: 1.5983  decode.d7.loss_cls: 0.2810  decode.d7.loss_mask: 1.8601  decode.d7.loss_dice: 1.6281  decode.d8.loss_cls: 0.3250  decode.d8.loss_mask: 1.8286  decode.d8.loss_dice: 1.6273  mix_decode.loss_cls: 0.2351  mix_decode.loss_mask: 0.5549  mix_decode.loss_dice: 0.6612  mix_decode.d0.loss_cls: 0.2738  mix_decode.d0.loss_mask: 0.5518  mix_decode.d0.loss_dice: 0.7207  mix_decode.d1.loss_cls: 0.2652  mix_decode.d1.loss_mask: 0.5335  mix_decode.d1.loss_dice: 0.6511  mix_decode.d2.loss_cls: 0.3172  mix_decode.d2.loss_mask: 0.5325  mix_decode.d2.loss_dice: 0.6378  mix_decode.d3.loss_cls: 0.2904  mix_decode.d3.loss_mask: 0.5366  mix_decode.d3.loss_dice: 0.6595  mix_decode.d4.loss_cls: 0.2998  mix_decode.d4.loss_mask: 0.5362  mix_decode.d4.loss_dice: 0.6516  mix_decode.d5.loss_cls: 0.2760  mix_decode.d5.loss_mask: 0.5803  mix_decode.d5.loss_dice: 0.7002  mix_decode.d6.loss_cls: 0.2796  mix_decode.d6.loss_mask: 0.5410  mix_decode.d6.loss_dice: 0.6764  mix_decode.d7.loss_cls: 0.3001  mix_decode.d7.loss_mask: 0.5330  mix_decode.d7.loss_dice: 0.6351  mix_decode.d8.loss_cls: 0.2433  mix_decode.d8.loss_mask: 0.5637  mix_decode.d8.loss_dice: 0.6681
2025/03/29 15:11:33 - mmengine - INFO - Iter(train) [11150/20000]  base_lr: 4.8011e-05 lr: 4.8011e-05  eta: 2:18:00  time: 1.1665  data_time: 0.0240  memory: 11217  loss: 58.3081  decode.loss_cls: 0.2220  decode.loss_mask: 2.0814  decode.loss_dice: 1.7547  decode.d0.loss_cls: 0.3520  decode.d0.loss_mask: 2.0400  decode.d0.loss_dice: 1.7781  decode.d1.loss_cls: 0.2720  decode.d1.loss_mask: 2.0455  decode.d1.loss_dice: 1.7708  decode.d2.loss_cls: 0.2402  decode.d2.loss_mask: 2.0639  decode.d2.loss_dice: 1.7683  decode.d3.loss_cls: 0.2622  decode.d3.loss_mask: 2.0591  decode.d3.loss_dice: 1.7739  decode.d4.loss_cls: 0.2779  decode.d4.loss_mask: 2.0194  decode.d4.loss_dice: 1.7737  decode.d5.loss_cls: 0.2659  decode.d5.loss_mask: 2.0306  decode.d5.loss_dice: 1.7527  decode.d6.loss_cls: 0.2158  decode.d6.loss_mask: 2.0631  decode.d6.loss_dice: 1.7593  decode.d7.loss_cls: 0.2682  decode.d7.loss_mask: 2.0621  decode.d7.loss_dice: 1.7544  decode.d8.loss_cls: 0.2366  decode.d8.loss_mask: 2.0255  decode.d8.loss_dice: 1.7321  mix_decode.loss_cls: 0.2827  mix_decode.loss_mask: 0.6404  mix_decode.loss_dice: 0.7985  mix_decode.d0.loss_cls: 0.2926  mix_decode.d0.loss_mask: 0.6364  mix_decode.d0.loss_dice: 0.8736  mix_decode.d1.loss_cls: 0.2676  mix_decode.d1.loss_mask: 0.6372  mix_decode.d1.loss_dice: 0.8287  mix_decode.d2.loss_cls: 0.2999  mix_decode.d2.loss_mask: 0.6522  mix_decode.d2.loss_dice: 0.8029  mix_decode.d3.loss_cls: 0.3129  mix_decode.d3.loss_mask: 0.6507  mix_decode.d3.loss_dice: 0.8111  mix_decode.d4.loss_cls: 0.2673  mix_decode.d4.loss_mask: 0.6438  mix_decode.d4.loss_dice: 0.8248  mix_decode.d5.loss_cls: 0.2708  mix_decode.d5.loss_mask: 0.6645  mix_decode.d5.loss_dice: 0.8411  mix_decode.d6.loss_cls: 0.2410  mix_decode.d6.loss_mask: 0.6495  mix_decode.d6.loss_dice: 0.8420  mix_decode.d7.loss_cls: 0.3322  mix_decode.d7.loss_mask: 0.6466  mix_decode.d7.loss_dice: 0.8184  mix_decode.d8.loss_cls: 0.2775  mix_decode.d8.loss_mask: 0.6565  mix_decode.d8.loss_dice: 0.8230
2025/03/29 15:12:31 - mmengine - INFO - Iter(train) [11200/20000]  base_lr: 4.7767e-05 lr: 4.7767e-05  eta: 2:17:22  time: 1.1514  data_time: 0.0224  memory: 11214  loss: 53.3949  decode.loss_cls: 0.4028  decode.loss_mask: 1.8210  decode.loss_dice: 1.5946  decode.d0.loss_cls: 0.5417  decode.d0.loss_mask: 1.8360  decode.d0.loss_dice: 1.6183  decode.d1.loss_cls: 0.4227  decode.d1.loss_mask: 1.8552  decode.d1.loss_dice: 1.5822  decode.d2.loss_cls: 0.4298  decode.d2.loss_mask: 1.7798  decode.d2.loss_dice: 1.5502  decode.d3.loss_cls: 0.4577  decode.d3.loss_mask: 1.7992  decode.d3.loss_dice: 1.5661  decode.d4.loss_cls: 0.4616  decode.d4.loss_mask: 1.7796  decode.d4.loss_dice: 1.5321  decode.d5.loss_cls: 0.4303  decode.d5.loss_mask: 1.8263  decode.d5.loss_dice: 1.5979  decode.d6.loss_cls: 0.4174  decode.d6.loss_mask: 1.7631  decode.d6.loss_dice: 1.5523  decode.d7.loss_cls: 0.4610  decode.d7.loss_mask: 1.7793  decode.d7.loss_dice: 1.5502  decode.d8.loss_cls: 0.4079  decode.d8.loss_mask: 1.8284  decode.d8.loss_dice: 1.5614  mix_decode.loss_cls: 0.2765  mix_decode.loss_mask: 0.5622  mix_decode.loss_dice: 0.6534  mix_decode.d0.loss_cls: 0.3588  mix_decode.d0.loss_mask: 0.5752  mix_decode.d0.loss_dice: 0.6864  mix_decode.d1.loss_cls: 0.2882  mix_decode.d1.loss_mask: 0.5517  mix_decode.d1.loss_dice: 0.6691  mix_decode.d2.loss_cls: 0.2404  mix_decode.d2.loss_mask: 0.6048  mix_decode.d2.loss_dice: 0.6943  mix_decode.d3.loss_cls: 0.2740  mix_decode.d3.loss_mask: 0.5732  mix_decode.d3.loss_dice: 0.6671  mix_decode.d4.loss_cls: 0.2541  mix_decode.d4.loss_mask: 0.5948  mix_decode.d4.loss_dice: 0.6863  mix_decode.d5.loss_cls: 0.2655  mix_decode.d5.loss_mask: 0.5677  mix_decode.d5.loss_dice: 0.6604  mix_decode.d6.loss_cls: 0.2436  mix_decode.d6.loss_mask: 0.5802  mix_decode.d6.loss_dice: 0.6852  mix_decode.d7.loss_cls: 0.2582  mix_decode.d7.loss_mask: 0.5642  mix_decode.d7.loss_dice: 0.6726  mix_decode.d8.loss_cls: 0.2780  mix_decode.d8.loss_mask: 0.5613  mix_decode.d8.loss_dice: 0.6412
2025/03/29 15:13:29 - mmengine - INFO - Iter(train) [11250/20000]  base_lr: 4.7523e-05 lr: 4.7523e-05  eta: 2:16:43  time: 1.1512  data_time: 0.0222  memory: 11223  loss: 59.1543  decode.loss_cls: 0.1200  decode.loss_mask: 2.0575  decode.loss_dice: 2.0256  decode.d0.loss_cls: 0.3105  decode.d0.loss_mask: 2.0268  decode.d0.loss_dice: 1.9470  decode.d1.loss_cls: 0.2392  decode.d1.loss_mask: 2.0163  decode.d1.loss_dice: 1.9694  decode.d2.loss_cls: 0.2333  decode.d2.loss_mask: 1.9785  decode.d2.loss_dice: 1.9115  decode.d3.loss_cls: 0.2437  decode.d3.loss_mask: 1.9737  decode.d3.loss_dice: 1.9207  decode.d4.loss_cls: 0.2520  decode.d4.loss_mask: 1.9635  decode.d4.loss_dice: 1.9468  decode.d5.loss_cls: 0.2636  decode.d5.loss_mask: 1.9549  decode.d5.loss_dice: 1.9910  decode.d6.loss_cls: 0.2090  decode.d6.loss_mask: 2.0004  decode.d6.loss_dice: 1.9670  decode.d7.loss_cls: 0.2063  decode.d7.loss_mask: 2.0401  decode.d7.loss_dice: 1.9971  decode.d8.loss_cls: 0.2193  decode.d8.loss_mask: 1.9978  decode.d8.loss_dice: 2.0067  mix_decode.loss_cls: 0.3205  mix_decode.loss_mask: 0.5959  mix_decode.loss_dice: 0.7789  mix_decode.d0.loss_cls: 0.4081  mix_decode.d0.loss_mask: 0.6381  mix_decode.d0.loss_dice: 0.8746  mix_decode.d1.loss_cls: 0.3054  mix_decode.d1.loss_mask: 0.5758  mix_decode.d1.loss_dice: 0.7923  mix_decode.d2.loss_cls: 0.3225  mix_decode.d2.loss_mask: 0.5748  mix_decode.d2.loss_dice: 0.7741  mix_decode.d3.loss_cls: 0.3405  mix_decode.d3.loss_mask: 0.5761  mix_decode.d3.loss_dice: 0.7644  mix_decode.d4.loss_cls: 0.3603  mix_decode.d4.loss_mask: 0.5769  mix_decode.d4.loss_dice: 0.7845  mix_decode.d5.loss_cls: 0.3491  mix_decode.d5.loss_mask: 0.5678  mix_decode.d5.loss_dice: 0.7820  mix_decode.d6.loss_cls: 0.3346  mix_decode.d6.loss_mask: 0.5620  mix_decode.d6.loss_dice: 0.7576  mix_decode.d7.loss_cls: 0.3839  mix_decode.d7.loss_mask: 0.5721  mix_decode.d7.loss_dice: 0.7707  mix_decode.d8.loss_cls: 0.3605  mix_decode.d8.loss_mask: 0.5789  mix_decode.d8.loss_dice: 0.7817
2025/03/29 15:14:26 - mmengine - INFO - Iter(train) [11300/20000]  base_lr: 4.7278e-05 lr: 4.7278e-05  eta: 2:16:05  time: 1.1512  data_time: 0.0223  memory: 11202  loss: 50.9283  decode.loss_cls: 0.4131  decode.loss_mask: 1.5489  decode.loss_dice: 1.7005  decode.d0.loss_cls: 0.5749  decode.d0.loss_mask: 1.5933  decode.d0.loss_dice: 1.6769  decode.d1.loss_cls: 0.3767  decode.d1.loss_mask: 1.6056  decode.d1.loss_dice: 1.6995  decode.d2.loss_cls: 0.3726  decode.d2.loss_mask: 1.6418  decode.d2.loss_dice: 1.6698  decode.d3.loss_cls: 0.4169  decode.d3.loss_mask: 1.6021  decode.d3.loss_dice: 1.6943  decode.d4.loss_cls: 0.4224  decode.d4.loss_mask: 1.6254  decode.d4.loss_dice: 1.6794  decode.d5.loss_cls: 0.4239  decode.d5.loss_mask: 1.5723  decode.d5.loss_dice: 1.6647  decode.d6.loss_cls: 0.4517  decode.d6.loss_mask: 1.6305  decode.d6.loss_dice: 1.7110  decode.d7.loss_cls: 0.4160  decode.d7.loss_mask: 1.5808  decode.d7.loss_dice: 1.6773  decode.d8.loss_cls: 0.4036  decode.d8.loss_mask: 1.5788  decode.d8.loss_dice: 1.6713  mix_decode.loss_cls: 0.2408  mix_decode.loss_mask: 0.4239  mix_decode.loss_dice: 0.6649  mix_decode.d0.loss_cls: 0.3435  mix_decode.d0.loss_mask: 0.4220  mix_decode.d0.loss_dice: 0.7246  mix_decode.d1.loss_cls: 0.2522  mix_decode.d1.loss_mask: 0.4200  mix_decode.d1.loss_dice: 0.6952  mix_decode.d2.loss_cls: 0.2275  mix_decode.d2.loss_mask: 0.4482  mix_decode.d2.loss_dice: 0.6902  mix_decode.d3.loss_cls: 0.2634  mix_decode.d3.loss_mask: 0.4300  mix_decode.d3.loss_dice: 0.6687  mix_decode.d4.loss_cls: 0.2529  mix_decode.d4.loss_mask: 0.4288  mix_decode.d4.loss_dice: 0.6888  mix_decode.d5.loss_cls: 0.2610  mix_decode.d5.loss_mask: 0.4307  mix_decode.d5.loss_dice: 0.6858  mix_decode.d6.loss_cls: 0.3028  mix_decode.d6.loss_mask: 0.4260  mix_decode.d6.loss_dice: 0.6841  mix_decode.d7.loss_cls: 0.3206  mix_decode.d7.loss_mask: 0.4104  mix_decode.d7.loss_dice: 0.6487  mix_decode.d8.loss_cls: 0.2972  mix_decode.d8.loss_mask: 0.4268  mix_decode.d8.loss_dice: 0.6526
2025/03/29 15:15:24 - mmengine - INFO - Iter(train) [11350/20000]  base_lr: 4.7033e-05 lr: 4.7033e-05  eta: 2:15:26  time: 1.1562  data_time: 0.0253  memory: 11214  loss: 64.3666  decode.loss_cls: 0.2665  decode.loss_mask: 2.2515  decode.loss_dice: 1.8922  decode.d0.loss_cls: 0.4331  decode.d0.loss_mask: 2.3497  decode.d0.loss_dice: 1.9624  decode.d1.loss_cls: 0.3895  decode.d1.loss_mask: 2.2989  decode.d1.loss_dice: 1.8338  decode.d2.loss_cls: 0.4201  decode.d2.loss_mask: 2.2546  decode.d2.loss_dice: 1.8334  decode.d3.loss_cls: 0.4179  decode.d3.loss_mask: 2.1749  decode.d3.loss_dice: 1.8083  decode.d4.loss_cls: 0.3532  decode.d4.loss_mask: 2.2772  decode.d4.loss_dice: 1.8513  decode.d5.loss_cls: 0.3961  decode.d5.loss_mask: 2.2524  decode.d5.loss_dice: 1.8354  decode.d6.loss_cls: 0.2546  decode.d6.loss_mask: 2.3089  decode.d6.loss_dice: 1.8863  decode.d7.loss_cls: 0.3449  decode.d7.loss_mask: 2.2687  decode.d7.loss_dice: 1.8455  decode.d8.loss_cls: 0.3130  decode.d8.loss_mask: 2.2666  decode.d8.loss_dice: 1.8908  mix_decode.loss_cls: 0.3083  mix_decode.loss_mask: 0.7761  mix_decode.loss_dice: 0.8342  mix_decode.d0.loss_cls: 0.4295  mix_decode.d0.loss_mask: 0.7646  mix_decode.d0.loss_dice: 0.8743  mix_decode.d1.loss_cls: 0.3619  mix_decode.d1.loss_mask: 0.7559  mix_decode.d1.loss_dice: 0.8264  mix_decode.d2.loss_cls: 0.3299  mix_decode.d2.loss_mask: 0.7688  mix_decode.d2.loss_dice: 0.8175  mix_decode.d3.loss_cls: 0.3188  mix_decode.d3.loss_mask: 0.7729  mix_decode.d3.loss_dice: 0.8373  mix_decode.d4.loss_cls: 0.2849  mix_decode.d4.loss_mask: 0.8034  mix_decode.d4.loss_dice: 0.8687  mix_decode.d5.loss_cls: 0.3123  mix_decode.d5.loss_mask: 0.7430  mix_decode.d5.loss_dice: 0.8374  mix_decode.d6.loss_cls: 0.3099  mix_decode.d6.loss_mask: 0.7559  mix_decode.d6.loss_dice: 0.8489  mix_decode.d7.loss_cls: 0.3560  mix_decode.d7.loss_mask: 0.7745  mix_decode.d7.loss_dice: 0.8301  mix_decode.d8.loss_cls: 0.3115  mix_decode.d8.loss_mask: 0.7833  mix_decode.d8.loss_dice: 0.8385
2025/03/29 15:16:22 - mmengine - INFO - Iter(train) [11400/20000]  base_lr: 4.6789e-05 lr: 4.6789e-05  eta: 2:14:47  time: 1.1531  data_time: 0.0227  memory: 11213  loss: 54.7644  decode.loss_cls: 0.2689  decode.loss_mask: 1.8835  decode.loss_dice: 1.6097  decode.d0.loss_cls: 0.3824  decode.d0.loss_mask: 1.8822  decode.d0.loss_dice: 1.5972  decode.d1.loss_cls: 0.2290  decode.d1.loss_mask: 1.8811  decode.d1.loss_dice: 1.6082  decode.d2.loss_cls: 0.2525  decode.d2.loss_mask: 1.8441  decode.d2.loss_dice: 1.6235  decode.d3.loss_cls: 0.2598  decode.d3.loss_mask: 1.8940  decode.d3.loss_dice: 1.5925  decode.d4.loss_cls: 0.2555  decode.d4.loss_mask: 1.8696  decode.d4.loss_dice: 1.5900  decode.d5.loss_cls: 0.2387  decode.d5.loss_mask: 1.8907  decode.d5.loss_dice: 1.6251  decode.d6.loss_cls: 0.2868  decode.d6.loss_mask: 1.8649  decode.d6.loss_dice: 1.6253  decode.d7.loss_cls: 0.2895  decode.d7.loss_mask: 1.8645  decode.d7.loss_dice: 1.6158  decode.d8.loss_cls: 0.2688  decode.d8.loss_mask: 1.8702  decode.d8.loss_dice: 1.6005  mix_decode.loss_cls: 0.3041  mix_decode.loss_mask: 0.6549  mix_decode.loss_dice: 0.7476  mix_decode.d0.loss_cls: 0.3576  mix_decode.d0.loss_mask: 0.6594  mix_decode.d0.loss_dice: 0.7678  mix_decode.d1.loss_cls: 0.2829  mix_decode.d1.loss_mask: 0.6477  mix_decode.d1.loss_dice: 0.7472  mix_decode.d2.loss_cls: 0.3156  mix_decode.d2.loss_mask: 0.6698  mix_decode.d2.loss_dice: 0.7473  mix_decode.d3.loss_cls: 0.2819  mix_decode.d3.loss_mask: 0.6479  mix_decode.d3.loss_dice: 0.7304  mix_decode.d4.loss_cls: 0.3256  mix_decode.d4.loss_mask: 0.6285  mix_decode.d4.loss_dice: 0.7191  mix_decode.d5.loss_cls: 0.3353  mix_decode.d5.loss_mask: 0.6501  mix_decode.d5.loss_dice: 0.7473  mix_decode.d6.loss_cls: 0.3554  mix_decode.d6.loss_mask: 0.6579  mix_decode.d6.loss_dice: 0.7576  mix_decode.d7.loss_cls: 0.3479  mix_decode.d7.loss_mask: 0.6393  mix_decode.d7.loss_dice: 0.7440  mix_decode.d8.loss_cls: 0.3604  mix_decode.d8.loss_mask: 0.6482  mix_decode.d8.loss_dice: 0.7211
2025/03/29 15:17:20 - mmengine - INFO - Iter(train) [11450/20000]  base_lr: 4.6544e-05 lr: 4.6544e-05  eta: 2:14:08  time: 1.1624  data_time: 0.0236  memory: 11224  loss: 62.3571  decode.loss_cls: 0.5174  decode.loss_mask: 1.9113  decode.loss_dice: 1.7671  decode.d0.loss_cls: 0.5758  decode.d0.loss_mask: 2.0451  decode.d0.loss_dice: 1.8211  decode.d1.loss_cls: 0.5442  decode.d1.loss_mask: 1.9912  decode.d1.loss_dice: 1.7681  decode.d2.loss_cls: 0.5133  decode.d2.loss_mask: 1.9750  decode.d2.loss_dice: 1.8113  decode.d3.loss_cls: 0.4703  decode.d3.loss_mask: 1.9607  decode.d3.loss_dice: 1.8346  decode.d4.loss_cls: 0.4786  decode.d4.loss_mask: 1.9622  decode.d4.loss_dice: 1.8406  decode.d5.loss_cls: 0.4284  decode.d5.loss_mask: 2.0241  decode.d5.loss_dice: 1.8307  decode.d6.loss_cls: 0.5078  decode.d6.loss_mask: 2.0517  decode.d6.loss_dice: 1.8923  decode.d7.loss_cls: 0.5418  decode.d7.loss_mask: 1.9795  decode.d7.loss_dice: 1.7693  decode.d8.loss_cls: 0.5653  decode.d8.loss_mask: 1.9608  decode.d8.loss_dice: 1.7733  mix_decode.loss_cls: 0.3344  mix_decode.loss_mask: 0.7355  mix_decode.loss_dice: 0.8892  mix_decode.d0.loss_cls: 0.3556  mix_decode.d0.loss_mask: 0.7037  mix_decode.d0.loss_dice: 0.9463  mix_decode.d1.loss_cls: 0.3230  mix_decode.d1.loss_mask: 0.6993  mix_decode.d1.loss_dice: 0.8584  mix_decode.d2.loss_cls: 0.3245  mix_decode.d2.loss_mask: 0.6958  mix_decode.d2.loss_dice: 0.8548  mix_decode.d3.loss_cls: 0.3350  mix_decode.d3.loss_mask: 0.6986  mix_decode.d3.loss_dice: 0.8542  mix_decode.d4.loss_cls: 0.2999  mix_decode.d4.loss_mask: 0.6865  mix_decode.d4.loss_dice: 0.8791  mix_decode.d5.loss_cls: 0.3605  mix_decode.d5.loss_mask: 0.6893  mix_decode.d5.loss_dice: 0.8511  mix_decode.d6.loss_cls: 0.3660  mix_decode.d6.loss_mask: 0.6985  mix_decode.d6.loss_dice: 0.8910  mix_decode.d7.loss_cls: 0.4163  mix_decode.d7.loss_mask: 0.7017  mix_decode.d7.loss_dice: 0.8820  mix_decode.d8.loss_cls: 0.3607  mix_decode.d8.loss_mask: 0.6924  mix_decode.d8.loss_dice: 0.8608
2025/03/29 15:18:17 - mmengine - INFO - Iter(train) [11500/20000]  base_lr: 4.6299e-05 lr: 4.6299e-05  eta: 2:13:29  time: 1.1450  data_time: 0.0227  memory: 11222  loss: 52.9634  decode.loss_cls: 0.2788  decode.loss_mask: 1.6490  decode.loss_dice: 1.7770  decode.d0.loss_cls: 0.4263  decode.d0.loss_mask: 1.6934  decode.d0.loss_dice: 1.8154  decode.d1.loss_cls: 0.3943  decode.d1.loss_mask: 1.6447  decode.d1.loss_dice: 1.7862  decode.d2.loss_cls: 0.3428  decode.d2.loss_mask: 1.6823  decode.d2.loss_dice: 1.8217  decode.d3.loss_cls: 0.3360  decode.d3.loss_mask: 1.7148  decode.d3.loss_dice: 1.8429  decode.d4.loss_cls: 0.3748  decode.d4.loss_mask: 1.6438  decode.d4.loss_dice: 1.7956  decode.d5.loss_cls: 0.2969  decode.d5.loss_mask: 1.6863  decode.d5.loss_dice: 1.8376  decode.d6.loss_cls: 0.3547  decode.d6.loss_mask: 1.6833  decode.d6.loss_dice: 1.8160  decode.d7.loss_cls: 0.3173  decode.d7.loss_mask: 1.6818  decode.d7.loss_dice: 1.7878  decode.d8.loss_cls: 0.2950  decode.d8.loss_mask: 1.6600  decode.d8.loss_dice: 1.8298  mix_decode.loss_cls: 0.1595  mix_decode.loss_mask: 0.6378  mix_decode.loss_dice: 0.6510  mix_decode.d0.loss_cls: 0.2273  mix_decode.d0.loss_mask: 0.6248  mix_decode.d0.loss_dice: 0.6833  mix_decode.d1.loss_cls: 0.1624  mix_decode.d1.loss_mask: 0.6344  mix_decode.d1.loss_dice: 0.6673  mix_decode.d2.loss_cls: 0.1472  mix_decode.d2.loss_mask: 0.6549  mix_decode.d2.loss_dice: 0.6438  mix_decode.d3.loss_cls: 0.1149  mix_decode.d3.loss_mask: 0.6452  mix_decode.d3.loss_dice: 0.6654  mix_decode.d4.loss_cls: 0.1919  mix_decode.d4.loss_mask: 0.6315  mix_decode.d4.loss_dice: 0.6559  mix_decode.d5.loss_cls: 0.1679  mix_decode.d5.loss_mask: 0.6414  mix_decode.d5.loss_dice: 0.6624  mix_decode.d6.loss_cls: 0.1598  mix_decode.d6.loss_mask: 0.6443  mix_decode.d6.loss_dice: 0.6610  mix_decode.d7.loss_cls: 0.1545  mix_decode.d7.loss_mask: 0.6620  mix_decode.d7.loss_dice: 0.6738  mix_decode.d8.loss_cls: 0.1681  mix_decode.d8.loss_mask: 0.6419  mix_decode.d8.loss_dice: 0.6616
2025/03/29 15:19:15 - mmengine - INFO - Iter(train) [11550/20000]  base_lr: 4.6054e-05 lr: 4.6054e-05  eta: 2:12:50  time: 1.1643  data_time: 0.0251  memory: 11217  loss: 52.3596  decode.loss_cls: 0.2305  decode.loss_mask: 1.9009  decode.loss_dice: 1.5023  decode.d0.loss_cls: 0.4642  decode.d0.loss_mask: 1.9447  decode.d0.loss_dice: 1.5548  decode.d1.loss_cls: 0.3426  decode.d1.loss_mask: 1.8883  decode.d1.loss_dice: 1.5094  decode.d2.loss_cls: 0.2680  decode.d2.loss_mask: 1.9163  decode.d2.loss_dice: 1.4973  decode.d3.loss_cls: 0.2629  decode.d3.loss_mask: 1.8923  decode.d3.loss_dice: 1.5068  decode.d4.loss_cls: 0.3071  decode.d4.loss_mask: 1.8521  decode.d4.loss_dice: 1.4907  decode.d5.loss_cls: 0.2559  decode.d5.loss_mask: 1.8772  decode.d5.loss_dice: 1.5052  decode.d6.loss_cls: 0.3342  decode.d6.loss_mask: 1.8749  decode.d6.loss_dice: 1.5288  decode.d7.loss_cls: 0.2768  decode.d7.loss_mask: 1.8472  decode.d7.loss_dice: 1.4796  decode.d8.loss_cls: 0.3156  decode.d8.loss_mask: 1.8711  decode.d8.loss_dice: 1.4676  mix_decode.loss_cls: 0.2448  mix_decode.loss_mask: 0.5789  mix_decode.loss_dice: 0.7375  mix_decode.d0.loss_cls: 0.3294  mix_decode.d0.loss_mask: 0.5622  mix_decode.d0.loss_dice: 0.7716  mix_decode.d1.loss_cls: 0.2528  mix_decode.d1.loss_mask: 0.5587  mix_decode.d1.loss_dice: 0.7364  mix_decode.d2.loss_cls: 0.2482  mix_decode.d2.loss_mask: 0.5685  mix_decode.d2.loss_dice: 0.7269  mix_decode.d3.loss_cls: 0.2378  mix_decode.d3.loss_mask: 0.5473  mix_decode.d3.loss_dice: 0.7120  mix_decode.d4.loss_cls: 0.1991  mix_decode.d4.loss_mask: 0.5498  mix_decode.d4.loss_dice: 0.7249  mix_decode.d5.loss_cls: 0.2015  mix_decode.d5.loss_mask: 0.5521  mix_decode.d5.loss_dice: 0.7406  mix_decode.d6.loss_cls: 0.2222  mix_decode.d6.loss_mask: 0.5548  mix_decode.d6.loss_dice: 0.7320  mix_decode.d7.loss_cls: 0.2519  mix_decode.d7.loss_mask: 0.5549  mix_decode.d7.loss_dice: 0.7379  mix_decode.d8.loss_cls: 0.2466  mix_decode.d8.loss_mask: 0.5698  mix_decode.d8.loss_dice: 0.7432
2025/03/29 15:20:13 - mmengine - INFO - Iter(train) [11600/20000]  base_lr: 4.5808e-05 lr: 4.5808e-05  eta: 2:12:10  time: 1.1522  data_time: 0.0222  memory: 11227  loss: 58.2030  decode.loss_cls: 0.4419  decode.loss_mask: 1.9129  decode.loss_dice: 1.6989  decode.d0.loss_cls: 0.5080  decode.d0.loss_mask: 1.9835  decode.d0.loss_dice: 1.6988  decode.d1.loss_cls: 0.3973  decode.d1.loss_mask: 1.9109  decode.d1.loss_dice: 1.6685  decode.d2.loss_cls: 0.4382  decode.d2.loss_mask: 1.8768  decode.d2.loss_dice: 1.6722  decode.d3.loss_cls: 0.3396  decode.d3.loss_mask: 1.9057  decode.d3.loss_dice: 1.6906  decode.d4.loss_cls: 0.3857  decode.d4.loss_mask: 1.8768  decode.d4.loss_dice: 1.6927  decode.d5.loss_cls: 0.3723  decode.d5.loss_mask: 1.8945  decode.d5.loss_dice: 1.7294  decode.d6.loss_cls: 0.3261  decode.d6.loss_mask: 1.9171  decode.d6.loss_dice: 1.7039  decode.d7.loss_cls: 0.4512  decode.d7.loss_mask: 1.8760  decode.d7.loss_dice: 1.6819  decode.d8.loss_cls: 0.4761  decode.d8.loss_mask: 1.9191  decode.d8.loss_dice: 1.6838  mix_decode.loss_cls: 0.2195  mix_decode.loss_mask: 0.7320  mix_decode.loss_dice: 0.7911  mix_decode.d0.loss_cls: 0.3377  mix_decode.d0.loss_mask: 0.7582  mix_decode.d0.loss_dice: 0.8485  mix_decode.d1.loss_cls: 0.2653  mix_decode.d1.loss_mask: 0.7701  mix_decode.d1.loss_dice: 0.8214  mix_decode.d2.loss_cls: 0.2702  mix_decode.d2.loss_mask: 0.7102  mix_decode.d2.loss_dice: 0.7679  mix_decode.d3.loss_cls: 0.2444  mix_decode.d3.loss_mask: 0.7179  mix_decode.d3.loss_dice: 0.8022  mix_decode.d4.loss_cls: 0.2503  mix_decode.d4.loss_mask: 0.7454  mix_decode.d4.loss_dice: 0.8143  mix_decode.d5.loss_cls: 0.2650  mix_decode.d5.loss_mask: 0.6969  mix_decode.d5.loss_dice: 0.8050  mix_decode.d6.loss_cls: 0.2422  mix_decode.d6.loss_mask: 0.7597  mix_decode.d6.loss_dice: 0.8444  mix_decode.d7.loss_cls: 0.2566  mix_decode.d7.loss_mask: 0.7313  mix_decode.d7.loss_dice: 0.8247  mix_decode.d8.loss_cls: 0.2434  mix_decode.d8.loss_mask: 0.7280  mix_decode.d8.loss_dice: 0.8087
2025/03/29 15:21:10 - mmengine - INFO - Iter(train) [11650/20000]  base_lr: 4.5563e-05 lr: 4.5563e-05  eta: 2:11:30  time: 1.1460  data_time: 0.0224  memory: 11216  loss: 58.9306  decode.loss_cls: 0.5072  decode.loss_mask: 1.9792  decode.loss_dice: 1.8198  decode.d0.loss_cls: 0.5871  decode.d0.loss_mask: 1.9912  decode.d0.loss_dice: 1.8837  decode.d1.loss_cls: 0.5226  decode.d1.loss_mask: 1.9444  decode.d1.loss_dice: 1.7726  decode.d2.loss_cls: 0.5069  decode.d2.loss_mask: 1.9621  decode.d2.loss_dice: 1.7243  decode.d3.loss_cls: 0.4899  decode.d3.loss_mask: 1.9883  decode.d3.loss_dice: 1.7879  decode.d4.loss_cls: 0.5209  decode.d4.loss_mask: 1.9758  decode.d4.loss_dice: 1.8501  decode.d5.loss_cls: 0.5951  decode.d5.loss_mask: 1.9675  decode.d5.loss_dice: 1.7874  decode.d6.loss_cls: 0.6334  decode.d6.loss_mask: 1.9842  decode.d6.loss_dice: 1.7945  decode.d7.loss_cls: 0.6061  decode.d7.loss_mask: 1.9844  decode.d7.loss_dice: 1.8460  decode.d8.loss_cls: 0.5112  decode.d8.loss_mask: 1.9859  decode.d8.loss_dice: 1.8188  mix_decode.loss_cls: 0.2173  mix_decode.loss_mask: 0.6275  mix_decode.loss_dice: 0.6840  mix_decode.d0.loss_cls: 0.2903  mix_decode.d0.loss_mask: 0.6255  mix_decode.d0.loss_dice: 0.7054  mix_decode.d1.loss_cls: 0.2281  mix_decode.d1.loss_mask: 0.6446  mix_decode.d1.loss_dice: 0.6797  mix_decode.d2.loss_cls: 0.2551  mix_decode.d2.loss_mask: 0.6478  mix_decode.d2.loss_dice: 0.6708  mix_decode.d3.loss_cls: 0.1998  mix_decode.d3.loss_mask: 0.6423  mix_decode.d3.loss_dice: 0.6849  mix_decode.d4.loss_cls: 0.2037  mix_decode.d4.loss_mask: 0.6446  mix_decode.d4.loss_dice: 0.6879  mix_decode.d5.loss_cls: 0.2308  mix_decode.d5.loss_mask: 0.6422  mix_decode.d5.loss_dice: 0.6706  mix_decode.d6.loss_cls: 0.2503  mix_decode.d6.loss_mask: 0.6384  mix_decode.d6.loss_dice: 0.6870  mix_decode.d7.loss_cls: 0.2183  mix_decode.d7.loss_mask: 0.6429  mix_decode.d7.loss_dice: 0.7128  mix_decode.d8.loss_cls: 0.2357  mix_decode.d8.loss_mask: 0.6425  mix_decode.d8.loss_dice: 0.6911
2025/03/29 15:22:08 - mmengine - INFO - Iter(train) [11700/20000]  base_lr: 4.5317e-05 lr: 4.5317e-05  eta: 2:10:50  time: 1.1568  data_time: 0.0232  memory: 11216  loss: 57.9539  decode.loss_cls: 0.1401  decode.loss_mask: 2.1122  decode.loss_dice: 1.8850  decode.d0.loss_cls: 0.3432  decode.d0.loss_mask: 2.1239  decode.d0.loss_dice: 1.8172  decode.d1.loss_cls: 0.1481  decode.d1.loss_mask: 2.1584  decode.d1.loss_dice: 1.8772  decode.d2.loss_cls: 0.1475  decode.d2.loss_mask: 2.1457  decode.d2.loss_dice: 1.8944  decode.d3.loss_cls: 0.1470  decode.d3.loss_mask: 2.1219  decode.d3.loss_dice: 1.8541  decode.d4.loss_cls: 0.1792  decode.d4.loss_mask: 2.1063  decode.d4.loss_dice: 1.8780  decode.d5.loss_cls: 0.2106  decode.d5.loss_mask: 2.0763  decode.d5.loss_dice: 1.8496  decode.d6.loss_cls: 0.2340  decode.d6.loss_mask: 2.0923  decode.d6.loss_dice: 1.8571  decode.d7.loss_cls: 0.1770  decode.d7.loss_mask: 2.1219  decode.d7.loss_dice: 1.8518  decode.d8.loss_cls: 0.1957  decode.d8.loss_mask: 2.1098  decode.d8.loss_dice: 1.8848  mix_decode.loss_cls: 0.2023  mix_decode.loss_mask: 0.6926  mix_decode.loss_dice: 0.6825  mix_decode.d0.loss_cls: 0.2490  mix_decode.d0.loss_mask: 0.6975  mix_decode.d0.loss_dice: 0.7301  mix_decode.d1.loss_cls: 0.1935  mix_decode.d1.loss_mask: 0.7075  mix_decode.d1.loss_dice: 0.7330  mix_decode.d2.loss_cls: 0.1882  mix_decode.d2.loss_mask: 0.7126  mix_decode.d2.loss_dice: 0.7063  mix_decode.d3.loss_cls: 0.1849  mix_decode.d3.loss_mask: 0.6960  mix_decode.d3.loss_dice: 0.6974  mix_decode.d4.loss_cls: 0.1815  mix_decode.d4.loss_mask: 0.7103  mix_decode.d4.loss_dice: 0.7129  mix_decode.d5.loss_cls: 0.1877  mix_decode.d5.loss_mask: 0.7171  mix_decode.d5.loss_dice: 0.7414  mix_decode.d6.loss_cls: 0.2557  mix_decode.d6.loss_mask: 0.6843  mix_decode.d6.loss_dice: 0.7076  mix_decode.d7.loss_cls: 0.1932  mix_decode.d7.loss_mask: 0.6993  mix_decode.d7.loss_dice: 0.7119  mix_decode.d8.loss_cls: 0.2627  mix_decode.d8.loss_mask: 0.6850  mix_decode.d8.loss_dice: 0.6898
2025/03/29 15:23:05 - mmengine - INFO - Iter(train) [11750/20000]  base_lr: 4.5071e-05 lr: 4.5071e-05  eta: 2:10:10  time: 1.1476  data_time: 0.0224  memory: 11208  loss: 59.6207  decode.loss_cls: 0.5024  decode.loss_mask: 1.9460  decode.loss_dice: 1.7237  decode.d0.loss_cls: 0.5789  decode.d0.loss_mask: 1.9786  decode.d0.loss_dice: 1.8095  decode.d1.loss_cls: 0.4767  decode.d1.loss_mask: 1.9941  decode.d1.loss_dice: 1.8244  decode.d2.loss_cls: 0.4704  decode.d2.loss_mask: 1.9747  decode.d2.loss_dice: 1.8101  decode.d3.loss_cls: 0.4805  decode.d3.loss_mask: 1.9628  decode.d3.loss_dice: 1.8014  decode.d4.loss_cls: 0.4140  decode.d4.loss_mask: 2.0043  decode.d4.loss_dice: 1.8154  decode.d5.loss_cls: 0.4261  decode.d5.loss_mask: 2.0473  decode.d5.loss_dice: 1.7586  decode.d6.loss_cls: 0.4631  decode.d6.loss_mask: 2.0633  decode.d6.loss_dice: 1.7849  decode.d7.loss_cls: 0.4609  decode.d7.loss_mask: 2.0206  decode.d7.loss_dice: 1.7675  decode.d8.loss_cls: 0.4120  decode.d8.loss_mask: 1.9889  decode.d8.loss_dice: 1.7785  mix_decode.loss_cls: 0.3145  mix_decode.loss_mask: 0.6288  mix_decode.loss_dice: 0.7443  mix_decode.d0.loss_cls: 0.4044  mix_decode.d0.loss_mask: 0.6223  mix_decode.d0.loss_dice: 0.7636  mix_decode.d1.loss_cls: 0.2784  mix_decode.d1.loss_mask: 0.6374  mix_decode.d1.loss_dice: 0.7454  mix_decode.d2.loss_cls: 0.3091  mix_decode.d2.loss_mask: 0.6120  mix_decode.d2.loss_dice: 0.7458  mix_decode.d3.loss_cls: 0.3065  mix_decode.d3.loss_mask: 0.6178  mix_decode.d3.loss_dice: 0.7395  mix_decode.d4.loss_cls: 0.3436  mix_decode.d4.loss_mask: 0.6216  mix_decode.d4.loss_dice: 0.7356  mix_decode.d5.loss_cls: 0.3123  mix_decode.d5.loss_mask: 0.6818  mix_decode.d5.loss_dice: 0.7934  mix_decode.d6.loss_cls: 0.2527  mix_decode.d6.loss_mask: 0.6435  mix_decode.d6.loss_dice: 0.7822  mix_decode.d7.loss_cls: 0.3457  mix_decode.d7.loss_mask: 0.6486  mix_decode.d7.loss_dice: 0.7300  mix_decode.d8.loss_cls: 0.3011  mix_decode.d8.loss_mask: 0.6664  mix_decode.d8.loss_dice: 0.7524
2025/03/29 15:24:03 - mmengine - INFO - Iter(train) [11800/20000]  base_lr: 4.4825e-05 lr: 4.4825e-05  eta: 2:09:30  time: 1.1506  data_time: 0.0224  memory: 11212  loss: 56.1069  decode.loss_cls: 0.2781  decode.loss_mask: 2.0062  decode.loss_dice: 1.7126  decode.d0.loss_cls: 0.4677  decode.d0.loss_mask: 1.9772  decode.d0.loss_dice: 1.7141  decode.d1.loss_cls: 0.2761  decode.d1.loss_mask: 2.0014  decode.d1.loss_dice: 1.7232  decode.d2.loss_cls: 0.2146  decode.d2.loss_mask: 1.9892  decode.d2.loss_dice: 1.7392  decode.d3.loss_cls: 0.2345  decode.d3.loss_mask: 2.0004  decode.d3.loss_dice: 1.7693  decode.d4.loss_cls: 0.1603  decode.d4.loss_mask: 2.0236  decode.d4.loss_dice: 1.7564  decode.d5.loss_cls: 0.1976  decode.d5.loss_mask: 2.0096  decode.d5.loss_dice: 1.7374  decode.d6.loss_cls: 0.2062  decode.d6.loss_mask: 2.0061  decode.d6.loss_dice: 1.7160  decode.d7.loss_cls: 0.2566  decode.d7.loss_mask: 2.0058  decode.d7.loss_dice: 1.7457  decode.d8.loss_cls: 0.2685  decode.d8.loss_mask: 1.9958  decode.d8.loss_dice: 1.7326  mix_decode.loss_cls: 0.2262  mix_decode.loss_mask: 0.6249  mix_decode.loss_dice: 0.7188  mix_decode.d0.loss_cls: 0.3089  mix_decode.d0.loss_mask: 0.6212  mix_decode.d0.loss_dice: 0.8194  mix_decode.d1.loss_cls: 0.1868  mix_decode.d1.loss_mask: 0.6616  mix_decode.d1.loss_dice: 0.7478  mix_decode.d2.loss_cls: 0.1879  mix_decode.d2.loss_mask: 0.6622  mix_decode.d2.loss_dice: 0.7378  mix_decode.d3.loss_cls: 0.1779  mix_decode.d3.loss_mask: 0.6528  mix_decode.d3.loss_dice: 0.7431  mix_decode.d4.loss_cls: 0.1510  mix_decode.d4.loss_mask: 0.6562  mix_decode.d4.loss_dice: 0.7773  mix_decode.d5.loss_cls: 0.2085  mix_decode.d5.loss_mask: 0.6518  mix_decode.d5.loss_dice: 0.7706  mix_decode.d6.loss_cls: 0.1915  mix_decode.d6.loss_mask: 0.6588  mix_decode.d6.loss_dice: 0.7532  mix_decode.d7.loss_cls: 0.2825  mix_decode.d7.loss_mask: 0.6180  mix_decode.d7.loss_dice: 0.7479  mix_decode.d8.loss_cls: 0.2355  mix_decode.d8.loss_mask: 0.6406  mix_decode.d8.loss_dice: 0.7638
2025/03/29 15:25:01 - mmengine - INFO - Iter(train) [11850/20000]  base_lr: 4.4579e-05 lr: 4.4579e-05  eta: 2:08:50  time: 1.1520  data_time: 0.0230  memory: 11207  loss: 59.1120  decode.loss_cls: 0.2985  decode.loss_mask: 1.9647  decode.loss_dice: 1.8097  decode.d0.loss_cls: 0.4173  decode.d0.loss_mask: 1.9819  decode.d0.loss_dice: 1.7610  decode.d1.loss_cls: 0.3004  decode.d1.loss_mask: 1.9348  decode.d1.loss_dice: 1.7521  decode.d2.loss_cls: 0.3402  decode.d2.loss_mask: 1.9250  decode.d2.loss_dice: 1.7636  decode.d3.loss_cls: 0.2586  decode.d3.loss_mask: 1.9754  decode.d3.loss_dice: 1.7550  decode.d4.loss_cls: 0.2569  decode.d4.loss_mask: 1.9716  decode.d4.loss_dice: 1.7501  decode.d5.loss_cls: 0.2525  decode.d5.loss_mask: 1.9899  decode.d5.loss_dice: 1.7898  decode.d6.loss_cls: 0.3263  decode.d6.loss_mask: 1.9861  decode.d6.loss_dice: 1.7653  decode.d7.loss_cls: 0.2860  decode.d7.loss_mask: 1.9934  decode.d7.loss_dice: 1.8086  decode.d8.loss_cls: 0.2922  decode.d8.loss_mask: 1.9554  decode.d8.loss_dice: 1.7730  mix_decode.loss_cls: 0.4231  mix_decode.loss_mask: 0.6549  mix_decode.loss_dice: 0.8380  mix_decode.d0.loss_cls: 0.3631  mix_decode.d0.loss_mask: 0.6232  mix_decode.d0.loss_dice: 0.8943  mix_decode.d1.loss_cls: 0.3686  mix_decode.d1.loss_mask: 0.6141  mix_decode.d1.loss_dice: 0.8615  mix_decode.d2.loss_cls: 0.3659  mix_decode.d2.loss_mask: 0.6230  mix_decode.d2.loss_dice: 0.8540  mix_decode.d3.loss_cls: 0.3434  mix_decode.d3.loss_mask: 0.6395  mix_decode.d3.loss_dice: 0.8776  mix_decode.d4.loss_cls: 0.3097  mix_decode.d4.loss_mask: 0.6910  mix_decode.d4.loss_dice: 0.8609  mix_decode.d5.loss_cls: 0.3503  mix_decode.d5.loss_mask: 0.6368  mix_decode.d5.loss_dice: 0.8300  mix_decode.d6.loss_cls: 0.3178  mix_decode.d6.loss_mask: 0.6597  mix_decode.d6.loss_dice: 0.8757  mix_decode.d7.loss_cls: 0.3758  mix_decode.d7.loss_mask: 0.6539  mix_decode.d7.loss_dice: 0.8721  mix_decode.d8.loss_cls: 0.3674  mix_decode.d8.loss_mask: 0.6387  mix_decode.d8.loss_dice: 0.8925
2025/03/29 15:25:58 - mmengine - INFO - Iter(train) [11900/20000]  base_lr: 4.4333e-05 lr: 4.4333e-05  eta: 2:08:09  time: 1.1517  data_time: 0.0222  memory: 11207  loss: 61.4635  decode.loss_cls: 0.4692  decode.loss_mask: 2.0325  decode.loss_dice: 1.7175  decode.d0.loss_cls: 0.5588  decode.d0.loss_mask: 2.0433  decode.d0.loss_dice: 1.8026  decode.d1.loss_cls: 0.4667  decode.d1.loss_mask: 2.0353  decode.d1.loss_dice: 1.7597  decode.d2.loss_cls: 0.4674  decode.d2.loss_mask: 2.0341  decode.d2.loss_dice: 1.7284  decode.d3.loss_cls: 0.5149  decode.d3.loss_mask: 1.9824  decode.d3.loss_dice: 1.7547  decode.d4.loss_cls: 0.3801  decode.d4.loss_mask: 2.0753  decode.d4.loss_dice: 1.8179  decode.d5.loss_cls: 0.5165  decode.d5.loss_mask: 1.9538  decode.d5.loss_dice: 1.7780  decode.d6.loss_cls: 0.5297  decode.d6.loss_mask: 2.0199  decode.d6.loss_dice: 1.7900  decode.d7.loss_cls: 0.4844  decode.d7.loss_mask: 2.0124  decode.d7.loss_dice: 1.7285  decode.d8.loss_cls: 0.5003  decode.d8.loss_mask: 2.0217  decode.d8.loss_dice: 1.7462  mix_decode.loss_cls: 0.2937  mix_decode.loss_mask: 0.7429  mix_decode.loss_dice: 0.7774  mix_decode.d0.loss_cls: 0.4049  mix_decode.d0.loss_mask: 0.6796  mix_decode.d0.loss_dice: 0.8580  mix_decode.d1.loss_cls: 0.3145  mix_decode.d1.loss_mask: 0.7230  mix_decode.d1.loss_dice: 0.7832  mix_decode.d2.loss_cls: 0.3230  mix_decode.d2.loss_mask: 0.7419  mix_decode.d2.loss_dice: 0.7665  mix_decode.d3.loss_cls: 0.3465  mix_decode.d3.loss_mask: 0.7224  mix_decode.d3.loss_dice: 0.7641  mix_decode.d4.loss_cls: 0.2950  mix_decode.d4.loss_mask: 0.7287  mix_decode.d4.loss_dice: 0.8356  mix_decode.d5.loss_cls: 0.3836  mix_decode.d5.loss_mask: 0.7130  mix_decode.d5.loss_dice: 0.8058  mix_decode.d6.loss_cls: 0.3433  mix_decode.d6.loss_mask: 0.7560  mix_decode.d6.loss_dice: 0.8175  mix_decode.d7.loss_cls: 0.3595  mix_decode.d7.loss_mask: 0.7518  mix_decode.d7.loss_dice: 0.8086  mix_decode.d8.loss_cls: 0.3535  mix_decode.d8.loss_mask: 0.7618  mix_decode.d8.loss_dice: 0.7860
2025/03/29 15:26:56 - mmengine - INFO - Iter(train) [11950/20000]  base_lr: 4.4087e-05 lr: 4.4087e-05  eta: 2:07:29  time: 1.1504  data_time: 0.0223  memory: 11217  loss: 55.7923  decode.loss_cls: 0.4544  decode.loss_mask: 1.9726  decode.loss_dice: 1.7014  decode.d0.loss_cls: 0.6148  decode.d0.loss_mask: 1.9675  decode.d0.loss_dice: 1.7131  decode.d1.loss_cls: 0.4925  decode.d1.loss_mask: 2.0081  decode.d1.loss_dice: 1.6952  decode.d2.loss_cls: 0.4892  decode.d2.loss_mask: 1.9322  decode.d2.loss_dice: 1.6808  decode.d3.loss_cls: 0.4483  decode.d3.loss_mask: 2.0899  decode.d3.loss_dice: 1.6941  decode.d4.loss_cls: 0.4525  decode.d4.loss_mask: 2.0522  decode.d4.loss_dice: 1.6922  decode.d5.loss_cls: 0.4300  decode.d5.loss_mask: 2.0259  decode.d5.loss_dice: 1.7188  decode.d6.loss_cls: 0.4667  decode.d6.loss_mask: 2.0277  decode.d6.loss_dice: 1.7070  decode.d7.loss_cls: 0.4760  decode.d7.loss_mask: 1.9553  decode.d7.loss_dice: 1.7028  decode.d8.loss_cls: 0.5208  decode.d8.loss_mask: 2.0507  decode.d8.loss_dice: 1.6838  mix_decode.loss_cls: 0.1420  mix_decode.loss_mask: 0.5634  mix_decode.loss_dice: 0.6500  mix_decode.d0.loss_cls: 0.2817  mix_decode.d0.loss_mask: 0.5585  mix_decode.d0.loss_dice: 0.6807  mix_decode.d1.loss_cls: 0.1483  mix_decode.d1.loss_mask: 0.5509  mix_decode.d1.loss_dice: 0.6643  mix_decode.d2.loss_cls: 0.1447  mix_decode.d2.loss_mask: 0.5625  mix_decode.d2.loss_dice: 0.6398  mix_decode.d3.loss_cls: 0.1220  mix_decode.d3.loss_mask: 0.5674  mix_decode.d3.loss_dice: 0.6286  mix_decode.d4.loss_cls: 0.1630  mix_decode.d4.loss_mask: 0.5580  mix_decode.d4.loss_dice: 0.6525  mix_decode.d5.loss_cls: 0.1350  mix_decode.d5.loss_mask: 0.5717  mix_decode.d5.loss_dice: 0.6719  mix_decode.d6.loss_cls: 0.1612  mix_decode.d6.loss_mask: 0.5529  mix_decode.d6.loss_dice: 0.6709  mix_decode.d7.loss_cls: 0.1551  mix_decode.d7.loss_mask: 0.5606  mix_decode.d7.loss_dice: 0.6801  mix_decode.d8.loss_cls: 0.1444  mix_decode.d8.loss_mask: 0.6035  mix_decode.d8.loss_dice: 0.6902
2025/03/29 15:27:54 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 15:27:54 - mmengine - INFO - Iter(train) [12000/20000]  base_lr: 4.3840e-05 lr: 4.3840e-05  eta: 2:06:48  time: 1.1590  data_time: 0.0235  memory: 11216  loss: 60.9044  decode.loss_cls: 0.4875  decode.loss_mask: 1.9070  decode.loss_dice: 1.7916  decode.d0.loss_cls: 0.6960  decode.d0.loss_mask: 1.9724  decode.d0.loss_dice: 1.8592  decode.d1.loss_cls: 0.4183  decode.d1.loss_mask: 1.9388  decode.d1.loss_dice: 1.7462  decode.d2.loss_cls: 0.4430  decode.d2.loss_mask: 1.8462  decode.d2.loss_dice: 1.7033  decode.d3.loss_cls: 0.4601  decode.d3.loss_mask: 1.8858  decode.d3.loss_dice: 1.7715  decode.d4.loss_cls: 0.4051  decode.d4.loss_mask: 1.9434  decode.d4.loss_dice: 1.7406  decode.d5.loss_cls: 0.4222  decode.d5.loss_mask: 1.9642  decode.d5.loss_dice: 1.7696  decode.d6.loss_cls: 0.4467  decode.d6.loss_mask: 1.9367  decode.d6.loss_dice: 1.7841  decode.d7.loss_cls: 0.5191  decode.d7.loss_mask: 1.9494  decode.d7.loss_dice: 1.7380  decode.d8.loss_cls: 0.4996  decode.d8.loss_mask: 1.9042  decode.d8.loss_dice: 1.8605  mix_decode.loss_cls: 0.3636  mix_decode.loss_mask: 0.7156  mix_decode.loss_dice: 0.8731  mix_decode.d0.loss_cls: 0.3051  mix_decode.d0.loss_mask: 0.7016  mix_decode.d0.loss_dice: 0.9259  mix_decode.d1.loss_cls: 0.2866  mix_decode.d1.loss_mask: 0.7177  mix_decode.d1.loss_dice: 0.8731  mix_decode.d2.loss_cls: 0.3473  mix_decode.d2.loss_mask: 0.7110  mix_decode.d2.loss_dice: 0.8251  mix_decode.d3.loss_cls: 0.3385  mix_decode.d3.loss_mask: 0.7037  mix_decode.d3.loss_dice: 0.8741  mix_decode.d4.loss_cls: 0.3125  mix_decode.d4.loss_mask: 0.7096  mix_decode.d4.loss_dice: 0.8763  mix_decode.d5.loss_cls: 0.2807  mix_decode.d5.loss_mask: 0.7367  mix_decode.d5.loss_dice: 0.8867  mix_decode.d6.loss_cls: 0.2904  mix_decode.d6.loss_mask: 0.7032  mix_decode.d6.loss_dice: 0.9162  mix_decode.d7.loss_cls: 0.3320  mix_decode.d7.loss_mask: 0.6981  mix_decode.d7.loss_dice: 0.8854  mix_decode.d8.loss_cls: 0.3170  mix_decode.d8.loss_mask: 0.6953  mix_decode.d8.loss_dice: 0.8917
2025/03/29 15:27:54 - mmengine - INFO - Saving checkpoint at 12000 iterations
2025/03/29 15:27:59 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:05:59  time: 0.0912  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:04 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:54  time: 0.0912  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:08 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:49  time: 0.0911  data_time: 0.0018  memory: 3082  
2025/03/29 15:28:13 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:44  time: 0.0910  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:17 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:39  time: 0.0911  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:22 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:34  time: 0.0914  data_time: 0.0018  memory: 3082  
2025/03/29 15:28:27 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:30  time: 0.0913  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:31 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:25  time: 0.0913  data_time: 0.0018  memory: 3082  
2025/03/29 15:28:36 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:21  time: 0.0913  data_time: 0.0018  memory: 3082  
2025/03/29 15:28:40 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:16  time: 0.0913  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:45 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:12  time: 0.0913  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:49 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:07  time: 0.0916  data_time: 0.0017  memory: 3082  
2025/03/29 15:28:54 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:02  time: 0.0911  data_time: 0.0016  memory: 3082  
2025/03/29 15:28:59 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:58  time: 0.0915  data_time: 0.0019  memory: 3082  
2025/03/29 15:29:03 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:53  time: 0.0915  data_time: 0.0017  memory: 3082  
2025/03/29 15:29:08 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:49  time: 0.0915  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:12 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:44  time: 0.0914  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:17 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:40  time: 0.0914  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:22 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:35  time: 0.0942  data_time: 0.0019  memory: 3082  
2025/03/29 15:29:26 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:31  time: 0.0918  data_time: 0.0017  memory: 3082  
2025/03/29 15:29:31 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:26  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:35 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:22  time: 0.0930  data_time: 0.0019  memory: 3082  
2025/03/29 15:29:40 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:18  time: 0.0923  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:45 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:13  time: 0.0919  data_time: 0.0017  memory: 3082  
2025/03/29 15:29:49 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:08  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:54 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:04  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:29:58 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:59  time: 0.0920  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:03 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:55  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:08 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0914  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:12 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:46  time: 0.0917  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:17 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0913  data_time: 0.0017  memory: 3082  
2025/03/29 15:30:21 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:37  time: 0.0918  data_time: 0.0017  memory: 3082  
2025/03/29 15:30:26 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:31 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:28  time: 0.0915  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:35 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:23  time: 0.0914  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:40 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0914  data_time: 0.0017  memory: 3082  
2025/03/29 15:30:44 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:14  time: 0.0916  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:49 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0920  data_time: 0.0018  memory: 3082  
2025/03/29 15:30:54 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:05  time: 0.0917  data_time: 0.0017  memory: 3082  
2025/03/29 15:30:58 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:03 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:08 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:51  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:12 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:17 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:42  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:21 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:26 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:33  time: 0.0917  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:31 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0917  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:35 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0922  data_time: 0.0019  memory: 3082  
2025/03/29 15:31:40 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:19  time: 0.0920  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:44 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0921  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:49 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:10  time: 0.0943  data_time: 0.0020  memory: 3082  
2025/03/29 15:31:54 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0920  data_time: 0.0018  memory: 3082  
2025/03/29 15:31:58 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:01  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:32:03 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0918  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:08 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0917  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:12 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:47  time: 0.0920  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:17 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0917  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:21 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:38  time: 0.0918  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:26 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0918  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:31 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:32:35 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0919  data_time: 0.0017  memory: 3082  
2025/03/29 15:32:40 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0918  data_time: 0.0018  memory: 3082  
2025/03/29 15:32:44 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:15  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:32:49 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0920  data_time: 0.0018  memory: 3082  
2025/03/29 15:32:54 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:32:58 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0954  data_time: 0.0020  memory: 3082  
2025/03/29 15:33:03 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0920  data_time: 0.0019  memory: 3082  
2025/03/29 15:33:07 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0921  data_time: 0.0019  memory: 3082  
2025/03/29 15:33:12 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0920  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:17 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:43  time: 0.0919  data_time: 0.0017  memory: 3082  
2025/03/29 15:33:21 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0922  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:26 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0922  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:31 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0921  data_time: 0.0019  memory: 3082  
2025/03/29 15:33:35 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0922  data_time: 0.0019  memory: 3082  
2025/03/29 15:33:40 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:20  time: 0.0921  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:44 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0922  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:49 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0921  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:54 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0919  data_time: 0.0018  memory: 3082  
2025/03/29 15:33:58 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0957  data_time: 0.0022  memory: 3082  
2025/03/29 15:34:00 - mmengine - INFO - per class results:
2025/03/29 15:34:00 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 49.54 |  62.7 |
|   building   | 62.82 | 78.09 |
|     road     |  48.4 |  54.0 |
|    water     | 65.77 | 78.29 |
|    barren    | 16.11 | 36.61 |
|    forest    | 36.98 | 71.64 |
| agricultural | 65.26 |  79.1 |
+--------------+-------+-------+
2025/03/29 15:34:00 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 69.3600  mIoU: 49.2700  mAcc: 65.7800  data_time: 0.0018  time: 0.0919
2025/03/29 15:34:00 - mmengine - INFO - The previous best checkpoint /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0/best_mIoU_iter_10000.pth is removed
2025/03/29 15:34:01 - mmengine - INFO - The best checkpoint with 49.2700 mIoU at 12000 iter is saved to best_mIoU_iter_12000.pth.
2025/03/29 15:35:00 - mmengine - INFO - Iter(train) [12050/20000]  base_lr: 4.3594e-05 lr: 4.3594e-05  eta: 2:06:09  time: 1.1565  data_time: 0.0232  memory: 11212  loss: 53.8388  decode.loss_cls: 0.2844  decode.loss_mask: 1.8975  decode.loss_dice: 1.6385  decode.d0.loss_cls: 0.2869  decode.d0.loss_mask: 1.9566  decode.d0.loss_dice: 1.6714  decode.d1.loss_cls: 0.2375  decode.d1.loss_mask: 1.9226  decode.d1.loss_dice: 1.6135  decode.d2.loss_cls: 0.2233  decode.d2.loss_mask: 1.9192  decode.d2.loss_dice: 1.6119  decode.d3.loss_cls: 0.2801  decode.d3.loss_mask: 1.9351  decode.d3.loss_dice: 1.6369  decode.d4.loss_cls: 0.2451  decode.d4.loss_mask: 1.9455  decode.d4.loss_dice: 1.6405  decode.d5.loss_cls: 0.2149  decode.d5.loss_mask: 1.9042  decode.d5.loss_dice: 1.7099  decode.d6.loss_cls: 0.3057  decode.d6.loss_mask: 1.9199  decode.d6.loss_dice: 1.6311  decode.d7.loss_cls: 0.2353  decode.d7.loss_mask: 1.9326  decode.d7.loss_dice: 1.6538  decode.d8.loss_cls: 0.2731  decode.d8.loss_mask: 1.9110  decode.d8.loss_dice: 1.6627  mix_decode.loss_cls: 0.2044  mix_decode.loss_mask: 0.6550  mix_decode.loss_dice: 0.6756  mix_decode.d0.loss_cls: 0.2696  mix_decode.d0.loss_mask: 0.6392  mix_decode.d0.loss_dice: 0.7011  mix_decode.d1.loss_cls: 0.1964  mix_decode.d1.loss_mask: 0.6401  mix_decode.d1.loss_dice: 0.6782  mix_decode.d2.loss_cls: 0.2336  mix_decode.d2.loss_mask: 0.6554  mix_decode.d2.loss_dice: 0.6597  mix_decode.d3.loss_cls: 0.2023  mix_decode.d3.loss_mask: 0.6407  mix_decode.d3.loss_dice: 0.6627  mix_decode.d4.loss_cls: 0.1787  mix_decode.d4.loss_mask: 0.6930  mix_decode.d4.loss_dice: 0.6865  mix_decode.d5.loss_cls: 0.1826  mix_decode.d5.loss_mask: 0.6818  mix_decode.d5.loss_dice: 0.7004  mix_decode.d6.loss_cls: 0.2162  mix_decode.d6.loss_mask: 0.6693  mix_decode.d6.loss_dice: 0.6849  mix_decode.d7.loss_cls: 0.2222  mix_decode.d7.loss_mask: 0.6602  mix_decode.d7.loss_dice: 0.6866  mix_decode.d8.loss_cls: 0.2104  mix_decode.d8.loss_mask: 0.6480  mix_decode.d8.loss_dice: 0.7032
2025/03/29 15:35:58 - mmengine - INFO - Iter(train) [12100/20000]  base_lr: 4.3347e-05 lr: 4.3347e-05  eta: 2:05:28  time: 1.1557  data_time: 0.0233  memory: 11214  loss: 56.8508  decode.loss_cls: 0.4384  decode.loss_mask: 1.9461  decode.loss_dice: 1.6929  decode.d0.loss_cls: 0.4413  decode.d0.loss_mask: 1.9985  decode.d0.loss_dice: 1.7645  decode.d1.loss_cls: 0.4198  decode.d1.loss_mask: 1.9858  decode.d1.loss_dice: 1.7162  decode.d2.loss_cls: 0.4229  decode.d2.loss_mask: 2.0212  decode.d2.loss_dice: 1.7472  decode.d3.loss_cls: 0.4179  decode.d3.loss_mask: 2.0244  decode.d3.loss_dice: 1.7431  decode.d4.loss_cls: 0.4362  decode.d4.loss_mask: 2.0010  decode.d4.loss_dice: 1.7382  decode.d5.loss_cls: 0.3876  decode.d5.loss_mask: 1.9567  decode.d5.loss_dice: 1.7667  decode.d6.loss_cls: 0.3982  decode.d6.loss_mask: 2.0206  decode.d6.loss_dice: 1.7258  decode.d7.loss_cls: 0.3745  decode.d7.loss_mask: 1.9897  decode.d7.loss_dice: 1.7535  decode.d8.loss_cls: 0.4354  decode.d8.loss_mask: 1.9293  decode.d8.loss_dice: 1.7113  mix_decode.loss_cls: 0.2233  mix_decode.loss_mask: 0.6717  mix_decode.loss_dice: 0.6311  mix_decode.d0.loss_cls: 0.2876  mix_decode.d0.loss_mask: 0.6739  mix_decode.d0.loss_dice: 0.6823  mix_decode.d1.loss_cls: 0.2177  mix_decode.d1.loss_mask: 0.6596  mix_decode.d1.loss_dice: 0.6389  mix_decode.d2.loss_cls: 0.2227  mix_decode.d2.loss_mask: 0.6685  mix_decode.d2.loss_dice: 0.6214  mix_decode.d3.loss_cls: 0.2113  mix_decode.d3.loss_mask: 0.6673  mix_decode.d3.loss_dice: 0.6507  mix_decode.d4.loss_cls: 0.2373  mix_decode.d4.loss_mask: 0.6782  mix_decode.d4.loss_dice: 0.6433  mix_decode.d5.loss_cls: 0.2316  mix_decode.d5.loss_mask: 0.6716  mix_decode.d5.loss_dice: 0.6297  mix_decode.d6.loss_cls: 0.2140  mix_decode.d6.loss_mask: 0.6791  mix_decode.d6.loss_dice: 0.6416  mix_decode.d7.loss_cls: 0.2532  mix_decode.d7.loss_mask: 0.6697  mix_decode.d7.loss_dice: 0.6292  mix_decode.d8.loss_cls: 0.2594  mix_decode.d8.loss_mask: 0.6478  mix_decode.d8.loss_dice: 0.6321
2025/03/29 15:36:56 - mmengine - INFO - Iter(train) [12150/20000]  base_lr: 4.3100e-05 lr: 4.3100e-05  eta: 2:04:47  time: 1.1578  data_time: 0.0226  memory: 11223  loss: 59.1110  decode.loss_cls: 0.5068  decode.loss_mask: 1.9246  decode.loss_dice: 1.8329  decode.d0.loss_cls: 0.5197  decode.d0.loss_mask: 1.9621  decode.d0.loss_dice: 1.9076  decode.d1.loss_cls: 0.5335  decode.d1.loss_mask: 1.9558  decode.d1.loss_dice: 1.7916  decode.d2.loss_cls: 0.5492  decode.d2.loss_mask: 1.8901  decode.d2.loss_dice: 1.7556  decode.d3.loss_cls: 0.5878  decode.d3.loss_mask: 1.8682  decode.d3.loss_dice: 1.7986  decode.d4.loss_cls: 0.4017  decode.d4.loss_mask: 1.9733  decode.d4.loss_dice: 1.8550  decode.d5.loss_cls: 0.4565  decode.d5.loss_mask: 1.9658  decode.d5.loss_dice: 1.8368  decode.d6.loss_cls: 0.5722  decode.d6.loss_mask: 1.9569  decode.d6.loss_dice: 1.8306  decode.d7.loss_cls: 0.5276  decode.d7.loss_mask: 1.9623  decode.d7.loss_dice: 1.8158  decode.d8.loss_cls: 0.5541  decode.d8.loss_mask: 1.9478  decode.d8.loss_dice: 1.8336  mix_decode.loss_cls: 0.2463  mix_decode.loss_mask: 0.6053  mix_decode.loss_dice: 0.7573  mix_decode.d0.loss_cls: 0.3533  mix_decode.d0.loss_mask: 0.5892  mix_decode.d0.loss_dice: 0.7729  mix_decode.d1.loss_cls: 0.2992  mix_decode.d1.loss_mask: 0.6006  mix_decode.d1.loss_dice: 0.7104  mix_decode.d2.loss_cls: 0.2474  mix_decode.d2.loss_mask: 0.6186  mix_decode.d2.loss_dice: 0.7276  mix_decode.d3.loss_cls: 0.2161  mix_decode.d3.loss_mask: 0.6156  mix_decode.d3.loss_dice: 0.7686  mix_decode.d4.loss_cls: 0.1985  mix_decode.d4.loss_mask: 0.6032  mix_decode.d4.loss_dice: 0.7644  mix_decode.d5.loss_cls: 0.2460  mix_decode.d5.loss_mask: 0.6252  mix_decode.d5.loss_dice: 0.7472  mix_decode.d6.loss_cls: 0.2425  mix_decode.d6.loss_mask: 0.6494  mix_decode.d6.loss_dice: 0.7720  mix_decode.d7.loss_cls: 0.2635  mix_decode.d7.loss_mask: 0.6225  mix_decode.d7.loss_dice: 0.7420  mix_decode.d8.loss_cls: 0.3040  mix_decode.d8.loss_mask: 0.5874  mix_decode.d8.loss_dice: 0.7412
2025/03/29 15:37:53 - mmengine - INFO - Iter(train) [12200/20000]  base_lr: 4.2853e-05 lr: 4.2853e-05  eta: 2:04:05  time: 1.1482  data_time: 0.0224  memory: 11216  loss: 55.7644  decode.loss_cls: 0.2434  decode.loss_mask: 1.9503  decode.loss_dice: 1.7215  decode.d0.loss_cls: 0.4131  decode.d0.loss_mask: 1.9615  decode.d0.loss_dice: 1.6823  decode.d1.loss_cls: 0.3078  decode.d1.loss_mask: 2.0010  decode.d1.loss_dice: 1.7107  decode.d2.loss_cls: 0.3370  decode.d2.loss_mask: 1.9452  decode.d2.loss_dice: 1.7081  decode.d3.loss_cls: 0.2826  decode.d3.loss_mask: 1.9391  decode.d3.loss_dice: 1.6899  decode.d4.loss_cls: 0.2951  decode.d4.loss_mask: 1.9785  decode.d4.loss_dice: 1.7093  decode.d5.loss_cls: 0.2792  decode.d5.loss_mask: 2.0201  decode.d5.loss_dice: 1.7244  decode.d6.loss_cls: 0.2821  decode.d6.loss_mask: 2.0077  decode.d6.loss_dice: 1.7312  decode.d7.loss_cls: 0.2703  decode.d7.loss_mask: 1.9864  decode.d7.loss_dice: 1.7416  decode.d8.loss_cls: 0.3046  decode.d8.loss_mask: 1.9532  decode.d8.loss_dice: 1.7083  mix_decode.loss_cls: 0.2143  mix_decode.loss_mask: 0.5950  mix_decode.loss_dice: 0.7483  mix_decode.d0.loss_cls: 0.3042  mix_decode.d0.loss_mask: 0.5736  mix_decode.d0.loss_dice: 0.7919  mix_decode.d1.loss_cls: 0.2387  mix_decode.d1.loss_mask: 0.5815  mix_decode.d1.loss_dice: 0.7580  mix_decode.d2.loss_cls: 0.2306  mix_decode.d2.loss_mask: 0.5875  mix_decode.d2.loss_dice: 0.7451  mix_decode.d3.loss_cls: 0.2087  mix_decode.d3.loss_mask: 0.6067  mix_decode.d3.loss_dice: 0.7533  mix_decode.d4.loss_cls: 0.2391  mix_decode.d4.loss_mask: 0.5966  mix_decode.d4.loss_dice: 0.7588  mix_decode.d5.loss_cls: 0.2312  mix_decode.d5.loss_mask: 0.5837  mix_decode.d5.loss_dice: 0.7677  mix_decode.d6.loss_cls: 0.2426  mix_decode.d6.loss_mask: 0.5946  mix_decode.d6.loss_dice: 0.7435  mix_decode.d7.loss_cls: 0.2327  mix_decode.d7.loss_mask: 0.5961  mix_decode.d7.loss_dice: 0.7658  mix_decode.d8.loss_cls: 0.2411  mix_decode.d8.loss_mask: 0.5805  mix_decode.d8.loss_dice: 0.7673
2025/03/29 15:38:51 - mmengine - INFO - Iter(train) [12250/20000]  base_lr: 4.2605e-05 lr: 4.2605e-05  eta: 2:03:24  time: 1.1481  data_time: 0.0228  memory: 11211  loss: 57.2138  decode.loss_cls: 0.3518  decode.loss_mask: 2.1755  decode.loss_dice: 1.6980  decode.d0.loss_cls: 0.4425  decode.d0.loss_mask: 2.1377  decode.d0.loss_dice: 1.7223  decode.d1.loss_cls: 0.3536  decode.d1.loss_mask: 2.1526  decode.d1.loss_dice: 1.7854  decode.d2.loss_cls: 0.3175  decode.d2.loss_mask: 2.1614  decode.d2.loss_dice: 1.7496  decode.d3.loss_cls: 0.3282  decode.d3.loss_mask: 2.1599  decode.d3.loss_dice: 1.7611  decode.d4.loss_cls: 0.2986  decode.d4.loss_mask: 2.1690  decode.d4.loss_dice: 1.7777  decode.d5.loss_cls: 0.3419  decode.d5.loss_mask: 2.1910  decode.d5.loss_dice: 1.7689  decode.d6.loss_cls: 0.3523  decode.d6.loss_mask: 2.1825  decode.d6.loss_dice: 1.7989  decode.d7.loss_cls: 0.3723  decode.d7.loss_mask: 2.1790  decode.d7.loss_dice: 1.7685  decode.d8.loss_cls: 0.3247  decode.d8.loss_mask: 2.1773  decode.d8.loss_dice: 1.7827  mix_decode.loss_cls: 0.2371  mix_decode.loss_mask: 0.5885  mix_decode.loss_dice: 0.6123  mix_decode.d0.loss_cls: 0.3113  mix_decode.d0.loss_mask: 0.5691  mix_decode.d0.loss_dice: 0.6640  mix_decode.d1.loss_cls: 0.1830  mix_decode.d1.loss_mask: 0.5835  mix_decode.d1.loss_dice: 0.6668  mix_decode.d2.loss_cls: 0.2064  mix_decode.d2.loss_mask: 0.5777  mix_decode.d2.loss_dice: 0.6337  mix_decode.d3.loss_cls: 0.1942  mix_decode.d3.loss_mask: 0.5908  mix_decode.d3.loss_dice: 0.6282  mix_decode.d4.loss_cls: 0.1704  mix_decode.d4.loss_mask: 0.5971  mix_decode.d4.loss_dice: 0.6432  mix_decode.d5.loss_cls: 0.2324  mix_decode.d5.loss_mask: 0.5838  mix_decode.d5.loss_dice: 0.6122  mix_decode.d6.loss_cls: 0.1822  mix_decode.d6.loss_mask: 0.6030  mix_decode.d6.loss_dice: 0.6621  mix_decode.d7.loss_cls: 0.2193  mix_decode.d7.loss_mask: 0.5852  mix_decode.d7.loss_dice: 0.6402  mix_decode.d8.loss_cls: 0.2579  mix_decode.d8.loss_mask: 0.5800  mix_decode.d8.loss_dice: 0.6156
2025/03/29 15:39:49 - mmengine - INFO - Iter(train) [12300/20000]  base_lr: 4.2358e-05 lr: 4.2358e-05  eta: 2:02:42  time: 1.1515  data_time: 0.0224  memory: 11214  loss: 55.8259  decode.loss_cls: 0.3564  decode.loss_mask: 1.9087  decode.loss_dice: 1.6082  decode.d0.loss_cls: 0.3796  decode.d0.loss_mask: 1.9252  decode.d0.loss_dice: 1.6720  decode.d1.loss_cls: 0.3173  decode.d1.loss_mask: 1.9431  decode.d1.loss_dice: 1.6320  decode.d2.loss_cls: 0.3977  decode.d2.loss_mask: 1.8860  decode.d2.loss_dice: 1.6069  decode.d3.loss_cls: 0.3729  decode.d3.loss_mask: 1.9316  decode.d3.loss_dice: 1.6260  decode.d4.loss_cls: 0.3748  decode.d4.loss_mask: 1.9528  decode.d4.loss_dice: 1.6597  decode.d5.loss_cls: 0.3328  decode.d5.loss_mask: 1.9364  decode.d5.loss_dice: 1.6396  decode.d6.loss_cls: 0.3953  decode.d6.loss_mask: 1.9339  decode.d6.loss_dice: 1.6767  decode.d7.loss_cls: 0.3733  decode.d7.loss_mask: 1.9520  decode.d7.loss_dice: 1.6420  decode.d8.loss_cls: 0.3024  decode.d8.loss_mask: 1.9173  decode.d8.loss_dice: 1.6230  mix_decode.loss_cls: 0.2230  mix_decode.loss_mask: 0.6660  mix_decode.loss_dice: 0.7212  mix_decode.d0.loss_cls: 0.2749  mix_decode.d0.loss_mask: 0.6760  mix_decode.d0.loss_dice: 0.8054  mix_decode.d1.loss_cls: 0.1749  mix_decode.d1.loss_mask: 0.6888  mix_decode.d1.loss_dice: 0.7751  mix_decode.d2.loss_cls: 0.2136  mix_decode.d2.loss_mask: 0.6671  mix_decode.d2.loss_dice: 0.7507  mix_decode.d3.loss_cls: 0.2093  mix_decode.d3.loss_mask: 0.6778  mix_decode.d3.loss_dice: 0.7503  mix_decode.d4.loss_cls: 0.2157  mix_decode.d4.loss_mask: 0.6726  mix_decode.d4.loss_dice: 0.7758  mix_decode.d5.loss_cls: 0.1970  mix_decode.d5.loss_mask: 0.6773  mix_decode.d5.loss_dice: 0.7815  mix_decode.d6.loss_cls: 0.1921  mix_decode.d6.loss_mask: 0.6900  mix_decode.d6.loss_dice: 0.7823  mix_decode.d7.loss_cls: 0.2149  mix_decode.d7.loss_mask: 0.6845  mix_decode.d7.loss_dice: 0.7644  mix_decode.d8.loss_cls: 0.2265  mix_decode.d8.loss_mask: 0.6685  mix_decode.d8.loss_dice: 0.7328
2025/03/29 15:40:46 - mmengine - INFO - Iter(train) [12350/20000]  base_lr: 4.2110e-05 lr: 4.2110e-05  eta: 2:02:01  time: 1.1525  data_time: 0.0224  memory: 11210  loss: 56.7406  decode.loss_cls: 0.5298  decode.loss_mask: 1.6455  decode.loss_dice: 1.7282  decode.d0.loss_cls: 0.5267  decode.d0.loss_mask: 1.6894  decode.d0.loss_dice: 1.8381  decode.d1.loss_cls: 0.5170  decode.d1.loss_mask: 1.6881  decode.d1.loss_dice: 1.7434  decode.d2.loss_cls: 0.5499  decode.d2.loss_mask: 1.6059  decode.d2.loss_dice: 1.6935  decode.d3.loss_cls: 0.4753  decode.d3.loss_mask: 1.6925  decode.d3.loss_dice: 1.7444  decode.d4.loss_cls: 0.5038  decode.d4.loss_mask: 1.6488  decode.d4.loss_dice: 1.7241  decode.d5.loss_cls: 0.4909  decode.d5.loss_mask: 1.6655  decode.d5.loss_dice: 1.7647  decode.d6.loss_cls: 0.4967  decode.d6.loss_mask: 1.6332  decode.d6.loss_dice: 1.7418  decode.d7.loss_cls: 0.5250  decode.d7.loss_mask: 1.7026  decode.d7.loss_dice: 1.7303  decode.d8.loss_cls: 0.6226  decode.d8.loss_mask: 1.6369  decode.d8.loss_dice: 1.6911  mix_decode.loss_cls: 0.2688  mix_decode.loss_mask: 0.7387  mix_decode.loss_dice: 0.7121  mix_decode.d0.loss_cls: 0.3242  mix_decode.d0.loss_mask: 0.7273  mix_decode.d0.loss_dice: 0.8034  mix_decode.d1.loss_cls: 0.2533  mix_decode.d1.loss_mask: 0.7884  mix_decode.d1.loss_dice: 0.7441  mix_decode.d2.loss_cls: 0.3284  mix_decode.d2.loss_mask: 0.6980  mix_decode.d2.loss_dice: 0.6942  mix_decode.d3.loss_cls: 0.2687  mix_decode.d3.loss_mask: 0.7396  mix_decode.d3.loss_dice: 0.7139  mix_decode.d4.loss_cls: 0.2100  mix_decode.d4.loss_mask: 0.7503  mix_decode.d4.loss_dice: 0.7386  mix_decode.d5.loss_cls: 0.2241  mix_decode.d5.loss_mask: 0.7546  mix_decode.d5.loss_dice: 0.7723  mix_decode.d6.loss_cls: 0.2696  mix_decode.d6.loss_mask: 0.7441  mix_decode.d6.loss_dice: 0.7393  mix_decode.d7.loss_cls: 0.2549  mix_decode.d7.loss_mask: 0.7428  mix_decode.d7.loss_dice: 0.7324  mix_decode.d8.loss_cls: 0.2517  mix_decode.d8.loss_mask: 0.7759  mix_decode.d8.loss_dice: 0.7310
2025/03/29 15:41:44 - mmengine - INFO - Iter(train) [12400/20000]  base_lr: 4.1862e-05 lr: 4.1862e-05  eta: 2:01:19  time: 1.1496  data_time: 0.0228  memory: 11217  loss: 54.2789  decode.loss_cls: 0.4752  decode.loss_mask: 1.8705  decode.loss_dice: 1.6372  decode.d0.loss_cls: 0.4337  decode.d0.loss_mask: 1.9089  decode.d0.loss_dice: 1.7170  decode.d1.loss_cls: 0.4657  decode.d1.loss_mask: 1.9170  decode.d1.loss_dice: 1.6496  decode.d2.loss_cls: 0.4392  decode.d2.loss_mask: 1.8812  decode.d2.loss_dice: 1.6263  decode.d3.loss_cls: 0.4709  decode.d3.loss_mask: 1.8430  decode.d3.loss_dice: 1.6202  decode.d4.loss_cls: 0.4129  decode.d4.loss_mask: 1.9074  decode.d4.loss_dice: 1.6922  decode.d5.loss_cls: 0.3630  decode.d5.loss_mask: 1.9219  decode.d5.loss_dice: 1.6801  decode.d6.loss_cls: 0.5016  decode.d6.loss_mask: 1.9095  decode.d6.loss_dice: 1.5998  decode.d7.loss_cls: 0.4900  decode.d7.loss_mask: 1.8715  decode.d7.loss_dice: 1.6291  decode.d8.loss_cls: 0.4519  decode.d8.loss_mask: 1.8945  decode.d8.loss_dice: 1.6106  mix_decode.loss_cls: 0.2386  mix_decode.loss_mask: 0.5653  mix_decode.loss_dice: 0.6100  mix_decode.d0.loss_cls: 0.3152  mix_decode.d0.loss_mask: 0.5627  mix_decode.d0.loss_dice: 0.6566  mix_decode.d1.loss_cls: 0.2595  mix_decode.d1.loss_mask: 0.5610  mix_decode.d1.loss_dice: 0.6388  mix_decode.d2.loss_cls: 0.2486  mix_decode.d2.loss_mask: 0.5672  mix_decode.d2.loss_dice: 0.5979  mix_decode.d3.loss_cls: 0.2245  mix_decode.d3.loss_mask: 0.5744  mix_decode.d3.loss_dice: 0.6096  mix_decode.d4.loss_cls: 0.2576  mix_decode.d4.loss_mask: 0.5799  mix_decode.d4.loss_dice: 0.6163  mix_decode.d5.loss_cls: 0.2694  mix_decode.d5.loss_mask: 0.5633  mix_decode.d5.loss_dice: 0.6158  mix_decode.d6.loss_cls: 0.2706  mix_decode.d6.loss_mask: 0.5538  mix_decode.d6.loss_dice: 0.6063  mix_decode.d7.loss_cls: 0.2612  mix_decode.d7.loss_mask: 0.5455  mix_decode.d7.loss_dice: 0.5962  mix_decode.d8.loss_cls: 0.2714  mix_decode.d8.loss_mask: 0.5521  mix_decode.d8.loss_dice: 0.5979
2025/03/29 15:42:42 - mmengine - INFO - Iter(train) [12450/20000]  base_lr: 4.1615e-05 lr: 4.1615e-05  eta: 2:00:37  time: 1.1580  data_time: 0.0229  memory: 11210  loss: 61.4221  decode.loss_cls: 0.4830  decode.loss_mask: 1.9541  decode.loss_dice: 1.8779  decode.d0.loss_cls: 0.4044  decode.d0.loss_mask: 1.9509  decode.d0.loss_dice: 2.0220  decode.d1.loss_cls: 0.4763  decode.d1.loss_mask: 1.9195  decode.d1.loss_dice: 1.8669  decode.d2.loss_cls: 0.4831  decode.d2.loss_mask: 1.8962  decode.d2.loss_dice: 1.8638  decode.d3.loss_cls: 0.4675  decode.d3.loss_mask: 1.8885  decode.d3.loss_dice: 1.8575  decode.d4.loss_cls: 0.4755  decode.d4.loss_mask: 1.8875  decode.d4.loss_dice: 1.8941  decode.d5.loss_cls: 0.4751  decode.d5.loss_mask: 1.9244  decode.d5.loss_dice: 1.9094  decode.d6.loss_cls: 0.5015  decode.d6.loss_mask: 1.9566  decode.d6.loss_dice: 1.8647  decode.d7.loss_cls: 0.5287  decode.d7.loss_mask: 1.8953  decode.d7.loss_dice: 1.8315  decode.d8.loss_cls: 0.5324  decode.d8.loss_mask: 1.8988  decode.d8.loss_dice: 1.8516  mix_decode.loss_cls: 0.2981  mix_decode.loss_mask: 0.7211  mix_decode.loss_dice: 0.8383  mix_decode.d0.loss_cls: 0.3009  mix_decode.d0.loss_mask: 0.7171  mix_decode.d0.loss_dice: 0.9145  mix_decode.d1.loss_cls: 0.3010  mix_decode.d1.loss_mask: 0.7062  mix_decode.d1.loss_dice: 0.8552  mix_decode.d2.loss_cls: 0.2854  mix_decode.d2.loss_mask: 0.7042  mix_decode.d2.loss_dice: 0.8309  mix_decode.d3.loss_cls: 0.3185  mix_decode.d3.loss_mask: 0.7025  mix_decode.d3.loss_dice: 0.8184  mix_decode.d4.loss_cls: 0.2160  mix_decode.d4.loss_mask: 0.7529  mix_decode.d4.loss_dice: 0.8818  mix_decode.d5.loss_cls: 0.2716  mix_decode.d5.loss_mask: 0.7252  mix_decode.d5.loss_dice: 0.8524  mix_decode.d6.loss_cls: 0.2457  mix_decode.d6.loss_mask: 0.7109  mix_decode.d6.loss_dice: 0.8640  mix_decode.d7.loss_cls: 0.2569  mix_decode.d7.loss_mask: 0.7429  mix_decode.d7.loss_dice: 0.8619  mix_decode.d8.loss_cls: 0.2974  mix_decode.d8.loss_mask: 0.7293  mix_decode.d8.loss_dice: 0.8620
2025/03/29 15:43:39 - mmengine - INFO - Iter(train) [12500/20000]  base_lr: 4.1366e-05 lr: 4.1366e-05  eta: 1:59:55  time: 1.1550  data_time: 0.0229  memory: 11214  loss: 55.2418  decode.loss_cls: 0.2095  decode.loss_mask: 2.0667  decode.loss_dice: 1.7070  decode.d0.loss_cls: 0.3746  decode.d0.loss_mask: 2.0505  decode.d0.loss_dice: 1.7255  decode.d1.loss_cls: 0.2841  decode.d1.loss_mask: 2.0266  decode.d1.loss_dice: 1.6913  decode.d2.loss_cls: 0.2376  decode.d2.loss_mask: 2.0430  decode.d2.loss_dice: 1.6949  decode.d3.loss_cls: 0.2239  decode.d3.loss_mask: 2.0178  decode.d3.loss_dice: 1.6999  decode.d4.loss_cls: 0.1948  decode.d4.loss_mask: 2.0343  decode.d4.loss_dice: 1.7103  decode.d5.loss_cls: 0.2280  decode.d5.loss_mask: 2.0072  decode.d5.loss_dice: 1.7049  decode.d6.loss_cls: 0.2418  decode.d6.loss_mask: 2.0484  decode.d6.loss_dice: 1.7360  decode.d7.loss_cls: 0.2537  decode.d7.loss_mask: 1.9961  decode.d7.loss_dice: 1.6747  decode.d8.loss_cls: 0.2539  decode.d8.loss_mask: 2.0363  decode.d8.loss_dice: 1.7195  mix_decode.loss_cls: 0.2121  mix_decode.loss_mask: 0.5623  mix_decode.loss_dice: 0.7227  mix_decode.d0.loss_cls: 0.3074  mix_decode.d0.loss_mask: 0.5576  mix_decode.d0.loss_dice: 0.8088  mix_decode.d1.loss_cls: 0.2475  mix_decode.d1.loss_mask: 0.5576  mix_decode.d1.loss_dice: 0.7354  mix_decode.d2.loss_cls: 0.2235  mix_decode.d2.loss_mask: 0.5676  mix_decode.d2.loss_dice: 0.7530  mix_decode.d3.loss_cls: 0.1946  mix_decode.d3.loss_mask: 0.5664  mix_decode.d3.loss_dice: 0.7319  mix_decode.d4.loss_cls: 0.2202  mix_decode.d4.loss_mask: 0.5508  mix_decode.d4.loss_dice: 0.7317  mix_decode.d5.loss_cls: 0.2070  mix_decode.d5.loss_mask: 0.5597  mix_decode.d5.loss_dice: 0.7526  mix_decode.d6.loss_cls: 0.2203  mix_decode.d6.loss_mask: 0.5572  mix_decode.d6.loss_dice: 0.7366  mix_decode.d7.loss_cls: 0.2339  mix_decode.d7.loss_mask: 0.5466  mix_decode.d7.loss_dice: 0.7530  mix_decode.d8.loss_cls: 0.2337  mix_decode.d8.loss_mask: 0.5606  mix_decode.d8.loss_dice: 0.7367
2025/03/29 15:44:37 - mmengine - INFO - Iter(train) [12550/20000]  base_lr: 4.1118e-05 lr: 4.1118e-05  eta: 1:59:12  time: 1.1536  data_time: 0.0229  memory: 11216  loss: 60.8200  decode.loss_cls: 0.3985  decode.loss_mask: 1.9710  decode.loss_dice: 1.8409  decode.d0.loss_cls: 0.5385  decode.d0.loss_mask: 1.9247  decode.d0.loss_dice: 1.8214  decode.d1.loss_cls: 0.3749  decode.d1.loss_mask: 1.9475  decode.d1.loss_dice: 1.8443  decode.d2.loss_cls: 0.4444  decode.d2.loss_mask: 1.9330  decode.d2.loss_dice: 1.8673  decode.d3.loss_cls: 0.3954  decode.d3.loss_mask: 1.9676  decode.d3.loss_dice: 1.8817  decode.d4.loss_cls: 0.3967  decode.d4.loss_mask: 1.9691  decode.d4.loss_dice: 1.8488  decode.d5.loss_cls: 0.4133  decode.d5.loss_mask: 1.9631  decode.d5.loss_dice: 1.8597  decode.d6.loss_cls: 0.4133  decode.d6.loss_mask: 1.9819  decode.d6.loss_dice: 1.8921  decode.d7.loss_cls: 0.4387  decode.d7.loss_mask: 1.9635  decode.d7.loss_dice: 1.8785  decode.d8.loss_cls: 0.4151  decode.d8.loss_mask: 1.9613  decode.d8.loss_dice: 1.8542  mix_decode.loss_cls: 0.3352  mix_decode.loss_mask: 0.6885  mix_decode.loss_dice: 0.8209  mix_decode.d0.loss_cls: 0.3460  mix_decode.d0.loss_mask: 0.7010  mix_decode.d0.loss_dice: 0.8956  mix_decode.d1.loss_cls: 0.3103  mix_decode.d1.loss_mask: 0.6675  mix_decode.d1.loss_dice: 0.8066  mix_decode.d2.loss_cls: 0.3303  mix_decode.d2.loss_mask: 0.6724  mix_decode.d2.loss_dice: 0.7975  mix_decode.d3.loss_cls: 0.3062  mix_decode.d3.loss_mask: 0.6934  mix_decode.d3.loss_dice: 0.8156  mix_decode.d4.loss_cls: 0.3095  mix_decode.d4.loss_mask: 0.6859  mix_decode.d4.loss_dice: 0.8301  mix_decode.d5.loss_cls: 0.3292  mix_decode.d5.loss_mask: 0.6804  mix_decode.d5.loss_dice: 0.8285  mix_decode.d6.loss_cls: 0.3425  mix_decode.d6.loss_mask: 0.6876  mix_decode.d6.loss_dice: 0.8217  mix_decode.d7.loss_cls: 0.3343  mix_decode.d7.loss_mask: 0.6913  mix_decode.d7.loss_dice: 0.8593  mix_decode.d8.loss_cls: 0.3147  mix_decode.d8.loss_mask: 0.6892  mix_decode.d8.loss_dice: 0.8282
2025/03/29 15:45:35 - mmengine - INFO - Iter(train) [12600/20000]  base_lr: 4.0870e-05 lr: 4.0870e-05  eta: 1:58:30  time: 1.1519  data_time: 0.0225  memory: 11208  loss: 57.1334  decode.loss_cls: 0.3959  decode.loss_mask: 1.8370  decode.loss_dice: 1.5667  decode.d0.loss_cls: 0.3942  decode.d0.loss_mask: 1.8714  decode.d0.loss_dice: 1.6743  decode.d1.loss_cls: 0.2762  decode.d1.loss_mask: 1.8625  decode.d1.loss_dice: 1.6369  decode.d2.loss_cls: 0.3034  decode.d2.loss_mask: 1.8828  decode.d2.loss_dice: 1.6403  decode.d3.loss_cls: 0.2975  decode.d3.loss_mask: 1.8821  decode.d3.loss_dice: 1.6312  decode.d4.loss_cls: 0.3376  decode.d4.loss_mask: 1.8444  decode.d4.loss_dice: 1.5977  decode.d5.loss_cls: 0.3617  decode.d5.loss_mask: 1.8687  decode.d5.loss_dice: 1.5968  decode.d6.loss_cls: 0.3842  decode.d6.loss_mask: 1.8581  decode.d6.loss_dice: 1.6126  decode.d7.loss_cls: 0.3378  decode.d7.loss_mask: 1.9340  decode.d7.loss_dice: 1.6496  decode.d8.loss_cls: 0.3648  decode.d8.loss_mask: 1.8248  decode.d8.loss_dice: 1.6044  mix_decode.loss_cls: 0.2714  mix_decode.loss_mask: 0.7709  mix_decode.loss_dice: 0.8322  mix_decode.d0.loss_cls: 0.3532  mix_decode.d0.loss_mask: 0.7518  mix_decode.d0.loss_dice: 0.8976  mix_decode.d1.loss_cls: 0.3605  mix_decode.d1.loss_mask: 0.7294  mix_decode.d1.loss_dice: 0.8444  mix_decode.d2.loss_cls: 0.2320  mix_decode.d2.loss_mask: 0.7647  mix_decode.d2.loss_dice: 0.8431  mix_decode.d3.loss_cls: 0.2862  mix_decode.d3.loss_mask: 0.7726  mix_decode.d3.loss_dice: 0.8211  mix_decode.d4.loss_cls: 0.2771  mix_decode.d4.loss_mask: 0.7229  mix_decode.d4.loss_dice: 0.8145  mix_decode.d5.loss_cls: 0.2714  mix_decode.d5.loss_mask: 0.7409  mix_decode.d5.loss_dice: 0.8308  mix_decode.d6.loss_cls: 0.2612  mix_decode.d6.loss_mask: 0.7800  mix_decode.d6.loss_dice: 0.8495  mix_decode.d7.loss_cls: 0.2801  mix_decode.d7.loss_mask: 0.7384  mix_decode.d7.loss_dice: 0.8288  mix_decode.d8.loss_cls: 0.2841  mix_decode.d8.loss_mask: 0.7671  mix_decode.d8.loss_dice: 0.8256
2025/03/29 15:46:32 - mmengine - INFO - Iter(train) [12650/20000]  base_lr: 4.0621e-05 lr: 4.0621e-05  eta: 1:57:48  time: 1.1640  data_time: 0.0242  memory: 11222  loss: 56.2432  decode.loss_cls: 0.3579  decode.loss_mask: 1.7942  decode.loss_dice: 1.7095  decode.d0.loss_cls: 0.4815  decode.d0.loss_mask: 1.7817  decode.d0.loss_dice: 1.7042  decode.d1.loss_cls: 0.4311  decode.d1.loss_mask: 1.7905  decode.d1.loss_dice: 1.6908  decode.d2.loss_cls: 0.3796  decode.d2.loss_mask: 1.8052  decode.d2.loss_dice: 1.6835  decode.d3.loss_cls: 0.4143  decode.d3.loss_mask: 1.8185  decode.d3.loss_dice: 1.6796  decode.d4.loss_cls: 0.3727  decode.d4.loss_mask: 1.8212  decode.d4.loss_dice: 1.7023  decode.d5.loss_cls: 0.4092  decode.d5.loss_mask: 1.8498  decode.d5.loss_dice: 1.6650  decode.d6.loss_cls: 0.4333  decode.d6.loss_mask: 1.7966  decode.d6.loss_dice: 1.6857  decode.d7.loss_cls: 0.3978  decode.d7.loss_mask: 1.7983  decode.d7.loss_dice: 1.7079  decode.d8.loss_cls: 0.3885  decode.d8.loss_mask: 1.7982  decode.d8.loss_dice: 1.6856  mix_decode.loss_cls: 0.4137  mix_decode.loss_mask: 0.6359  mix_decode.loss_dice: 0.6628  mix_decode.d0.loss_cls: 0.4578  mix_decode.d0.loss_mask: 0.6175  mix_decode.d0.loss_dice: 0.7290  mix_decode.d1.loss_cls: 0.4694  mix_decode.d1.loss_mask: 0.6038  mix_decode.d1.loss_dice: 0.6229  mix_decode.d2.loss_cls: 0.4341  mix_decode.d2.loss_mask: 0.6107  mix_decode.d2.loss_dice: 0.6348  mix_decode.d3.loss_cls: 0.3995  mix_decode.d3.loss_mask: 0.6036  mix_decode.d3.loss_dice: 0.6492  mix_decode.d4.loss_cls: 0.4524  mix_decode.d4.loss_mask: 0.6016  mix_decode.d4.loss_dice: 0.6580  mix_decode.d5.loss_cls: 0.4273  mix_decode.d5.loss_mask: 0.6606  mix_decode.d5.loss_dice: 0.6753  mix_decode.d6.loss_cls: 0.4396  mix_decode.d6.loss_mask: 0.6267  mix_decode.d6.loss_dice: 0.6658  mix_decode.d7.loss_cls: 0.4235  mix_decode.d7.loss_mask: 0.6246  mix_decode.d7.loss_dice: 0.6757  mix_decode.d8.loss_cls: 0.4736  mix_decode.d8.loss_mask: 0.6114  mix_decode.d8.loss_dice: 0.6482
2025/03/29 15:47:30 - mmengine - INFO - Iter(train) [12700/20000]  base_lr: 4.0372e-05 lr: 4.0372e-05  eta: 1:57:05  time: 1.1647  data_time: 0.0243  memory: 11213  loss: 55.0771  decode.loss_cls: 0.4635  decode.loss_mask: 1.5558  decode.loss_dice: 1.8274  decode.d0.loss_cls: 0.4541  decode.d0.loss_mask: 1.5605  decode.d0.loss_dice: 1.9237  decode.d1.loss_cls: 0.4700  decode.d1.loss_mask: 1.5515  decode.d1.loss_dice: 1.8478  decode.d2.loss_cls: 0.4714  decode.d2.loss_mask: 1.5633  decode.d2.loss_dice: 1.8593  decode.d3.loss_cls: 0.4701  decode.d3.loss_mask: 1.5195  decode.d3.loss_dice: 1.8482  decode.d4.loss_cls: 0.5202  decode.d4.loss_mask: 1.5174  decode.d4.loss_dice: 1.8451  decode.d5.loss_cls: 0.5014  decode.d5.loss_mask: 1.5710  decode.d5.loss_dice: 1.8927  decode.d6.loss_cls: 0.4400  decode.d6.loss_mask: 1.6262  decode.d6.loss_dice: 1.8802  decode.d7.loss_cls: 0.5255  decode.d7.loss_mask: 1.5362  decode.d7.loss_dice: 1.8616  decode.d8.loss_cls: 0.5640  decode.d8.loss_mask: 1.5089  decode.d8.loss_dice: 1.8633  mix_decode.loss_cls: 0.1870  mix_decode.loss_mask: 0.6046  mix_decode.loss_dice: 0.7614  mix_decode.d0.loss_cls: 0.2442  mix_decode.d0.loss_mask: 0.5913  mix_decode.d0.loss_dice: 0.8334  mix_decode.d1.loss_cls: 0.2457  mix_decode.d1.loss_mask: 0.5930  mix_decode.d1.loss_dice: 0.7646  mix_decode.d2.loss_cls: 0.1917  mix_decode.d2.loss_mask: 0.6237  mix_decode.d2.loss_dice: 0.7801  mix_decode.d3.loss_cls: 0.2117  mix_decode.d3.loss_mask: 0.6118  mix_decode.d3.loss_dice: 0.7473  mix_decode.d4.loss_cls: 0.2130  mix_decode.d4.loss_mask: 0.6101  mix_decode.d4.loss_dice: 0.7958  mix_decode.d5.loss_cls: 0.2090  mix_decode.d5.loss_mask: 0.6169  mix_decode.d5.loss_dice: 0.8043  mix_decode.d6.loss_cls: 0.1792  mix_decode.d6.loss_mask: 0.6154  mix_decode.d6.loss_dice: 0.7795  mix_decode.d7.loss_cls: 0.2361  mix_decode.d7.loss_mask: 0.5974  mix_decode.d7.loss_dice: 0.7958  mix_decode.d8.loss_cls: 0.2189  mix_decode.d8.loss_mask: 0.5995  mix_decode.d8.loss_dice: 0.7750
2025/03/29 15:48:28 - mmengine - INFO - Iter(train) [12750/20000]  base_lr: 4.0123e-05 lr: 4.0123e-05  eta: 1:56:22  time: 1.1493  data_time: 0.0225  memory: 11207  loss: 58.3949  decode.loss_cls: 0.2613  decode.loss_mask: 2.0952  decode.loss_dice: 1.7572  decode.d0.loss_cls: 0.4063  decode.d0.loss_mask: 2.1314  decode.d0.loss_dice: 1.7128  decode.d1.loss_cls: 0.2582  decode.d1.loss_mask: 2.1114  decode.d1.loss_dice: 1.7557  decode.d2.loss_cls: 0.2913  decode.d2.loss_mask: 2.0643  decode.d2.loss_dice: 1.7747  decode.d3.loss_cls: 0.2420  decode.d3.loss_mask: 2.1353  decode.d3.loss_dice: 1.7677  decode.d4.loss_cls: 0.2700  decode.d4.loss_mask: 2.0678  decode.d4.loss_dice: 1.7515  decode.d5.loss_cls: 0.2434  decode.d5.loss_mask: 2.0956  decode.d5.loss_dice: 1.7498  decode.d6.loss_cls: 0.2913  decode.d6.loss_mask: 2.0688  decode.d6.loss_dice: 1.7281  decode.d7.loss_cls: 0.2637  decode.d7.loss_mask: 2.0669  decode.d7.loss_dice: 1.7638  decode.d8.loss_cls: 0.2708  decode.d8.loss_mask: 2.0787  decode.d8.loss_dice: 1.7590  mix_decode.loss_cls: 0.2931  mix_decode.loss_mask: 0.6667  mix_decode.loss_dice: 0.7846  mix_decode.d0.loss_cls: 0.3787  mix_decode.d0.loss_mask: 0.6449  mix_decode.d0.loss_dice: 0.7945  mix_decode.d1.loss_cls: 0.2781  mix_decode.d1.loss_mask: 0.6664  mix_decode.d1.loss_dice: 0.7536  mix_decode.d2.loss_cls: 0.3241  mix_decode.d2.loss_mask: 0.6548  mix_decode.d2.loss_dice: 0.7259  mix_decode.d3.loss_cls: 0.2535  mix_decode.d3.loss_mask: 0.6486  mix_decode.d3.loss_dice: 0.7223  mix_decode.d4.loss_cls: 0.2890  mix_decode.d4.loss_mask: 0.6459  mix_decode.d4.loss_dice: 0.7405  mix_decode.d5.loss_cls: 0.2928  mix_decode.d5.loss_mask: 0.6678  mix_decode.d5.loss_dice: 0.7851  mix_decode.d6.loss_cls: 0.2448  mix_decode.d6.loss_mask: 0.6546  mix_decode.d6.loss_dice: 0.7834  mix_decode.d7.loss_cls: 0.2991  mix_decode.d7.loss_mask: 0.6591  mix_decode.d7.loss_dice: 0.7799  mix_decode.d8.loss_cls: 0.3170  mix_decode.d8.loss_mask: 0.6521  mix_decode.d8.loss_dice: 0.7600
2025/03/29 15:49:25 - mmengine - INFO - Iter(train) [12800/20000]  base_lr: 3.9874e-05 lr: 3.9874e-05  eta: 1:55:39  time: 1.1489  data_time: 0.0227  memory: 11215  loss: 55.1282  decode.loss_cls: 0.4714  decode.loss_mask: 1.8043  decode.loss_dice: 1.5679  decode.d0.loss_cls: 0.5343  decode.d0.loss_mask: 1.7994  decode.d0.loss_dice: 1.5728  decode.d1.loss_cls: 0.4286  decode.d1.loss_mask: 1.7768  decode.d1.loss_dice: 1.6146  decode.d2.loss_cls: 0.4724  decode.d2.loss_mask: 1.7663  decode.d2.loss_dice: 1.6384  decode.d3.loss_cls: 0.4323  decode.d3.loss_mask: 1.8082  decode.d3.loss_dice: 1.5779  decode.d4.loss_cls: 0.4213  decode.d4.loss_mask: 1.7907  decode.d4.loss_dice: 1.6174  decode.d5.loss_cls: 0.4923  decode.d5.loss_mask: 1.7812  decode.d5.loss_dice: 1.5212  decode.d6.loss_cls: 0.4442  decode.d6.loss_mask: 1.8225  decode.d6.loss_dice: 1.6100  decode.d7.loss_cls: 0.4427  decode.d7.loss_mask: 1.7791  decode.d7.loss_dice: 1.6268  decode.d8.loss_cls: 0.4164  decode.d8.loss_mask: 1.8190  decode.d8.loss_dice: 1.5937  mix_decode.loss_cls: 0.2566  mix_decode.loss_mask: 0.6278  mix_decode.loss_dice: 0.7453  mix_decode.d0.loss_cls: 0.3507  mix_decode.d0.loss_mask: 0.6142  mix_decode.d0.loss_dice: 0.7578  mix_decode.d1.loss_cls: 0.3423  mix_decode.d1.loss_mask: 0.6412  mix_decode.d1.loss_dice: 0.7151  mix_decode.d2.loss_cls: 0.3208  mix_decode.d2.loss_mask: 0.6394  mix_decode.d2.loss_dice: 0.7475  mix_decode.d3.loss_cls: 0.3258  mix_decode.d3.loss_mask: 0.6323  mix_decode.d3.loss_dice: 0.7010  mix_decode.d4.loss_cls: 0.2982  mix_decode.d4.loss_mask: 0.6306  mix_decode.d4.loss_dice: 0.7258  mix_decode.d5.loss_cls: 0.2845  mix_decode.d5.loss_mask: 0.6220  mix_decode.d5.loss_dice: 0.7253  mix_decode.d6.loss_cls: 0.2934  mix_decode.d6.loss_mask: 0.6250  mix_decode.d6.loss_dice: 0.7355  mix_decode.d7.loss_cls: 0.3108  mix_decode.d7.loss_mask: 0.6179  mix_decode.d7.loss_dice: 0.7089  mix_decode.d8.loss_cls: 0.3169  mix_decode.d8.loss_mask: 0.6433  mix_decode.d8.loss_dice: 0.7283
2025/03/29 15:50:23 - mmengine - INFO - Iter(train) [12850/20000]  base_lr: 3.9625e-05 lr: 3.9625e-05  eta: 1:54:56  time: 1.1500  data_time: 0.0226  memory: 11220  loss: 51.1319  decode.loss_cls: 0.2919  decode.loss_mask: 1.6534  decode.loss_dice: 1.6437  decode.d0.loss_cls: 0.4043  decode.d0.loss_mask: 1.7102  decode.d0.loss_dice: 1.6477  decode.d1.loss_cls: 0.3787  decode.d1.loss_mask: 1.6863  decode.d1.loss_dice: 1.7026  decode.d2.loss_cls: 0.3154  decode.d2.loss_mask: 1.6999  decode.d2.loss_dice: 1.6967  decode.d3.loss_cls: 0.3912  decode.d3.loss_mask: 1.6707  decode.d3.loss_dice: 1.6328  decode.d4.loss_cls: 0.3307  decode.d4.loss_mask: 1.6804  decode.d4.loss_dice: 1.6408  decode.d5.loss_cls: 0.3758  decode.d5.loss_mask: 1.6757  decode.d5.loss_dice: 1.6564  decode.d6.loss_cls: 0.3712  decode.d6.loss_mask: 1.6712  decode.d6.loss_dice: 1.6503  decode.d7.loss_cls: 0.3493  decode.d7.loss_mask: 1.7002  decode.d7.loss_dice: 1.6431  decode.d8.loss_cls: 0.3005  decode.d8.loss_mask: 1.6861  decode.d8.loss_dice: 1.6968  mix_decode.loss_cls: 0.2311  mix_decode.loss_mask: 0.5324  mix_decode.loss_dice: 0.6408  mix_decode.d0.loss_cls: 0.2679  mix_decode.d0.loss_mask: 0.5144  mix_decode.d0.loss_dice: 0.6842  mix_decode.d1.loss_cls: 0.2213  mix_decode.d1.loss_mask: 0.5304  mix_decode.d1.loss_dice: 0.6665  mix_decode.d2.loss_cls: 0.2215  mix_decode.d2.loss_mask: 0.5162  mix_decode.d2.loss_dice: 0.6523  mix_decode.d3.loss_cls: 0.1880  mix_decode.d3.loss_mask: 0.5328  mix_decode.d3.loss_dice: 0.6647  mix_decode.d4.loss_cls: 0.2086  mix_decode.d4.loss_mask: 0.5228  mix_decode.d4.loss_dice: 0.6590  mix_decode.d5.loss_cls: 0.2220  mix_decode.d5.loss_mask: 0.5323  mix_decode.d5.loss_dice: 0.6586  mix_decode.d6.loss_cls: 0.2385  mix_decode.d6.loss_mask: 0.5392  mix_decode.d6.loss_dice: 0.6528  mix_decode.d7.loss_cls: 0.2374  mix_decode.d7.loss_mask: 0.5566  mix_decode.d7.loss_dice: 0.6470  mix_decode.d8.loss_cls: 0.2318  mix_decode.d8.loss_mask: 0.5389  mix_decode.d8.loss_dice: 0.6681
2025/03/29 15:51:20 - mmengine - INFO - Iter(train) [12900/20000]  base_lr: 3.9375e-05 lr: 3.9375e-05  eta: 1:54:13  time: 1.1521  data_time: 0.0225  memory: 11209  loss: 59.7857  decode.loss_cls: 0.2738  decode.loss_mask: 2.1671  decode.loss_dice: 1.7916  decode.d0.loss_cls: 0.4723  decode.d0.loss_mask: 2.2106  decode.d0.loss_dice: 1.7632  decode.d1.loss_cls: 0.3115  decode.d1.loss_mask: 2.1514  decode.d1.loss_dice: 1.8060  decode.d2.loss_cls: 0.2674  decode.d2.loss_mask: 2.1517  decode.d2.loss_dice: 1.7993  decode.d3.loss_cls: 0.2752  decode.d3.loss_mask: 2.1462  decode.d3.loss_dice: 1.7941  decode.d4.loss_cls: 0.2926  decode.d4.loss_mask: 2.1277  decode.d4.loss_dice: 1.7991  decode.d5.loss_cls: 0.2918  decode.d5.loss_mask: 2.1757  decode.d5.loss_dice: 1.7977  decode.d6.loss_cls: 0.2908  decode.d6.loss_mask: 2.1461  decode.d6.loss_dice: 1.7829  decode.d7.loss_cls: 0.2980  decode.d7.loss_mask: 2.1614  decode.d7.loss_dice: 1.7805  decode.d8.loss_cls: 0.3330  decode.d8.loss_mask: 2.1059  decode.d8.loss_dice: 1.8094  mix_decode.loss_cls: 0.3359  mix_decode.loss_mask: 0.5761  mix_decode.loss_dice: 0.7645  mix_decode.d0.loss_cls: 0.2922  mix_decode.d0.loss_mask: 0.5893  mix_decode.d0.loss_dice: 0.8825  mix_decode.d1.loss_cls: 0.3503  mix_decode.d1.loss_mask: 0.5655  mix_decode.d1.loss_dice: 0.7706  mix_decode.d2.loss_cls: 0.3783  mix_decode.d2.loss_mask: 0.5650  mix_decode.d2.loss_dice: 0.7796  mix_decode.d3.loss_cls: 0.3538  mix_decode.d3.loss_mask: 0.5573  mix_decode.d3.loss_dice: 0.7740  mix_decode.d4.loss_cls: 0.3306  mix_decode.d4.loss_mask: 0.5911  mix_decode.d4.loss_dice: 0.8047  mix_decode.d5.loss_cls: 0.2714  mix_decode.d5.loss_mask: 0.5978  mix_decode.d5.loss_dice: 0.8450  mix_decode.d6.loss_cls: 0.3404  mix_decode.d6.loss_mask: 0.5932  mix_decode.d6.loss_dice: 0.8254  mix_decode.d7.loss_cls: 0.4036  mix_decode.d7.loss_mask: 0.5792  mix_decode.d7.loss_dice: 0.7984  mix_decode.d8.loss_cls: 0.3719  mix_decode.d8.loss_mask: 0.5497  mix_decode.d8.loss_dice: 0.7743
2025/03/29 15:52:18 - mmengine - INFO - Iter(train) [12950/20000]  base_lr: 3.9126e-05 lr: 3.9126e-05  eta: 1:53:30  time: 1.1536  data_time: 0.0225  memory: 11219  loss: 59.7032  decode.loss_cls: 0.4202  decode.loss_mask: 2.0281  decode.loss_dice: 1.8526  decode.d0.loss_cls: 0.5426  decode.d0.loss_mask: 1.9951  decode.d0.loss_dice: 1.8432  decode.d1.loss_cls: 0.4340  decode.d1.loss_mask: 2.0105  decode.d1.loss_dice: 1.8291  decode.d2.loss_cls: 0.3971  decode.d2.loss_mask: 2.0256  decode.d2.loss_dice: 1.8165  decode.d3.loss_cls: 0.4473  decode.d3.loss_mask: 1.9823  decode.d3.loss_dice: 1.8011  decode.d4.loss_cls: 0.4328  decode.d4.loss_mask: 1.9881  decode.d4.loss_dice: 1.8589  decode.d5.loss_cls: 0.4647  decode.d5.loss_mask: 1.9997  decode.d5.loss_dice: 1.8123  decode.d6.loss_cls: 0.5089  decode.d6.loss_mask: 1.9799  decode.d6.loss_dice: 1.7965  decode.d7.loss_cls: 0.3947  decode.d7.loss_mask: 2.0224  decode.d7.loss_dice: 1.8431  decode.d8.loss_cls: 0.4499  decode.d8.loss_mask: 2.0094  decode.d8.loss_dice: 1.8656  mix_decode.loss_cls: 0.2716  mix_decode.loss_mask: 0.6946  mix_decode.loss_dice: 0.7299  mix_decode.d0.loss_cls: 0.2660  mix_decode.d0.loss_mask: 0.6754  mix_decode.d0.loss_dice: 0.7812  mix_decode.d1.loss_cls: 0.2949  mix_decode.d1.loss_mask: 0.6744  mix_decode.d1.loss_dice: 0.6796  mix_decode.d2.loss_cls: 0.2961  mix_decode.d2.loss_mask: 0.6791  mix_decode.d2.loss_dice: 0.6867  mix_decode.d3.loss_cls: 0.2837  mix_decode.d3.loss_mask: 0.6730  mix_decode.d3.loss_dice: 0.7077  mix_decode.d4.loss_cls: 0.2622  mix_decode.d4.loss_mask: 0.7047  mix_decode.d4.loss_dice: 0.7261  mix_decode.d5.loss_cls: 0.2978  mix_decode.d5.loss_mask: 0.6747  mix_decode.d5.loss_dice: 0.7031  mix_decode.d6.loss_cls: 0.2855  mix_decode.d6.loss_mask: 0.6953  mix_decode.d6.loss_dice: 0.7002  mix_decode.d7.loss_cls: 0.3269  mix_decode.d7.loss_mask: 0.6789  mix_decode.d7.loss_dice: 0.7184  mix_decode.d8.loss_cls: 0.2597  mix_decode.d8.loss_mask: 0.6819  mix_decode.d8.loss_dice: 0.7414
2025/03/29 15:53:16 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 15:53:16 - mmengine - INFO - Iter(train) [13000/20000]  base_lr: 3.8876e-05 lr: 3.8876e-05  eta: 1:52:47  time: 1.1615  data_time: 0.0242  memory: 11205  loss: 56.8031  decode.loss_cls: 0.4668  decode.loss_mask: 1.7437  decode.loss_dice: 1.8114  decode.d0.loss_cls: 0.4469  decode.d0.loss_mask: 1.7638  decode.d0.loss_dice: 1.8032  decode.d1.loss_cls: 0.4008  decode.d1.loss_mask: 1.7907  decode.d1.loss_dice: 1.8132  decode.d2.loss_cls: 0.4565  decode.d2.loss_mask: 1.7699  decode.d2.loss_dice: 1.8125  decode.d3.loss_cls: 0.4209  decode.d3.loss_mask: 1.7668  decode.d3.loss_dice: 1.8272  decode.d4.loss_cls: 0.4432  decode.d4.loss_mask: 1.8110  decode.d4.loss_dice: 1.8097  decode.d5.loss_cls: 0.4527  decode.d5.loss_mask: 1.7740  decode.d5.loss_dice: 1.8276  decode.d6.loss_cls: 0.4814  decode.d6.loss_mask: 1.7629  decode.d6.loss_dice: 1.7902  decode.d7.loss_cls: 0.4550  decode.d7.loss_mask: 1.7789  decode.d7.loss_dice: 1.8341  decode.d8.loss_cls: 0.3931  decode.d8.loss_mask: 1.7739  decode.d8.loss_dice: 1.7958  mix_decode.loss_cls: 0.1819  mix_decode.loss_mask: 0.7016  mix_decode.loss_dice: 0.7679  mix_decode.d0.loss_cls: 0.1886  mix_decode.d0.loss_mask: 0.7141  mix_decode.d0.loss_dice: 0.7919  mix_decode.d1.loss_cls: 0.2181  mix_decode.d1.loss_mask: 0.6840  mix_decode.d1.loss_dice: 0.7212  mix_decode.d2.loss_cls: 0.2212  mix_decode.d2.loss_mask: 0.7032  mix_decode.d2.loss_dice: 0.7324  mix_decode.d3.loss_cls: 0.2004  mix_decode.d3.loss_mask: 0.6932  mix_decode.d3.loss_dice: 0.7583  mix_decode.d4.loss_cls: 0.1527  mix_decode.d4.loss_mask: 0.7142  mix_decode.d4.loss_dice: 0.7697  mix_decode.d5.loss_cls: 0.1774  mix_decode.d5.loss_mask: 0.7124  mix_decode.d5.loss_dice: 0.7487  mix_decode.d6.loss_cls: 0.1827  mix_decode.d6.loss_mask: 0.6989  mix_decode.d6.loss_dice: 0.7543  mix_decode.d7.loss_cls: 0.2181  mix_decode.d7.loss_mask: 0.6919  mix_decode.d7.loss_dice: 0.7528  mix_decode.d8.loss_cls: 0.2150  mix_decode.d8.loss_mask: 0.7052  mix_decode.d8.loss_dice: 0.7531
2025/03/29 15:54:14 - mmengine - INFO - Iter(train) [13050/20000]  base_lr: 3.8626e-05 lr: 3.8626e-05  eta: 1:52:04  time: 1.1699  data_time: 0.0225  memory: 11216  loss: 56.3146  decode.loss_cls: 0.4380  decode.loss_mask: 1.8996  decode.loss_dice: 1.6778  decode.d0.loss_cls: 0.4361  decode.d0.loss_mask: 2.0070  decode.d0.loss_dice: 1.7370  decode.d1.loss_cls: 0.4073  decode.d1.loss_mask: 1.9529  decode.d1.loss_dice: 1.6638  decode.d2.loss_cls: 0.4614  decode.d2.loss_mask: 1.9206  decode.d2.loss_dice: 1.6947  decode.d3.loss_cls: 0.3775  decode.d3.loss_mask: 1.9272  decode.d3.loss_dice: 1.6516  decode.d4.loss_cls: 0.4463  decode.d4.loss_mask: 1.8464  decode.d4.loss_dice: 1.6201  decode.d5.loss_cls: 0.4213  decode.d5.loss_mask: 1.8766  decode.d5.loss_dice: 1.6866  decode.d6.loss_cls: 0.4453  decode.d6.loss_mask: 1.9005  decode.d6.loss_dice: 1.6624  decode.d7.loss_cls: 0.3304  decode.d7.loss_mask: 1.9740  decode.d7.loss_dice: 1.7060  decode.d8.loss_cls: 0.4556  decode.d8.loss_mask: 1.9291  decode.d8.loss_dice: 1.7046  mix_decode.loss_cls: 0.2450  mix_decode.loss_mask: 0.6196  mix_decode.loss_dice: 0.7204  mix_decode.d0.loss_cls: 0.2299  mix_decode.d0.loss_mask: 0.6399  mix_decode.d0.loss_dice: 0.8282  mix_decode.d1.loss_cls: 0.2467  mix_decode.d1.loss_mask: 0.5947  mix_decode.d1.loss_dice: 0.7032  mix_decode.d2.loss_cls: 0.2297  mix_decode.d2.loss_mask: 0.6159  mix_decode.d2.loss_dice: 0.7091  mix_decode.d3.loss_cls: 0.2429  mix_decode.d3.loss_mask: 0.6074  mix_decode.d3.loss_dice: 0.7134  mix_decode.d4.loss_cls: 0.2449  mix_decode.d4.loss_mask: 0.6177  mix_decode.d4.loss_dice: 0.7595  mix_decode.d5.loss_cls: 0.2640  mix_decode.d5.loss_mask: 0.6112  mix_decode.d5.loss_dice: 0.7555  mix_decode.d6.loss_cls: 0.2303  mix_decode.d6.loss_mask: 0.6158  mix_decode.d6.loss_dice: 0.7656  mix_decode.d7.loss_cls: 0.2387  mix_decode.d7.loss_mask: 0.6352  mix_decode.d7.loss_dice: 0.7740  mix_decode.d8.loss_cls: 0.2155  mix_decode.d8.loss_mask: 0.6267  mix_decode.d8.loss_dice: 0.7566
2025/03/29 15:55:12 - mmengine - INFO - Iter(train) [13100/20000]  base_lr: 3.8376e-05 lr: 3.8376e-05  eta: 1:51:21  time: 1.1517  data_time: 0.0223  memory: 11210  loss: 57.6198  decode.loss_cls: 0.3090  decode.loss_mask: 1.8776  decode.loss_dice: 1.8101  decode.d0.loss_cls: 0.4397  decode.d0.loss_mask: 1.9158  decode.d0.loss_dice: 1.8703  decode.d1.loss_cls: 0.3297  decode.d1.loss_mask: 1.8521  decode.d1.loss_dice: 1.8009  decode.d2.loss_cls: 0.3337  decode.d2.loss_mask: 1.8796  decode.d2.loss_dice: 1.7713  decode.d3.loss_cls: 0.3094  decode.d3.loss_mask: 1.9468  decode.d3.loss_dice: 1.7895  decode.d4.loss_cls: 0.3362  decode.d4.loss_mask: 1.9307  decode.d4.loss_dice: 1.7948  decode.d5.loss_cls: 0.3920  decode.d5.loss_mask: 1.8942  decode.d5.loss_dice: 1.7928  decode.d6.loss_cls: 0.3048  decode.d6.loss_mask: 1.9012  decode.d6.loss_dice: 1.8377  decode.d7.loss_cls: 0.3349  decode.d7.loss_mask: 1.8785  decode.d7.loss_dice: 1.7996  decode.d8.loss_cls: 0.3380  decode.d8.loss_mask: 1.8750  decode.d8.loss_dice: 1.8070  mix_decode.loss_cls: 0.2188  mix_decode.loss_mask: 0.6461  mix_decode.loss_dice: 0.8445  mix_decode.d0.loss_cls: 0.2095  mix_decode.d0.loss_mask: 0.6584  mix_decode.d0.loss_dice: 0.9092  mix_decode.d1.loss_cls: 0.2557  mix_decode.d1.loss_mask: 0.6522  mix_decode.d1.loss_dice: 0.8116  mix_decode.d2.loss_cls: 0.2148  mix_decode.d2.loss_mask: 0.6563  mix_decode.d2.loss_dice: 0.8169  mix_decode.d3.loss_cls: 0.2357  mix_decode.d3.loss_mask: 0.6467  mix_decode.d3.loss_dice: 0.8087  mix_decode.d4.loss_cls: 0.2501  mix_decode.d4.loss_mask: 0.6630  mix_decode.d4.loss_dice: 0.8347  mix_decode.d5.loss_cls: 0.2041  mix_decode.d5.loss_mask: 0.6668  mix_decode.d5.loss_dice: 0.8321  mix_decode.d6.loss_cls: 0.2483  mix_decode.d6.loss_mask: 0.6552  mix_decode.d6.loss_dice: 0.8318  mix_decode.d7.loss_cls: 0.2165  mix_decode.d7.loss_mask: 0.6407  mix_decode.d7.loss_dice: 0.8264  mix_decode.d8.loss_cls: 0.2366  mix_decode.d8.loss_mask: 0.6397  mix_decode.d8.loss_dice: 0.8357
2025/03/29 15:56:09 - mmengine - INFO - Iter(train) [13150/20000]  base_lr: 3.8125e-05 lr: 3.8125e-05  eta: 1:50:37  time: 1.1507  data_time: 0.0224  memory: 11219  loss: 52.0319  decode.loss_cls: 0.1987  decode.loss_mask: 1.6330  decode.loss_dice: 1.6664  decode.d0.loss_cls: 0.4245  decode.d0.loss_mask: 1.6157  decode.d0.loss_dice: 1.6559  decode.d1.loss_cls: 0.2274  decode.d1.loss_mask: 1.6278  decode.d1.loss_dice: 1.6593  decode.d2.loss_cls: 0.1952  decode.d2.loss_mask: 1.6278  decode.d2.loss_dice: 1.6636  decode.d3.loss_cls: 0.2322  decode.d3.loss_mask: 1.6464  decode.d3.loss_dice: 1.6766  decode.d4.loss_cls: 0.2378  decode.d4.loss_mask: 1.6363  decode.d4.loss_dice: 1.6792  decode.d5.loss_cls: 0.2005  decode.d5.loss_mask: 1.6249  decode.d5.loss_dice: 1.6781  decode.d6.loss_cls: 0.2224  decode.d6.loss_mask: 1.6342  decode.d6.loss_dice: 1.7076  decode.d7.loss_cls: 0.2214  decode.d7.loss_mask: 1.6019  decode.d7.loss_dice: 1.6763  decode.d8.loss_cls: 0.2470  decode.d8.loss_mask: 1.6029  decode.d8.loss_dice: 1.6415  mix_decode.loss_cls: 0.2633  mix_decode.loss_mask: 0.6867  mix_decode.loss_dice: 0.7284  mix_decode.d0.loss_cls: 0.3543  mix_decode.d0.loss_mask: 0.6209  mix_decode.d0.loss_dice: 0.7312  mix_decode.d1.loss_cls: 0.3320  mix_decode.d1.loss_mask: 0.6292  mix_decode.d1.loss_dice: 0.7068  mix_decode.d2.loss_cls: 0.3284  mix_decode.d2.loss_mask: 0.6442  mix_decode.d2.loss_dice: 0.7086  mix_decode.d3.loss_cls: 0.3274  mix_decode.d3.loss_mask: 0.6358  mix_decode.d3.loss_dice: 0.7146  mix_decode.d4.loss_cls: 0.2802  mix_decode.d4.loss_mask: 0.6522  mix_decode.d4.loss_dice: 0.7075  mix_decode.d5.loss_cls: 0.2791  mix_decode.d5.loss_mask: 0.6537  mix_decode.d5.loss_dice: 0.7083  mix_decode.d6.loss_cls: 0.2613  mix_decode.d6.loss_mask: 0.6540  mix_decode.d6.loss_dice: 0.7020  mix_decode.d7.loss_cls: 0.2973  mix_decode.d7.loss_mask: 0.6832  mix_decode.d7.loss_dice: 0.7218  mix_decode.d8.loss_cls: 0.2880  mix_decode.d8.loss_mask: 0.6609  mix_decode.d8.loss_dice: 0.7087
2025/03/29 15:57:07 - mmengine - INFO - Iter(train) [13200/20000]  base_lr: 3.7875e-05 lr: 3.7875e-05  eta: 1:49:53  time: 1.1519  data_time: 0.0229  memory: 11206  loss: 64.3542  decode.loss_cls: 0.4597  decode.loss_mask: 2.1287  decode.loss_dice: 1.8835  decode.d0.loss_cls: 0.5205  decode.d0.loss_mask: 2.1610  decode.d0.loss_dice: 1.9283  decode.d1.loss_cls: 0.4241  decode.d1.loss_mask: 2.1597  decode.d1.loss_dice: 1.8936  decode.d2.loss_cls: 0.4339  decode.d2.loss_mask: 2.1063  decode.d2.loss_dice: 1.8699  decode.d3.loss_cls: 0.5070  decode.d3.loss_mask: 2.1015  decode.d3.loss_dice: 1.8896  decode.d4.loss_cls: 0.4835  decode.d4.loss_mask: 2.1937  decode.d4.loss_dice: 1.9056  decode.d5.loss_cls: 0.5073  decode.d5.loss_mask: 2.1215  decode.d5.loss_dice: 1.8897  decode.d6.loss_cls: 0.5430  decode.d6.loss_mask: 2.0725  decode.d6.loss_dice: 1.8785  decode.d7.loss_cls: 0.4660  decode.d7.loss_mask: 2.1416  decode.d7.loss_dice: 1.8956  decode.d8.loss_cls: 0.4128  decode.d8.loss_mask: 2.1583  decode.d8.loss_dice: 1.8843  mix_decode.loss_cls: 0.2782  mix_decode.loss_mask: 0.7727  mix_decode.loss_dice: 0.8658  mix_decode.d0.loss_cls: 0.3377  mix_decode.d0.loss_mask: 0.8008  mix_decode.d0.loss_dice: 0.9440  mix_decode.d1.loss_cls: 0.3097  mix_decode.d1.loss_mask: 0.7636  mix_decode.d1.loss_dice: 0.8441  mix_decode.d2.loss_cls: 0.3220  mix_decode.d2.loss_mask: 0.7507  mix_decode.d2.loss_dice: 0.8330  mix_decode.d3.loss_cls: 0.2729  mix_decode.d3.loss_mask: 0.7380  mix_decode.d3.loss_dice: 0.8251  mix_decode.d4.loss_cls: 0.2600  mix_decode.d4.loss_mask: 0.7828  mix_decode.d4.loss_dice: 0.8522  mix_decode.d5.loss_cls: 0.2676  mix_decode.d5.loss_mask: 0.7968  mix_decode.d5.loss_dice: 0.8854  mix_decode.d6.loss_cls: 0.2737  mix_decode.d6.loss_mask: 0.7937  mix_decode.d6.loss_dice: 0.8809  mix_decode.d7.loss_cls: 0.3009  mix_decode.d7.loss_mask: 0.7674  mix_decode.d7.loss_dice: 0.8830  mix_decode.d8.loss_cls: 0.2679  mix_decode.d8.loss_mask: 0.7874  mix_decode.d8.loss_dice: 0.8753
2025/03/29 15:58:04 - mmengine - INFO - Iter(train) [13250/20000]  base_lr: 3.7624e-05 lr: 3.7624e-05  eta: 1:49:09  time: 1.1509  data_time: 0.0225  memory: 11207  loss: 57.9393  decode.loss_cls: 0.4126  decode.loss_mask: 2.1523  decode.loss_dice: 1.7896  decode.d0.loss_cls: 0.4936  decode.d0.loss_mask: 2.0832  decode.d0.loss_dice: 1.8417  decode.d1.loss_cls: 0.3642  decode.d1.loss_mask: 2.1124  decode.d1.loss_dice: 1.7760  decode.d2.loss_cls: 0.4065  decode.d2.loss_mask: 2.0989  decode.d2.loss_dice: 1.8053  decode.d3.loss_cls: 0.4112  decode.d3.loss_mask: 2.1298  decode.d3.loss_dice: 1.8144  decode.d4.loss_cls: 0.4285  decode.d4.loss_mask: 2.0750  decode.d4.loss_dice: 1.8514  decode.d5.loss_cls: 0.3962  decode.d5.loss_mask: 2.1655  decode.d5.loss_dice: 1.8296  decode.d6.loss_cls: 0.5308  decode.d6.loss_mask: 2.0639  decode.d6.loss_dice: 1.8168  decode.d7.loss_cls: 0.4419  decode.d7.loss_mask: 2.1852  decode.d7.loss_dice: 1.8406  decode.d8.loss_cls: 0.4453  decode.d8.loss_mask: 2.1403  decode.d8.loss_dice: 1.8157  mix_decode.loss_cls: 0.2177  mix_decode.loss_mask: 0.5379  mix_decode.loss_dice: 0.6659  mix_decode.d0.loss_cls: 0.2813  mix_decode.d0.loss_mask: 0.5122  mix_decode.d0.loss_dice: 0.7213  mix_decode.d1.loss_cls: 0.2030  mix_decode.d1.loss_mask: 0.5143  mix_decode.d1.loss_dice: 0.6630  mix_decode.d2.loss_cls: 0.2036  mix_decode.d2.loss_mask: 0.5521  mix_decode.d2.loss_dice: 0.6672  mix_decode.d3.loss_cls: 0.2280  mix_decode.d3.loss_mask: 0.5077  mix_decode.d3.loss_dice: 0.6584  mix_decode.d4.loss_cls: 0.2237  mix_decode.d4.loss_mask: 0.5043  mix_decode.d4.loss_dice: 0.6793  mix_decode.d5.loss_cls: 0.1713  mix_decode.d5.loss_mask: 0.5329  mix_decode.d5.loss_dice: 0.6970  mix_decode.d6.loss_cls: 0.2622  mix_decode.d6.loss_mask: 0.4967  mix_decode.d6.loss_dice: 0.6662  mix_decode.d7.loss_cls: 0.2098  mix_decode.d7.loss_mask: 0.5194  mix_decode.d7.loss_dice: 0.6738  mix_decode.d8.loss_cls: 0.2295  mix_decode.d8.loss_mask: 0.5418  mix_decode.d8.loss_dice: 0.6794
2025/03/29 15:59:02 - mmengine - INFO - Iter(train) [13300/20000]  base_lr: 3.7373e-05 lr: 3.7373e-05  eta: 1:48:25  time: 1.1522  data_time: 0.0227  memory: 11215  loss: 51.4855  decode.loss_cls: 0.3161  decode.loss_mask: 1.7406  decode.loss_dice: 1.5936  decode.d0.loss_cls: 0.4104  decode.d0.loss_mask: 1.7440  decode.d0.loss_dice: 1.5889  decode.d1.loss_cls: 0.4125  decode.d1.loss_mask: 1.7174  decode.d1.loss_dice: 1.5418  decode.d2.loss_cls: 0.3047  decode.d2.loss_mask: 1.7518  decode.d2.loss_dice: 1.5759  decode.d3.loss_cls: 0.3475  decode.d3.loss_mask: 1.7493  decode.d3.loss_dice: 1.5661  decode.d4.loss_cls: 0.2847  decode.d4.loss_mask: 1.7824  decode.d4.loss_dice: 1.5743  decode.d5.loss_cls: 0.3765  decode.d5.loss_mask: 1.7412  decode.d5.loss_dice: 1.6013  decode.d6.loss_cls: 0.3352  decode.d6.loss_mask: 1.7839  decode.d6.loss_dice: 1.5780  decode.d7.loss_cls: 0.3105  decode.d7.loss_mask: 1.7811  decode.d7.loss_dice: 1.5754  decode.d8.loss_cls: 0.3121  decode.d8.loss_mask: 1.7794  decode.d8.loss_dice: 1.5549  mix_decode.loss_cls: 0.2430  mix_decode.loss_mask: 0.4868  mix_decode.loss_dice: 0.6941  mix_decode.d0.loss_cls: 0.2810  mix_decode.d0.loss_mask: 0.4876  mix_decode.d0.loss_dice: 0.7703  mix_decode.d1.loss_cls: 0.3407  mix_decode.d1.loss_mask: 0.4924  mix_decode.d1.loss_dice: 0.6975  mix_decode.d2.loss_cls: 0.3072  mix_decode.d2.loss_mask: 0.4855  mix_decode.d2.loss_dice: 0.6752  mix_decode.d3.loss_cls: 0.2967  mix_decode.d3.loss_mask: 0.4854  mix_decode.d3.loss_dice: 0.6835  mix_decode.d4.loss_cls: 0.2631  mix_decode.d4.loss_mask: 0.4934  mix_decode.d4.loss_dice: 0.6977  mix_decode.d5.loss_cls: 0.2861  mix_decode.d5.loss_mask: 0.4851  mix_decode.d5.loss_dice: 0.7029  mix_decode.d6.loss_cls: 0.3047  mix_decode.d6.loss_mask: 0.4926  mix_decode.d6.loss_dice: 0.6922  mix_decode.d7.loss_cls: 0.2906  mix_decode.d7.loss_mask: 0.4878  mix_decode.d7.loss_dice: 0.6900  mix_decode.d8.loss_cls: 0.2775  mix_decode.d8.loss_mask: 0.4843  mix_decode.d8.loss_dice: 0.6791
2025/03/29 16:00:00 - mmengine - INFO - Iter(train) [13350/20000]  base_lr: 3.7122e-05 lr: 3.7122e-05  eta: 1:47:41  time: 1.1491  data_time: 0.0224  memory: 11204  loss: 58.7850  decode.loss_cls: 0.5036  decode.loss_mask: 1.8576  decode.loss_dice: 1.7936  decode.d0.loss_cls: 0.5961  decode.d0.loss_mask: 1.8925  decode.d0.loss_dice: 1.8526  decode.d1.loss_cls: 0.6019  decode.d1.loss_mask: 1.8253  decode.d1.loss_dice: 1.7960  decode.d2.loss_cls: 0.5815  decode.d2.loss_mask: 1.8628  decode.d2.loss_dice: 1.8238  decode.d3.loss_cls: 0.6040  decode.d3.loss_mask: 1.8477  decode.d3.loss_dice: 1.7851  decode.d4.loss_cls: 0.5018  decode.d4.loss_mask: 1.8446  decode.d4.loss_dice: 1.8057  decode.d5.loss_cls: 0.5513  decode.d5.loss_mask: 1.8409  decode.d5.loss_dice: 1.7922  decode.d6.loss_cls: 0.5203  decode.d6.loss_mask: 1.8393  decode.d6.loss_dice: 1.7726  decode.d7.loss_cls: 0.5440  decode.d7.loss_mask: 1.8928  decode.d7.loss_dice: 1.8144  decode.d8.loss_cls: 0.4366  decode.d8.loss_mask: 1.9052  decode.d8.loss_dice: 1.8360  mix_decode.loss_cls: 0.2021  mix_decode.loss_mask: 0.6423  mix_decode.loss_dice: 0.7933  mix_decode.d0.loss_cls: 0.2696  mix_decode.d0.loss_mask: 0.6237  mix_decode.d0.loss_dice: 0.8606  mix_decode.d1.loss_cls: 0.2362  mix_decode.d1.loss_mask: 0.5966  mix_decode.d1.loss_dice: 0.7848  mix_decode.d2.loss_cls: 0.2379  mix_decode.d2.loss_mask: 0.6170  mix_decode.d2.loss_dice: 0.7986  mix_decode.d3.loss_cls: 0.2680  mix_decode.d3.loss_mask: 0.6098  mix_decode.d3.loss_dice: 0.7751  mix_decode.d4.loss_cls: 0.2350  mix_decode.d4.loss_mask: 0.6145  mix_decode.d4.loss_dice: 0.8080  mix_decode.d5.loss_cls: 0.2366  mix_decode.d5.loss_mask: 0.6150  mix_decode.d5.loss_dice: 0.7985  mix_decode.d6.loss_cls: 0.2369  mix_decode.d6.loss_mask: 0.6195  mix_decode.d6.loss_dice: 0.8013  mix_decode.d7.loss_cls: 0.2556  mix_decode.d7.loss_mask: 0.6374  mix_decode.d7.loss_dice: 0.8033  mix_decode.d8.loss_cls: 0.2365  mix_decode.d8.loss_mask: 0.6438  mix_decode.d8.loss_dice: 0.8060
2025/03/29 16:00:57 - mmengine - INFO - Iter(train) [13400/20000]  base_lr: 3.6871e-05 lr: 3.6871e-05  eta: 1:46:57  time: 1.1554  data_time: 0.0225  memory: 11217  loss: 59.2418  decode.loss_cls: 0.4268  decode.loss_mask: 1.9308  decode.loss_dice: 1.7934  decode.d0.loss_cls: 0.4349  decode.d0.loss_mask: 1.9119  decode.d0.loss_dice: 1.9226  decode.d1.loss_cls: 0.4866  decode.d1.loss_mask: 1.8945  decode.d1.loss_dice: 1.8302  decode.d2.loss_cls: 0.4894  decode.d2.loss_mask: 1.9057  decode.d2.loss_dice: 1.7664  decode.d3.loss_cls: 0.4073  decode.d3.loss_mask: 1.9445  decode.d3.loss_dice: 1.7649  decode.d4.loss_cls: 0.3879  decode.d4.loss_mask: 1.9324  decode.d4.loss_dice: 1.8113  decode.d5.loss_cls: 0.5212  decode.d5.loss_mask: 1.9416  decode.d5.loss_dice: 1.8147  decode.d6.loss_cls: 0.5708  decode.d6.loss_mask: 1.9038  decode.d6.loss_dice: 1.8286  decode.d7.loss_cls: 0.4970  decode.d7.loss_mask: 1.8883  decode.d7.loss_dice: 1.8092  decode.d8.loss_cls: 0.4039  decode.d8.loss_mask: 1.9441  decode.d8.loss_dice: 1.7733  mix_decode.loss_cls: 0.3158  mix_decode.loss_mask: 0.5782  mix_decode.loss_dice: 0.8031  mix_decode.d0.loss_cls: 0.3442  mix_decode.d0.loss_mask: 0.5943  mix_decode.d0.loss_dice: 0.9010  mix_decode.d1.loss_cls: 0.3369  mix_decode.d1.loss_mask: 0.5838  mix_decode.d1.loss_dice: 0.8210  mix_decode.d2.loss_cls: 0.2874  mix_decode.d2.loss_mask: 0.5869  mix_decode.d2.loss_dice: 0.8007  mix_decode.d3.loss_cls: 0.2932  mix_decode.d3.loss_mask: 0.5943  mix_decode.d3.loss_dice: 0.8112  mix_decode.d4.loss_cls: 0.2776  mix_decode.d4.loss_mask: 0.6180  mix_decode.d4.loss_dice: 0.8349  mix_decode.d5.loss_cls: 0.3249  mix_decode.d5.loss_mask: 0.5916  mix_decode.d5.loss_dice: 0.8407  mix_decode.d6.loss_cls: 0.2711  mix_decode.d6.loss_mask: 0.6116  mix_decode.d6.loss_dice: 0.8437  mix_decode.d7.loss_cls: 0.3144  mix_decode.d7.loss_mask: 0.5831  mix_decode.d7.loss_dice: 0.8325  mix_decode.d8.loss_cls: 0.2791  mix_decode.d8.loss_mask: 0.5991  mix_decode.d8.loss_dice: 0.8298
2025/03/29 16:01:55 - mmengine - INFO - Iter(train) [13450/20000]  base_lr: 3.6619e-05 lr: 3.6619e-05  eta: 1:46:13  time: 1.1500  data_time: 0.0229  memory: 11209  loss: 60.0414  decode.loss_cls: 0.4412  decode.loss_mask: 2.0306  decode.loss_dice: 1.7744  decode.d0.loss_cls: 0.5205  decode.d0.loss_mask: 2.0875  decode.d0.loss_dice: 1.8361  decode.d1.loss_cls: 0.5416  decode.d1.loss_mask: 2.0265  decode.d1.loss_dice: 1.7602  decode.d2.loss_cls: 0.4588  decode.d2.loss_mask: 2.0785  decode.d2.loss_dice: 1.7582  decode.d3.loss_cls: 0.5660  decode.d3.loss_mask: 1.9867  decode.d3.loss_dice: 1.7051  decode.d4.loss_cls: 0.5290  decode.d4.loss_mask: 2.0163  decode.d4.loss_dice: 1.7231  decode.d5.loss_cls: 0.4630  decode.d5.loss_mask: 2.0332  decode.d5.loss_dice: 1.7915  decode.d6.loss_cls: 0.5631  decode.d6.loss_mask: 1.9086  decode.d6.loss_dice: 1.7172  decode.d7.loss_cls: 0.4976  decode.d7.loss_mask: 1.9959  decode.d7.loss_dice: 1.7603  decode.d8.loss_cls: 0.4390  decode.d8.loss_mask: 1.9865  decode.d8.loss_dice: 1.7849  mix_decode.loss_cls: 0.2845  mix_decode.loss_mask: 0.6880  mix_decode.loss_dice: 0.7186  mix_decode.d0.loss_cls: 0.2790  mix_decode.d0.loss_mask: 0.7293  mix_decode.d0.loss_dice: 0.7915  mix_decode.d1.loss_cls: 0.2517  mix_decode.d1.loss_mask: 0.7117  mix_decode.d1.loss_dice: 0.7486  mix_decode.d2.loss_cls: 0.2826  mix_decode.d2.loss_mask: 0.7082  mix_decode.d2.loss_dice: 0.7536  mix_decode.d3.loss_cls: 0.2670  mix_decode.d3.loss_mask: 0.6860  mix_decode.d3.loss_dice: 0.7455  mix_decode.d4.loss_cls: 0.2981  mix_decode.d4.loss_mask: 0.6994  mix_decode.d4.loss_dice: 0.7330  mix_decode.d5.loss_cls: 0.2831  mix_decode.d5.loss_mask: 0.7188  mix_decode.d5.loss_dice: 0.7329  mix_decode.d6.loss_cls: 0.2913  mix_decode.d6.loss_mask: 0.6863  mix_decode.d6.loss_dice: 0.7355  mix_decode.d7.loss_cls: 0.2754  mix_decode.d7.loss_mask: 0.7147  mix_decode.d7.loss_dice: 0.7422  mix_decode.d8.loss_cls: 0.2649  mix_decode.d8.loss_mask: 0.7016  mix_decode.d8.loss_dice: 0.7376
2025/03/29 16:02:52 - mmengine - INFO - Iter(train) [13500/20000]  base_lr: 3.6368e-05 lr: 3.6368e-05  eta: 1:45:29  time: 1.1486  data_time: 0.0225  memory: 11220  loss: 55.4835  decode.loss_cls: 0.4198  decode.loss_mask: 1.8787  decode.loss_dice: 1.6331  decode.d0.loss_cls: 0.5373  decode.d0.loss_mask: 1.7861  decode.d0.loss_dice: 1.6096  decode.d1.loss_cls: 0.5439  decode.d1.loss_mask: 1.8337  decode.d1.loss_dice: 1.5613  decode.d2.loss_cls: 0.5357  decode.d2.loss_mask: 1.8114  decode.d2.loss_dice: 1.5388  decode.d3.loss_cls: 0.4735  decode.d3.loss_mask: 1.8309  decode.d3.loss_dice: 1.5818  decode.d4.loss_cls: 0.5278  decode.d4.loss_mask: 1.8001  decode.d4.loss_dice: 1.5736  decode.d5.loss_cls: 0.5107  decode.d5.loss_mask: 1.8125  decode.d5.loss_dice: 1.6026  decode.d6.loss_cls: 0.6006  decode.d6.loss_mask: 1.8296  decode.d6.loss_dice: 1.5688  decode.d7.loss_cls: 0.5270  decode.d7.loss_mask: 1.8487  decode.d7.loss_dice: 1.5653  decode.d8.loss_cls: 0.4221  decode.d8.loss_mask: 1.8522  decode.d8.loss_dice: 1.5944  mix_decode.loss_cls: 0.3039  mix_decode.loss_mask: 0.5846  mix_decode.loss_dice: 0.6978  mix_decode.d0.loss_cls: 0.3835  mix_decode.d0.loss_mask: 0.5919  mix_decode.d0.loss_dice: 0.8039  mix_decode.d1.loss_cls: 0.2980  mix_decode.d1.loss_mask: 0.6150  mix_decode.d1.loss_dice: 0.7412  mix_decode.d2.loss_cls: 0.2945  mix_decode.d2.loss_mask: 0.5930  mix_decode.d2.loss_dice: 0.7130  mix_decode.d3.loss_cls: 0.3253  mix_decode.d3.loss_mask: 0.5831  mix_decode.d3.loss_dice: 0.6810  mix_decode.d4.loss_cls: 0.3383  mix_decode.d4.loss_mask: 0.5881  mix_decode.d4.loss_dice: 0.7045  mix_decode.d5.loss_cls: 0.3051  mix_decode.d5.loss_mask: 0.5640  mix_decode.d5.loss_dice: 0.7074  mix_decode.d6.loss_cls: 0.3154  mix_decode.d6.loss_mask: 0.5921  mix_decode.d6.loss_dice: 0.7021  mix_decode.d7.loss_cls: 0.3307  mix_decode.d7.loss_mask: 0.5759  mix_decode.d7.loss_dice: 0.7194  mix_decode.d8.loss_cls: 0.3373  mix_decode.d8.loss_mask: 0.5683  mix_decode.d8.loss_dice: 0.7134
2025/03/29 16:03:50 - mmengine - INFO - Iter(train) [13550/20000]  base_lr: 3.6116e-05 lr: 3.6116e-05  eta: 1:44:44  time: 1.1515  data_time: 0.0226  memory: 11206  loss: 49.5669  decode.loss_cls: 0.2815  decode.loss_mask: 1.7110  decode.loss_dice: 1.5712  decode.d0.loss_cls: 0.4008  decode.d0.loss_mask: 1.7404  decode.d0.loss_dice: 1.5984  decode.d1.loss_cls: 0.3080  decode.d1.loss_mask: 1.7103  decode.d1.loss_dice: 1.5760  decode.d2.loss_cls: 0.3139  decode.d2.loss_mask: 1.7088  decode.d2.loss_dice: 1.5656  decode.d3.loss_cls: 0.3108  decode.d3.loss_mask: 1.7225  decode.d3.loss_dice: 1.5630  decode.d4.loss_cls: 0.3251  decode.d4.loss_mask: 1.7225  decode.d4.loss_dice: 1.5834  decode.d5.loss_cls: 0.3281  decode.d5.loss_mask: 1.7591  decode.d5.loss_dice: 1.5743  decode.d6.loss_cls: 0.3102  decode.d6.loss_mask: 1.7267  decode.d6.loss_dice: 1.5830  decode.d7.loss_cls: 0.2917  decode.d7.loss_mask: 1.7330  decode.d7.loss_dice: 1.6095  decode.d8.loss_cls: 0.2735  decode.d8.loss_mask: 1.7407  decode.d8.loss_dice: 1.6123  mix_decode.loss_cls: 0.2502  mix_decode.loss_mask: 0.5222  mix_decode.loss_dice: 0.5440  mix_decode.d0.loss_cls: 0.3558  mix_decode.d0.loss_mask: 0.4707  mix_decode.d0.loss_dice: 0.5992  mix_decode.d1.loss_cls: 0.2426  mix_decode.d1.loss_mask: 0.5099  mix_decode.d1.loss_dice: 0.5519  mix_decode.d2.loss_cls: 0.2426  mix_decode.d2.loss_mask: 0.5137  mix_decode.d2.loss_dice: 0.5625  mix_decode.d3.loss_cls: 0.2300  mix_decode.d3.loss_mask: 0.5199  mix_decode.d3.loss_dice: 0.5568  mix_decode.d4.loss_cls: 0.2459  mix_decode.d4.loss_mask: 0.4875  mix_decode.d4.loss_dice: 0.5544  mix_decode.d5.loss_cls: 0.2495  mix_decode.d5.loss_mask: 0.5053  mix_decode.d5.loss_dice: 0.5460  mix_decode.d6.loss_cls: 0.2586  mix_decode.d6.loss_mask: 0.5090  mix_decode.d6.loss_dice: 0.5568  mix_decode.d7.loss_cls: 0.2731  mix_decode.d7.loss_mask: 0.4960  mix_decode.d7.loss_dice: 0.5641  mix_decode.d8.loss_cls: 0.2586  mix_decode.d8.loss_mask: 0.5449  mix_decode.d8.loss_dice: 0.5900
2025/03/29 16:04:47 - mmengine - INFO - Iter(train) [13600/20000]  base_lr: 3.5864e-05 lr: 3.5864e-05  eta: 1:44:00  time: 1.1554  data_time: 0.0233  memory: 11205  loss: 56.6014  decode.loss_cls: 0.3746  decode.loss_mask: 1.7749  decode.loss_dice: 1.7036  decode.d0.loss_cls: 0.4814  decode.d0.loss_mask: 1.7728  decode.d0.loss_dice: 1.7768  decode.d1.loss_cls: 0.4024  decode.d1.loss_mask: 1.7680  decode.d1.loss_dice: 1.7048  decode.d2.loss_cls: 0.3817  decode.d2.loss_mask: 1.7666  decode.d2.loss_dice: 1.7043  decode.d3.loss_cls: 0.4059  decode.d3.loss_mask: 1.7967  decode.d3.loss_dice: 1.7479  decode.d4.loss_cls: 0.3405  decode.d4.loss_mask: 1.7986  decode.d4.loss_dice: 1.7373  decode.d5.loss_cls: 0.3558  decode.d5.loss_mask: 1.7815  decode.d5.loss_dice: 1.7522  decode.d6.loss_cls: 0.3622  decode.d6.loss_mask: 1.8055  decode.d6.loss_dice: 1.7418  decode.d7.loss_cls: 0.3240  decode.d7.loss_mask: 1.7849  decode.d7.loss_dice: 1.7456  decode.d8.loss_cls: 0.3363  decode.d8.loss_mask: 1.7778  decode.d8.loss_dice: 1.7179  mix_decode.loss_cls: 0.3079  mix_decode.loss_mask: 0.6651  mix_decode.loss_dice: 0.7765  mix_decode.d0.loss_cls: 0.3360  mix_decode.d0.loss_mask: 0.6970  mix_decode.d0.loss_dice: 0.8256  mix_decode.d1.loss_cls: 0.3015  mix_decode.d1.loss_mask: 0.6426  mix_decode.d1.loss_dice: 0.7589  mix_decode.d2.loss_cls: 0.2946  mix_decode.d2.loss_mask: 0.7074  mix_decode.d2.loss_dice: 0.7781  mix_decode.d3.loss_cls: 0.2814  mix_decode.d3.loss_mask: 0.7186  mix_decode.d3.loss_dice: 0.7594  mix_decode.d4.loss_cls: 0.3038  mix_decode.d4.loss_mask: 0.6608  mix_decode.d4.loss_dice: 0.7722  mix_decode.d5.loss_cls: 0.2587  mix_decode.d5.loss_mask: 0.7195  mix_decode.d5.loss_dice: 0.7865  mix_decode.d6.loss_cls: 0.2858  mix_decode.d6.loss_mask: 0.6966  mix_decode.d6.loss_dice: 0.7675  mix_decode.d7.loss_cls: 0.3333  mix_decode.d7.loss_mask: 0.6847  mix_decode.d7.loss_dice: 0.7866  mix_decode.d8.loss_cls: 0.2907  mix_decode.d8.loss_mask: 0.6989  mix_decode.d8.loss_dice: 0.7813
2025/03/29 16:05:45 - mmengine - INFO - Iter(train) [13650/20000]  base_lr: 3.5611e-05 lr: 3.5611e-05  eta: 1:43:15  time: 1.1524  data_time: 0.0224  memory: 11211  loss: 55.6519  decode.loss_cls: 0.2259  decode.loss_mask: 1.8882  decode.loss_dice: 1.7180  decode.d0.loss_cls: 0.3885  decode.d0.loss_mask: 1.9042  decode.d0.loss_dice: 1.7653  decode.d1.loss_cls: 0.2440  decode.d1.loss_mask: 1.9493  decode.d1.loss_dice: 1.7471  decode.d2.loss_cls: 0.2220  decode.d2.loss_mask: 1.9574  decode.d2.loss_dice: 1.7315  decode.d3.loss_cls: 0.2863  decode.d3.loss_mask: 1.8793  decode.d3.loss_dice: 1.7149  decode.d4.loss_cls: 0.1896  decode.d4.loss_mask: 1.9696  decode.d4.loss_dice: 1.7652  decode.d5.loss_cls: 0.2283  decode.d5.loss_mask: 1.9400  decode.d5.loss_dice: 1.7445  decode.d6.loss_cls: 0.3076  decode.d6.loss_mask: 1.8967  decode.d6.loss_dice: 1.7118  decode.d7.loss_cls: 0.2508  decode.d7.loss_mask: 1.9006  decode.d7.loss_dice: 1.7439  decode.d8.loss_cls: 0.2469  decode.d8.loss_mask: 1.9356  decode.d8.loss_dice: 1.7578  mix_decode.loss_cls: 0.2400  mix_decode.loss_mask: 0.6347  mix_decode.loss_dice: 0.7049  mix_decode.d0.loss_cls: 0.2719  mix_decode.d0.loss_mask: 0.6419  mix_decode.d0.loss_dice: 0.7801  mix_decode.d1.loss_cls: 0.2632  mix_decode.d1.loss_mask: 0.6615  mix_decode.d1.loss_dice: 0.7628  mix_decode.d2.loss_cls: 0.2414  mix_decode.d2.loss_mask: 0.6610  mix_decode.d2.loss_dice: 0.7474  mix_decode.d3.loss_cls: 0.2598  mix_decode.d3.loss_mask: 0.6628  mix_decode.d3.loss_dice: 0.7290  mix_decode.d4.loss_cls: 0.2366  mix_decode.d4.loss_mask: 0.6486  mix_decode.d4.loss_dice: 0.7292  mix_decode.d5.loss_cls: 0.2270  mix_decode.d5.loss_mask: 0.6781  mix_decode.d5.loss_dice: 0.7552  mix_decode.d6.loss_cls: 0.2517  mix_decode.d6.loss_mask: 0.6455  mix_decode.d6.loss_dice: 0.7320  mix_decode.d7.loss_cls: 0.2223  mix_decode.d7.loss_mask: 0.6820  mix_decode.d7.loss_dice: 0.7510  mix_decode.d8.loss_cls: 0.2446  mix_decode.d8.loss_mask: 0.6496  mix_decode.d8.loss_dice: 0.7251
2025/03/29 16:06:43 - mmengine - INFO - Iter(train) [13700/20000]  base_lr: 3.5359e-05 lr: 3.5359e-05  eta: 1:42:30  time: 1.1485  data_time: 0.0225  memory: 11214  loss: 53.3666  decode.loss_cls: 0.2412  decode.loss_mask: 1.8268  decode.loss_dice: 1.6002  decode.d0.loss_cls: 0.3429  decode.d0.loss_mask: 1.8673  decode.d0.loss_dice: 1.6232  decode.d1.loss_cls: 0.2342  decode.d1.loss_mask: 1.8163  decode.d1.loss_dice: 1.6186  decode.d2.loss_cls: 0.2163  decode.d2.loss_mask: 1.8187  decode.d2.loss_dice: 1.6497  decode.d3.loss_cls: 0.2158  decode.d3.loss_mask: 1.8254  decode.d3.loss_dice: 1.6406  decode.d4.loss_cls: 0.1949  decode.d4.loss_mask: 1.8235  decode.d4.loss_dice: 1.6394  decode.d5.loss_cls: 0.2197  decode.d5.loss_mask: 1.8176  decode.d5.loss_dice: 1.6721  decode.d6.loss_cls: 0.1908  decode.d6.loss_mask: 1.8111  decode.d6.loss_dice: 1.6744  decode.d7.loss_cls: 0.2217  decode.d7.loss_mask: 1.8267  decode.d7.loss_dice: 1.6732  decode.d8.loss_cls: 0.2250  decode.d8.loss_mask: 1.8143  decode.d8.loss_dice: 1.6638  mix_decode.loss_cls: 0.1825  mix_decode.loss_mask: 0.6376  mix_decode.loss_dice: 0.7683  mix_decode.d0.loss_cls: 0.2467  mix_decode.d0.loss_mask: 0.6507  mix_decode.d0.loss_dice: 0.8442  mix_decode.d1.loss_cls: 0.2159  mix_decode.d1.loss_mask: 0.6316  mix_decode.d1.loss_dice: 0.7922  mix_decode.d2.loss_cls: 0.2121  mix_decode.d2.loss_mask: 0.6228  mix_decode.d2.loss_dice: 0.7830  mix_decode.d3.loss_cls: 0.1933  mix_decode.d3.loss_mask: 0.6229  mix_decode.d3.loss_dice: 0.7797  mix_decode.d4.loss_cls: 0.2225  mix_decode.d4.loss_mask: 0.6359  mix_decode.d4.loss_dice: 0.7762  mix_decode.d5.loss_cls: 0.1906  mix_decode.d5.loss_mask: 0.6633  mix_decode.d5.loss_dice: 0.7906  mix_decode.d6.loss_cls: 0.2132  mix_decode.d6.loss_mask: 0.6329  mix_decode.d6.loss_dice: 0.7863  mix_decode.d7.loss_cls: 0.2209  mix_decode.d7.loss_mask: 0.6370  mix_decode.d7.loss_dice: 0.7895  mix_decode.d8.loss_cls: 0.1951  mix_decode.d8.loss_mask: 0.6395  mix_decode.d8.loss_dice: 0.7843
2025/03/29 16:07:40 - mmengine - INFO - Iter(train) [13750/20000]  base_lr: 3.5106e-05 lr: 3.5106e-05  eta: 1:41:45  time: 1.1489  data_time: 0.0229  memory: 11219  loss: 49.0833  decode.loss_cls: 0.3202  decode.loss_mask: 1.6783  decode.loss_dice: 1.5836  decode.d0.loss_cls: 0.4669  decode.d0.loss_mask: 1.6453  decode.d0.loss_dice: 1.5158  decode.d1.loss_cls: 0.3277  decode.d1.loss_mask: 1.6307  decode.d1.loss_dice: 1.5557  decode.d2.loss_cls: 0.2889  decode.d2.loss_mask: 1.6618  decode.d2.loss_dice: 1.5722  decode.d3.loss_cls: 0.2435  decode.d3.loss_mask: 1.6734  decode.d3.loss_dice: 1.5901  decode.d4.loss_cls: 0.2532  decode.d4.loss_mask: 1.6746  decode.d4.loss_dice: 1.5883  decode.d5.loss_cls: 0.2626  decode.d5.loss_mask: 1.6818  decode.d5.loss_dice: 1.6080  decode.d6.loss_cls: 0.2342  decode.d6.loss_mask: 1.6945  decode.d6.loss_dice: 1.5953  decode.d7.loss_cls: 0.3347  decode.d7.loss_mask: 1.6473  decode.d7.loss_dice: 1.5390  decode.d8.loss_cls: 0.3090  decode.d8.loss_mask: 1.6788  decode.d8.loss_dice: 1.5894  mix_decode.loss_cls: 0.1328  mix_decode.loss_mask: 0.5701  mix_decode.loss_dice: 0.6387  mix_decode.d0.loss_cls: 0.2435  mix_decode.d0.loss_mask: 0.5378  mix_decode.d0.loss_dice: 0.6761  mix_decode.d1.loss_cls: 0.1890  mix_decode.d1.loss_mask: 0.5342  mix_decode.d1.loss_dice: 0.6514  mix_decode.d2.loss_cls: 0.1342  mix_decode.d2.loss_mask: 0.5424  mix_decode.d2.loss_dice: 0.6467  mix_decode.d3.loss_cls: 0.1475  mix_decode.d3.loss_mask: 0.5396  mix_decode.d3.loss_dice: 0.6396  mix_decode.d4.loss_cls: 0.1295  mix_decode.d4.loss_mask: 0.5688  mix_decode.d4.loss_dice: 0.6530  mix_decode.d5.loss_cls: 0.1452  mix_decode.d5.loss_mask: 0.5507  mix_decode.d5.loss_dice: 0.6564  mix_decode.d6.loss_cls: 0.1505  mix_decode.d6.loss_mask: 0.5803  mix_decode.d6.loss_dice: 0.6554  mix_decode.d7.loss_cls: 0.1413  mix_decode.d7.loss_mask: 0.5579  mix_decode.d7.loss_dice: 0.6714  mix_decode.d8.loss_cls: 0.1486  mix_decode.d8.loss_mask: 0.5570  mix_decode.d8.loss_dice: 0.6488
2025/03/29 16:08:38 - mmengine - INFO - Iter(train) [13800/20000]  base_lr: 3.4853e-05 lr: 3.4853e-05  eta: 1:41:00  time: 1.1615  data_time: 0.0239  memory: 11214  loss: 55.4358  decode.loss_cls: 0.2370  decode.loss_mask: 1.9905  decode.loss_dice: 1.7655  decode.d0.loss_cls: 0.4968  decode.d0.loss_mask: 1.9162  decode.d0.loss_dice: 1.6823  decode.d1.loss_cls: 0.3263  decode.d1.loss_mask: 1.9541  decode.d1.loss_dice: 1.6962  decode.d2.loss_cls: 0.2930  decode.d2.loss_mask: 1.9901  decode.d2.loss_dice: 1.7485  decode.d3.loss_cls: 0.2891  decode.d3.loss_mask: 1.9494  decode.d3.loss_dice: 1.7171  decode.d4.loss_cls: 0.2467  decode.d4.loss_mask: 1.9750  decode.d4.loss_dice: 1.7420  decode.d5.loss_cls: 0.3345  decode.d5.loss_mask: 1.9648  decode.d5.loss_dice: 1.7290  decode.d6.loss_cls: 0.2948  decode.d6.loss_mask: 1.9703  decode.d6.loss_dice: 1.7635  decode.d7.loss_cls: 0.2915  decode.d7.loss_mask: 1.9571  decode.d7.loss_dice: 1.7425  decode.d8.loss_cls: 0.2695  decode.d8.loss_mask: 1.9511  decode.d8.loss_dice: 1.7870  mix_decode.loss_cls: 0.2179  mix_decode.loss_mask: 0.6215  mix_decode.loss_dice: 0.6490  mix_decode.d0.loss_cls: 0.3041  mix_decode.d0.loss_mask: 0.6414  mix_decode.d0.loss_dice: 0.6823  mix_decode.d1.loss_cls: 0.2488  mix_decode.d1.loss_mask: 0.6441  mix_decode.d1.loss_dice: 0.6491  mix_decode.d2.loss_cls: 0.2227  mix_decode.d2.loss_mask: 0.6594  mix_decode.d2.loss_dice: 0.6199  mix_decode.d3.loss_cls: 0.2148  mix_decode.d3.loss_mask: 0.6484  mix_decode.d3.loss_dice: 0.6541  mix_decode.d4.loss_cls: 0.1986  mix_decode.d4.loss_mask: 0.6579  mix_decode.d4.loss_dice: 0.6742  mix_decode.d5.loss_cls: 0.2002  mix_decode.d5.loss_mask: 0.6621  mix_decode.d5.loss_dice: 0.6884  mix_decode.d6.loss_cls: 0.2255  mix_decode.d6.loss_mask: 0.6667  mix_decode.d6.loss_dice: 0.6668  mix_decode.d7.loss_cls: 0.2182  mix_decode.d7.loss_mask: 0.6460  mix_decode.d7.loss_dice: 0.6555  mix_decode.d8.loss_cls: 0.2171  mix_decode.d8.loss_mask: 0.6329  mix_decode.d8.loss_dice: 0.6770
2025/03/29 16:09:35 - mmengine - INFO - Iter(train) [13850/20000]  base_lr: 3.4600e-05 lr: 3.4600e-05  eta: 1:40:15  time: 1.1512  data_time: 0.0226  memory: 11214  loss: 57.1443  decode.loss_cls: 0.2665  decode.loss_mask: 2.0638  decode.loss_dice: 1.6356  decode.d0.loss_cls: 0.4525  decode.d0.loss_mask: 2.0682  decode.d0.loss_dice: 1.5842  decode.d1.loss_cls: 0.2826  decode.d1.loss_mask: 2.0770  decode.d1.loss_dice: 1.6077  decode.d2.loss_cls: 0.2723  decode.d2.loss_mask: 2.0613  decode.d2.loss_dice: 1.6149  decode.d3.loss_cls: 0.3378  decode.d3.loss_mask: 2.0229  decode.d3.loss_dice: 1.6158  decode.d4.loss_cls: 0.3259  decode.d4.loss_mask: 2.0437  decode.d4.loss_dice: 1.6449  decode.d5.loss_cls: 0.3438  decode.d5.loss_mask: 2.0094  decode.d5.loss_dice: 1.6074  decode.d6.loss_cls: 0.3004  decode.d6.loss_mask: 2.0644  decode.d6.loss_dice: 1.6197  decode.d7.loss_cls: 0.3254  decode.d7.loss_mask: 2.0473  decode.d7.loss_dice: 1.6053  decode.d8.loss_cls: 0.2293  decode.d8.loss_mask: 2.1015  decode.d8.loss_dice: 1.6496  mix_decode.loss_cls: 0.2560  mix_decode.loss_mask: 0.7438  mix_decode.loss_dice: 0.7125  mix_decode.d0.loss_cls: 0.3115  mix_decode.d0.loss_mask: 0.7322  mix_decode.d0.loss_dice: 0.7517  mix_decode.d1.loss_cls: 0.2760  mix_decode.d1.loss_mask: 0.7094  mix_decode.d1.loss_dice: 0.6801  mix_decode.d2.loss_cls: 0.2695  mix_decode.d2.loss_mask: 0.7579  mix_decode.d2.loss_dice: 0.7293  mix_decode.d3.loss_cls: 0.2923  mix_decode.d3.loss_mask: 0.7455  mix_decode.d3.loss_dice: 0.6954  mix_decode.d4.loss_cls: 0.2921  mix_decode.d4.loss_mask: 0.7280  mix_decode.d4.loss_dice: 0.7094  mix_decode.d5.loss_cls: 0.2863  mix_decode.d5.loss_mask: 0.7436  mix_decode.d5.loss_dice: 0.6861  mix_decode.d6.loss_cls: 0.2565  mix_decode.d6.loss_mask: 0.7197  mix_decode.d6.loss_dice: 0.7080  mix_decode.d7.loss_cls: 0.2887  mix_decode.d7.loss_mask: 0.7198  mix_decode.d7.loss_dice: 0.6996  mix_decode.d8.loss_cls: 0.2677  mix_decode.d8.loss_mask: 0.7601  mix_decode.d8.loss_dice: 0.7345
2025/03/29 16:10:33 - mmengine - INFO - Iter(train) [13900/20000]  base_lr: 3.4347e-05 lr: 3.4347e-05  eta: 1:39:30  time: 1.1709  data_time: 0.0235  memory: 11211  loss: 57.2112  decode.loss_cls: 0.3333  decode.loss_mask: 1.8570  decode.loss_dice: 1.8179  decode.d0.loss_cls: 0.4316  decode.d0.loss_mask: 1.8516  decode.d0.loss_dice: 1.9468  decode.d1.loss_cls: 0.4147  decode.d1.loss_mask: 1.8530  decode.d1.loss_dice: 1.7932  decode.d2.loss_cls: 0.3850  decode.d2.loss_mask: 1.8350  decode.d2.loss_dice: 1.8315  decode.d3.loss_cls: 0.3744  decode.d3.loss_mask: 1.8447  decode.d3.loss_dice: 1.8495  decode.d4.loss_cls: 0.4412  decode.d4.loss_mask: 1.8061  decode.d4.loss_dice: 1.8573  decode.d5.loss_cls: 0.3946  decode.d5.loss_mask: 1.8450  decode.d5.loss_dice: 1.7777  decode.d6.loss_cls: 0.3786  decode.d6.loss_mask: 1.8507  decode.d6.loss_dice: 1.8190  decode.d7.loss_cls: 0.4397  decode.d7.loss_mask: 1.8535  decode.d7.loss_dice: 1.8142  decode.d8.loss_cls: 0.4618  decode.d8.loss_mask: 1.8387  decode.d8.loss_dice: 1.8457  mix_decode.loss_cls: 0.3011  mix_decode.loss_mask: 0.5666  mix_decode.loss_dice: 0.7351  mix_decode.d0.loss_cls: 0.3187  mix_decode.d0.loss_mask: 0.5810  mix_decode.d0.loss_dice: 0.8099  mix_decode.d1.loss_cls: 0.3286  mix_decode.d1.loss_mask: 0.5590  mix_decode.d1.loss_dice: 0.7144  mix_decode.d2.loss_cls: 0.3567  mix_decode.d2.loss_mask: 0.5618  mix_decode.d2.loss_dice: 0.7110  mix_decode.d3.loss_cls: 0.3399  mix_decode.d3.loss_mask: 0.5583  mix_decode.d3.loss_dice: 0.7316  mix_decode.d4.loss_cls: 0.3150  mix_decode.d4.loss_mask: 0.5702  mix_decode.d4.loss_dice: 0.7610  mix_decode.d5.loss_cls: 0.3063  mix_decode.d5.loss_mask: 0.5625  mix_decode.d5.loss_dice: 0.7573  mix_decode.d6.loss_cls: 0.3509  mix_decode.d6.loss_mask: 0.5620  mix_decode.d6.loss_dice: 0.7423  mix_decode.d7.loss_cls: 0.3372  mix_decode.d7.loss_mask: 0.5511  mix_decode.d7.loss_dice: 0.7189  mix_decode.d8.loss_cls: 0.3669  mix_decode.d8.loss_mask: 0.5606  mix_decode.d8.loss_dice: 0.7323
2025/03/29 16:11:31 - mmengine - INFO - Iter(train) [13950/20000]  base_lr: 3.4094e-05 lr: 3.4094e-05  eta: 1:38:45  time: 1.1598  data_time: 0.0239  memory: 11214  loss: 54.3390  decode.loss_cls: 0.3361  decode.loss_mask: 1.7659  decode.loss_dice: 1.8023  decode.d0.loss_cls: 0.4165  decode.d0.loss_mask: 1.8021  decode.d0.loss_dice: 1.7889  decode.d1.loss_cls: 0.3590  decode.d1.loss_mask: 1.7613  decode.d1.loss_dice: 1.8001  decode.d2.loss_cls: 0.3242  decode.d2.loss_mask: 1.8058  decode.d2.loss_dice: 1.7904  decode.d3.loss_cls: 0.3796  decode.d3.loss_mask: 1.7821  decode.d3.loss_dice: 1.8267  decode.d4.loss_cls: 0.3499  decode.d4.loss_mask: 1.7678  decode.d4.loss_dice: 1.7924  decode.d5.loss_cls: 0.3386  decode.d5.loss_mask: 1.7532  decode.d5.loss_dice: 1.7946  decode.d6.loss_cls: 0.3741  decode.d6.loss_mask: 1.8045  decode.d6.loss_dice: 1.8404  decode.d7.loss_cls: 0.2911  decode.d7.loss_mask: 1.7762  decode.d7.loss_dice: 1.8168  decode.d8.loss_cls: 0.3366  decode.d8.loss_mask: 1.7601  decode.d8.loss_dice: 1.7987  mix_decode.loss_cls: 0.2192  mix_decode.loss_mask: 0.5917  mix_decode.loss_dice: 0.6537  mix_decode.d0.loss_cls: 0.2739  mix_decode.d0.loss_mask: 0.5926  mix_decode.d0.loss_dice: 0.7072  mix_decode.d1.loss_cls: 0.1973  mix_decode.d1.loss_mask: 0.6206  mix_decode.d1.loss_dice: 0.6912  mix_decode.d2.loss_cls: 0.2089  mix_decode.d2.loss_mask: 0.5920  mix_decode.d2.loss_dice: 0.6695  mix_decode.d3.loss_cls: 0.2273  mix_decode.d3.loss_mask: 0.5836  mix_decode.d3.loss_dice: 0.6571  mix_decode.d4.loss_cls: 0.2438  mix_decode.d4.loss_mask: 0.5984  mix_decode.d4.loss_dice: 0.6519  mix_decode.d5.loss_cls: 0.2634  mix_decode.d5.loss_mask: 0.5893  mix_decode.d5.loss_dice: 0.6628  mix_decode.d6.loss_cls: 0.2376  mix_decode.d6.loss_mask: 0.6013  mix_decode.d6.loss_dice: 0.6806  mix_decode.d7.loss_cls: 0.2453  mix_decode.d7.loss_mask: 0.6036  mix_decode.d7.loss_dice: 0.6621  mix_decode.d8.loss_cls: 0.2145  mix_decode.d8.loss_mask: 0.5986  mix_decode.d8.loss_dice: 0.6640
2025/03/29 16:12:29 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 16:12:29 - mmengine - INFO - Iter(train) [14000/20000]  base_lr: 3.3840e-05 lr: 3.3840e-05  eta: 1:38:00  time: 1.1560  data_time: 0.0229  memory: 11209  loss: 54.0651  decode.loss_cls: 0.4666  decode.loss_mask: 1.8416  decode.loss_dice: 1.7041  decode.d0.loss_cls: 0.5229  decode.d0.loss_mask: 1.8655  decode.d0.loss_dice: 1.7649  decode.d1.loss_cls: 0.4646  decode.d1.loss_mask: 1.8593  decode.d1.loss_dice: 1.7603  decode.d2.loss_cls: 0.4426  decode.d2.loss_mask: 1.8181  decode.d2.loss_dice: 1.7437  decode.d3.loss_cls: 0.5035  decode.d3.loss_mask: 1.7997  decode.d3.loss_dice: 1.7083  decode.d4.loss_cls: 0.5034  decode.d4.loss_mask: 1.7989  decode.d4.loss_dice: 1.7367  decode.d5.loss_cls: 0.4837  decode.d5.loss_mask: 1.8156  decode.d5.loss_dice: 1.6829  decode.d6.loss_cls: 0.5023  decode.d6.loss_mask: 1.8055  decode.d6.loss_dice: 1.6806  decode.d7.loss_cls: 0.4811  decode.d7.loss_mask: 1.7693  decode.d7.loss_dice: 1.6694  decode.d8.loss_cls: 0.4981  decode.d8.loss_mask: 1.7728  decode.d8.loss_dice: 1.6995  mix_decode.loss_cls: 0.1368  mix_decode.loss_mask: 0.5478  mix_decode.loss_dice: 0.6598  mix_decode.d0.loss_cls: 0.2174  mix_decode.d0.loss_mask: 0.5630  mix_decode.d0.loss_dice: 0.7124  mix_decode.d1.loss_cls: 0.1433  mix_decode.d1.loss_mask: 0.5798  mix_decode.d1.loss_dice: 0.6933  mix_decode.d2.loss_cls: 0.1266  mix_decode.d2.loss_mask: 0.5684  mix_decode.d2.loss_dice: 0.6811  mix_decode.d3.loss_cls: 0.1446  mix_decode.d3.loss_mask: 0.5616  mix_decode.d3.loss_dice: 0.6870  mix_decode.d4.loss_cls: 0.1438  mix_decode.d4.loss_mask: 0.5551  mix_decode.d4.loss_dice: 0.6710  mix_decode.d5.loss_cls: 0.1395  mix_decode.d5.loss_mask: 0.5455  mix_decode.d5.loss_dice: 0.6669  mix_decode.d6.loss_cls: 0.1155  mix_decode.d6.loss_mask: 0.5795  mix_decode.d6.loss_dice: 0.6927  mix_decode.d7.loss_cls: 0.1471  mix_decode.d7.loss_mask: 0.5544  mix_decode.d7.loss_dice: 0.6751  mix_decode.d8.loss_cls: 0.1788  mix_decode.d8.loss_mask: 0.5478  mix_decode.d8.loss_dice: 0.6639
2025/03/29 16:12:29 - mmengine - INFO - Saving checkpoint at 14000 iterations
2025/03/29 16:12:34 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:06:01  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:12:39 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:55  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 16:12:44 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:50  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 16:12:48 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:45  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:12:53 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:41  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:12:57 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:36  time: 0.0922  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:02 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:32  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:07 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:27  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:11 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:22  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:16 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:18  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:20 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:13  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:25 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:09  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:30 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:04  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:34 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:05:00  time: 0.0924  data_time: 0.0019  memory: 3083  
2025/03/29 16:13:39 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:55  time: 0.0924  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:43 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:50  time: 0.0911  data_time: 0.0017  memory: 3083  
2025/03/29 16:13:48 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:46  time: 0.0912  data_time: 0.0017  memory: 3083  
2025/03/29 16:13:53 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:41  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 16:13:57 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:36  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:02 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:32  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:06 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:27  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:11 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:23  time: 0.0913  data_time: 0.0017  memory: 3083  
2025/03/29 16:14:15 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:18  time: 0.0912  data_time: 0.0017  memory: 3083  
2025/03/29 16:14:20 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:13  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:25 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:09  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:29 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:04  time: 0.0913  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:34 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:04:00  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:38 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:55  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:43 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:47 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:46  time: 0.0913  data_time: 0.0017  memory: 3083  
2025/03/29 16:14:52 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 16:14:57 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:37  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:01 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0915  data_time: 0.0017  memory: 3083  
2025/03/29 16:15:06 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:27  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:10 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:23  time: 0.0915  data_time: 0.0017  memory: 3083  
2025/03/29 16:15:15 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:20 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:14  time: 0.0914  data_time: 0.0017  memory: 3083  
2025/03/29 16:15:24 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:29 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:04  time: 0.0915  data_time: 0.0017  memory: 3083  
2025/03/29 16:15:33 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:38 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:43 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:51  time: 0.0916  data_time: 0.0017  memory: 3083  
2025/03/29 16:15:47 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:52 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:42  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:15:56 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:01 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:05 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:10 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0941  data_time: 0.0020  memory: 3083  
2025/03/29 16:16:15 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:19  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:19 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0917  data_time: 0.0019  memory: 3083  
2025/03/29 16:16:24 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:28 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0948  data_time: 0.0020  memory: 3083  
2025/03/29 16:16:33 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0918  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:38 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0918  data_time: 0.0017  memory: 3083  
2025/03/29 16:16:42 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:47 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:47  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 16:16:51 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0918  data_time: 0.0019  memory: 3083  
2025/03/29 16:16:56 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0916  data_time: 0.0017  memory: 3083  
2025/03/29 16:17:01 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:05 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:10 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0917  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:15 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:19 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:15  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:24 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:28 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0916  data_time: 0.0017  memory: 3083  
2025/03/29 16:17:33 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:37 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:42 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0917  data_time: 0.0017  memory: 3083  
2025/03/29 16:17:47 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0917  data_time: 0.0017  memory: 3083  
2025/03/29 16:17:51 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:17:56 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0919  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:00 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:05 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:10 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0934  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:14 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:19  time: 0.0923  data_time: 0.0019  memory: 3083  
2025/03/29 16:18:19 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0915  data_time: 0.0017  memory: 3083  
2025/03/29 16:18:23 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0914  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:28 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0915  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:33 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0916  data_time: 0.0018  memory: 3083  
2025/03/29 16:18:34 - mmengine - INFO - per class results:
2025/03/29 16:18:34 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 56.31 | 77.23 |
|   building   | 63.35 | 78.73 |
|     road     | 48.68 |  54.3 |
|    water     | 66.61 | 76.14 |
|    barren    | 13.67 | 17.97 |
|    forest    | 38.44 |  51.3 |
| agricultural | 63.18 | 75.32 |
+--------------+-------+-------+
2025/03/29 16:18:34 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 72.5100  mIoU: 50.0300  mAcc: 61.5700  data_time: 0.0018  time: 0.0917
2025/03/29 16:18:34 - mmengine - INFO - The previous best checkpoint /data/xiaoxinghhh/code/my_mmcv/work_dirs/u2r/DA_spatial_32_fft_cut_off_0.2_suf6_scale_0.1/f1460_seed0/best_mIoU_iter_12000.pth is removed
2025/03/29 16:18:35 - mmengine - INFO - The best checkpoint with 50.0300 mIoU at 14000 iter is saved to best_mIoU_iter_14000.pth.
2025/03/29 16:19:34 - mmengine - INFO - Iter(train) [14050/20000]  base_lr: 3.3586e-05 lr: 3.3586e-05  eta: 1:37:16  time: 1.1531  data_time: 0.0234  memory: 11212  loss: 58.4567  decode.loss_cls: 0.3318  decode.loss_mask: 1.8852  decode.loss_dice: 1.8560  decode.d0.loss_cls: 0.4157  decode.d0.loss_mask: 1.9149  decode.d0.loss_dice: 1.8462  decode.d1.loss_cls: 0.3764  decode.d1.loss_mask: 1.8603  decode.d1.loss_dice: 1.8109  decode.d2.loss_cls: 0.3324  decode.d2.loss_mask: 1.8469  decode.d2.loss_dice: 1.7831  decode.d3.loss_cls: 0.3584  decode.d3.loss_mask: 1.8731  decode.d3.loss_dice: 1.8258  decode.d4.loss_cls: 0.3631  decode.d4.loss_mask: 1.8896  decode.d4.loss_dice: 1.7929  decode.d5.loss_cls: 0.3846  decode.d5.loss_mask: 1.8622  decode.d5.loss_dice: 1.7714  decode.d6.loss_cls: 0.4096  decode.d6.loss_mask: 1.8561  decode.d6.loss_dice: 1.7811  decode.d7.loss_cls: 0.3871  decode.d7.loss_mask: 1.8694  decode.d7.loss_dice: 1.7968  decode.d8.loss_cls: 0.3391  decode.d8.loss_mask: 1.8859  decode.d8.loss_dice: 1.8384  mix_decode.loss_cls: 0.1866  mix_decode.loss_mask: 0.7020  mix_decode.loss_dice: 0.8463  mix_decode.d0.loss_cls: 0.3258  mix_decode.d0.loss_mask: 0.6583  mix_decode.d0.loss_dice: 0.8879  mix_decode.d1.loss_cls: 0.2638  mix_decode.d1.loss_mask: 0.6745  mix_decode.d1.loss_dice: 0.8314  mix_decode.d2.loss_cls: 0.1997  mix_decode.d2.loss_mask: 0.7100  mix_decode.d2.loss_dice: 0.8719  mix_decode.d3.loss_cls: 0.2244  mix_decode.d3.loss_mask: 0.7053  mix_decode.d3.loss_dice: 0.8573  mix_decode.d4.loss_cls: 0.2316  mix_decode.d4.loss_mask: 0.7144  mix_decode.d4.loss_dice: 0.8472  mix_decode.d5.loss_cls: 0.2213  mix_decode.d5.loss_mask: 0.7083  mix_decode.d5.loss_dice: 0.8347  mix_decode.d6.loss_cls: 0.2156  mix_decode.d6.loss_mask: 0.7235  mix_decode.d6.loss_dice: 0.8686  mix_decode.d7.loss_cls: 0.2419  mix_decode.d7.loss_mask: 0.7137  mix_decode.d7.loss_dice: 0.8715  mix_decode.d8.loss_cls: 0.2332  mix_decode.d8.loss_mask: 0.7057  mix_decode.d8.loss_dice: 0.8353
2025/03/29 16:20:32 - mmengine - INFO - Iter(train) [14100/20000]  base_lr: 3.3332e-05 lr: 3.3332e-05  eta: 1:36:30  time: 1.1611  data_time: 0.0242  memory: 11223  loss: 56.3026  decode.loss_cls: 0.4432  decode.loss_mask: 1.8448  decode.loss_dice: 1.5543  decode.d0.loss_cls: 0.5221  decode.d0.loss_mask: 1.9227  decode.d0.loss_dice: 1.6420  decode.d1.loss_cls: 0.4983  decode.d1.loss_mask: 1.8612  decode.d1.loss_dice: 1.6045  decode.d2.loss_cls: 0.4401  decode.d2.loss_mask: 1.8242  decode.d2.loss_dice: 1.5784  decode.d3.loss_cls: 0.4890  decode.d3.loss_mask: 1.8471  decode.d3.loss_dice: 1.6291  decode.d4.loss_cls: 0.4028  decode.d4.loss_mask: 1.8572  decode.d4.loss_dice: 1.5845  decode.d5.loss_cls: 0.4550  decode.d5.loss_mask: 1.8533  decode.d5.loss_dice: 1.6461  decode.d6.loss_cls: 0.4454  decode.d6.loss_mask: 1.8617  decode.d6.loss_dice: 1.5750  decode.d7.loss_cls: 0.4482  decode.d7.loss_mask: 1.8370  decode.d7.loss_dice: 1.5701  decode.d8.loss_cls: 0.4899  decode.d8.loss_mask: 1.8082  decode.d8.loss_dice: 1.5700  mix_decode.loss_cls: 0.2357  mix_decode.loss_mask: 0.7719  mix_decode.loss_dice: 0.6774  mix_decode.d0.loss_cls: 0.3360  mix_decode.d0.loss_mask: 0.7571  mix_decode.d0.loss_dice: 0.7479  mix_decode.d1.loss_cls: 0.2880  mix_decode.d1.loss_mask: 0.7567  mix_decode.d1.loss_dice: 0.6798  mix_decode.d2.loss_cls: 0.2364  mix_decode.d2.loss_mask: 0.7486  mix_decode.d2.loss_dice: 0.6949  mix_decode.d3.loss_cls: 0.2818  mix_decode.d3.loss_mask: 0.7575  mix_decode.d3.loss_dice: 0.6943  mix_decode.d4.loss_cls: 0.2749  mix_decode.d4.loss_mask: 0.7366  mix_decode.d4.loss_dice: 0.6970  mix_decode.d5.loss_cls: 0.2648  mix_decode.d5.loss_mask: 0.7296  mix_decode.d5.loss_dice: 0.7216  mix_decode.d6.loss_cls: 0.2886  mix_decode.d6.loss_mask: 0.7241  mix_decode.d6.loss_dice: 0.7095  mix_decode.d7.loss_cls: 0.2642  mix_decode.d7.loss_mask: 0.7575  mix_decode.d7.loss_dice: 0.7093  mix_decode.d8.loss_cls: 0.2556  mix_decode.d8.loss_mask: 0.7262  mix_decode.d8.loss_dice: 0.6735
2025/03/29 16:21:29 - mmengine - INFO - Iter(train) [14150/20000]  base_lr: 3.3078e-05 lr: 3.3078e-05  eta: 1:35:45  time: 1.1503  data_time: 0.0227  memory: 11208  loss: 61.6415  decode.loss_cls: 0.4962  decode.loss_mask: 1.9654  decode.loss_dice: 1.8482  decode.d0.loss_cls: 0.5080  decode.d0.loss_mask: 1.9871  decode.d0.loss_dice: 1.9137  decode.d1.loss_cls: 0.5920  decode.d1.loss_mask: 1.9645  decode.d1.loss_dice: 1.8212  decode.d2.loss_cls: 0.6007  decode.d2.loss_mask: 1.9199  decode.d2.loss_dice: 1.8270  decode.d3.loss_cls: 0.4704  decode.d3.loss_mask: 1.9119  decode.d3.loss_dice: 1.8311  decode.d4.loss_cls: 0.4449  decode.d4.loss_mask: 1.9440  decode.d4.loss_dice: 1.8370  decode.d5.loss_cls: 0.5097  decode.d5.loss_mask: 1.9870  decode.d5.loss_dice: 1.8601  decode.d6.loss_cls: 0.5359  decode.d6.loss_mask: 1.9572  decode.d6.loss_dice: 1.8587  decode.d7.loss_cls: 0.5513  decode.d7.loss_mask: 1.9458  decode.d7.loss_dice: 1.8493  decode.d8.loss_cls: 0.5004  decode.d8.loss_mask: 1.9360  decode.d8.loss_dice: 1.8594  mix_decode.loss_cls: 0.2654  mix_decode.loss_mask: 0.7461  mix_decode.loss_dice: 0.8186  mix_decode.d0.loss_cls: 0.3227  mix_decode.d0.loss_mask: 0.7355  mix_decode.d0.loss_dice: 0.8569  mix_decode.d1.loss_cls: 0.2800  mix_decode.d1.loss_mask: 0.7225  mix_decode.d1.loss_dice: 0.8195  mix_decode.d2.loss_cls: 0.2567  mix_decode.d2.loss_mask: 0.7470  mix_decode.d2.loss_dice: 0.8023  mix_decode.d3.loss_cls: 0.2912  mix_decode.d3.loss_mask: 0.7589  mix_decode.d3.loss_dice: 0.8022  mix_decode.d4.loss_cls: 0.2282  mix_decode.d4.loss_mask: 0.7466  mix_decode.d4.loss_dice: 0.8353  mix_decode.d5.loss_cls: 0.2428  mix_decode.d5.loss_mask: 0.7505  mix_decode.d5.loss_dice: 0.8282  mix_decode.d6.loss_cls: 0.2669  mix_decode.d6.loss_mask: 0.7550  mix_decode.d6.loss_dice: 0.8322  mix_decode.d7.loss_cls: 0.3431  mix_decode.d7.loss_mask: 0.7275  mix_decode.d7.loss_dice: 0.8197  mix_decode.d8.loss_cls: 0.2583  mix_decode.d8.loss_mask: 0.7287  mix_decode.d8.loss_dice: 0.8187
2025/03/29 16:22:27 - mmengine - INFO - Iter(train) [14200/20000]  base_lr: 3.2823e-05 lr: 3.2823e-05  eta: 1:34:59  time: 1.1522  data_time: 0.0227  memory: 11220  loss: 54.7551  decode.loss_cls: 0.3338  decode.loss_mask: 1.9426  decode.loss_dice: 1.6881  decode.d0.loss_cls: 0.4929  decode.d0.loss_mask: 1.8647  decode.d0.loss_dice: 1.6507  decode.d1.loss_cls: 0.4198  decode.d1.loss_mask: 1.8135  decode.d1.loss_dice: 1.5990  decode.d2.loss_cls: 0.4781  decode.d2.loss_mask: 1.8065  decode.d2.loss_dice: 1.6420  decode.d3.loss_cls: 0.3742  decode.d3.loss_mask: 1.9240  decode.d3.loss_dice: 1.6529  decode.d4.loss_cls: 0.3324  decode.d4.loss_mask: 1.8998  decode.d4.loss_dice: 1.6603  decode.d5.loss_cls: 0.4287  decode.d5.loss_mask: 1.8279  decode.d5.loss_dice: 1.6360  decode.d6.loss_cls: 0.3883  decode.d6.loss_mask: 1.8201  decode.d6.loss_dice: 1.6246  decode.d7.loss_cls: 0.3744  decode.d7.loss_mask: 1.8728  decode.d7.loss_dice: 1.6657  decode.d8.loss_cls: 0.3468  decode.d8.loss_mask: 1.8334  decode.d8.loss_dice: 1.6543  mix_decode.loss_cls: 0.2684  mix_decode.loss_mask: 0.5803  mix_decode.loss_dice: 0.7243  mix_decode.d0.loss_cls: 0.3281  mix_decode.d0.loss_mask: 0.5786  mix_decode.d0.loss_dice: 0.7479  mix_decode.d1.loss_cls: 0.2849  mix_decode.d1.loss_mask: 0.5588  mix_decode.d1.loss_dice: 0.7056  mix_decode.d2.loss_cls: 0.2739  mix_decode.d2.loss_mask: 0.5787  mix_decode.d2.loss_dice: 0.6966  mix_decode.d3.loss_cls: 0.2875  mix_decode.d3.loss_mask: 0.5809  mix_decode.d3.loss_dice: 0.7032  mix_decode.d4.loss_cls: 0.2848  mix_decode.d4.loss_mask: 0.5700  mix_decode.d4.loss_dice: 0.7007  mix_decode.d5.loss_cls: 0.2623  mix_decode.d5.loss_mask: 0.6065  mix_decode.d5.loss_dice: 0.7045  mix_decode.d6.loss_cls: 0.2133  mix_decode.d6.loss_mask: 0.6007  mix_decode.d6.loss_dice: 0.7248  mix_decode.d7.loss_cls: 0.2456  mix_decode.d7.loss_mask: 0.6198  mix_decode.d7.loss_dice: 0.7299  mix_decode.d8.loss_cls: 0.2531  mix_decode.d8.loss_mask: 0.6019  mix_decode.d8.loss_dice: 0.6911
2025/03/29 16:23:25 - mmengine - INFO - Iter(train) [14250/20000]  base_lr: 3.2568e-05 lr: 3.2568e-05  eta: 1:34:13  time: 1.1519  data_time: 0.0229  memory: 11231  loss: 48.7859  decode.loss_cls: 0.4440  decode.loss_mask: 1.4982  decode.loss_dice: 1.6077  decode.d0.loss_cls: 0.6418  decode.d0.loss_mask: 1.4599  decode.d0.loss_dice: 1.5595  decode.d1.loss_cls: 0.4361  decode.d1.loss_mask: 1.4445  decode.d1.loss_dice: 1.5628  decode.d2.loss_cls: 0.4540  decode.d2.loss_mask: 1.4541  decode.d2.loss_dice: 1.5458  decode.d3.loss_cls: 0.4189  decode.d3.loss_mask: 1.4969  decode.d3.loss_dice: 1.5597  decode.d4.loss_cls: 0.5095  decode.d4.loss_mask: 1.5036  decode.d4.loss_dice: 1.5662  decode.d5.loss_cls: 0.5417  decode.d5.loss_mask: 1.4672  decode.d5.loss_dice: 1.5464  decode.d6.loss_cls: 0.4696  decode.d6.loss_mask: 1.4551  decode.d6.loss_dice: 1.5515  decode.d7.loss_cls: 0.5187  decode.d7.loss_mask: 1.4476  decode.d7.loss_dice: 1.5413  decode.d8.loss_cls: 0.3997  decode.d8.loss_mask: 1.5159  decode.d8.loss_dice: 1.6129  mix_decode.loss_cls: 0.1173  mix_decode.loss_mask: 0.5469  mix_decode.loss_dice: 0.6587  mix_decode.d0.loss_cls: 0.1954  mix_decode.d0.loss_mask: 0.5424  mix_decode.d0.loss_dice: 0.7132  mix_decode.d1.loss_cls: 0.1730  mix_decode.d1.loss_mask: 0.5422  mix_decode.d1.loss_dice: 0.6437  mix_decode.d2.loss_cls: 0.1167  mix_decode.d2.loss_mask: 0.5451  mix_decode.d2.loss_dice: 0.6382  mix_decode.d3.loss_cls: 0.1175  mix_decode.d3.loss_mask: 0.5471  mix_decode.d3.loss_dice: 0.6502  mix_decode.d4.loss_cls: 0.1186  mix_decode.d4.loss_mask: 0.5435  mix_decode.d4.loss_dice: 0.6575  mix_decode.d5.loss_cls: 0.1814  mix_decode.d5.loss_mask: 0.5516  mix_decode.d5.loss_dice: 0.6471  mix_decode.d6.loss_cls: 0.1379  mix_decode.d6.loss_mask: 0.5522  mix_decode.d6.loss_dice: 0.6558  mix_decode.d7.loss_cls: 0.1882  mix_decode.d7.loss_mask: 0.5438  mix_decode.d7.loss_dice: 0.6708  mix_decode.d8.loss_cls: 0.1734  mix_decode.d8.loss_mask: 0.5423  mix_decode.d8.loss_dice: 0.6438
2025/03/29 16:24:23 - mmengine - INFO - Iter(train) [14300/20000]  base_lr: 3.2313e-05 lr: 3.2313e-05  eta: 1:33:28  time: 1.1592  data_time: 0.0231  memory: 11224  loss: 58.5554  decode.loss_cls: 0.4636  decode.loss_mask: 1.9743  decode.loss_dice: 1.7237  decode.d0.loss_cls: 0.4878  decode.d0.loss_mask: 2.0876  decode.d0.loss_dice: 1.8555  decode.d1.loss_cls: 0.4955  decode.d1.loss_mask: 1.9768  decode.d1.loss_dice: 1.7247  decode.d2.loss_cls: 0.3928  decode.d2.loss_mask: 2.0391  decode.d2.loss_dice: 1.7406  decode.d3.loss_cls: 0.4717  decode.d3.loss_mask: 1.9837  decode.d3.loss_dice: 1.7050  decode.d4.loss_cls: 0.4557  decode.d4.loss_mask: 2.0347  decode.d4.loss_dice: 1.7096  decode.d5.loss_cls: 0.5107  decode.d5.loss_mask: 2.0154  decode.d5.loss_dice: 1.7224  decode.d6.loss_cls: 0.3873  decode.d6.loss_mask: 2.0586  decode.d6.loss_dice: 1.7492  decode.d7.loss_cls: 0.3988  decode.d7.loss_mask: 2.0625  decode.d7.loss_dice: 1.7858  decode.d8.loss_cls: 0.4211  decode.d8.loss_mask: 2.0215  decode.d8.loss_dice: 1.7616  mix_decode.loss_cls: 0.2082  mix_decode.loss_mask: 0.6844  mix_decode.loss_dice: 0.7158  mix_decode.d0.loss_cls: 0.2900  mix_decode.d0.loss_mask: 0.6725  mix_decode.d0.loss_dice: 0.8180  mix_decode.d1.loss_cls: 0.2620  mix_decode.d1.loss_mask: 0.6701  mix_decode.d1.loss_dice: 0.7096  mix_decode.d2.loss_cls: 0.2339  mix_decode.d2.loss_mask: 0.6835  mix_decode.d2.loss_dice: 0.7103  mix_decode.d3.loss_cls: 0.1926  mix_decode.d3.loss_mask: 0.6815  mix_decode.d3.loss_dice: 0.7090  mix_decode.d4.loss_cls: 0.2049  mix_decode.d4.loss_mask: 0.7044  mix_decode.d4.loss_dice: 0.7329  mix_decode.d5.loss_cls: 0.2481  mix_decode.d5.loss_mask: 0.6791  mix_decode.d5.loss_dice: 0.7133  mix_decode.d6.loss_cls: 0.2221  mix_decode.d6.loss_mask: 0.6762  mix_decode.d6.loss_dice: 0.6976  mix_decode.d7.loss_cls: 0.2346  mix_decode.d7.loss_mask: 0.6623  mix_decode.d7.loss_dice: 0.6944  mix_decode.d8.loss_cls: 0.2382  mix_decode.d8.loss_mask: 0.6702  mix_decode.d8.loss_dice: 0.7183
2025/03/29 16:25:21 - mmengine - INFO - Iter(train) [14350/20000]  base_lr: 3.2058e-05 lr: 3.2058e-05  eta: 1:32:42  time: 1.1502  data_time: 0.0223  memory: 11216  loss: 56.2784  decode.loss_cls: 0.3746  decode.loss_mask: 1.8525  decode.loss_dice: 1.7214  decode.d0.loss_cls: 0.4991  decode.d0.loss_mask: 1.7964  decode.d0.loss_dice: 1.7379  decode.d1.loss_cls: 0.3511  decode.d1.loss_mask: 1.8230  decode.d1.loss_dice: 1.7319  decode.d2.loss_cls: 0.3719  decode.d2.loss_mask: 1.7913  decode.d2.loss_dice: 1.7076  decode.d3.loss_cls: 0.4005  decode.d3.loss_mask: 1.8514  decode.d3.loss_dice: 1.7484  decode.d4.loss_cls: 0.3802  decode.d4.loss_mask: 1.8412  decode.d4.loss_dice: 1.7333  decode.d5.loss_cls: 0.3803  decode.d5.loss_mask: 1.8071  decode.d5.loss_dice: 1.7182  decode.d6.loss_cls: 0.4260  decode.d6.loss_mask: 1.8311  decode.d6.loss_dice: 1.7445  decode.d7.loss_cls: 0.3620  decode.d7.loss_mask: 1.8739  decode.d7.loss_dice: 1.7507  decode.d8.loss_cls: 0.3858  decode.d8.loss_mask: 1.8664  decode.d8.loss_dice: 1.7660  mix_decode.loss_cls: 0.3238  mix_decode.loss_mask: 0.5897  mix_decode.loss_dice: 0.7460  mix_decode.d0.loss_cls: 0.3375  mix_decode.d0.loss_mask: 0.5734  mix_decode.d0.loss_dice: 0.7934  mix_decode.d1.loss_cls: 0.3588  mix_decode.d1.loss_mask: 0.5775  mix_decode.d1.loss_dice: 0.7289  mix_decode.d2.loss_cls: 0.3014  mix_decode.d2.loss_mask: 0.5838  mix_decode.d2.loss_dice: 0.7385  mix_decode.d3.loss_cls: 0.2890  mix_decode.d3.loss_mask: 0.6157  mix_decode.d3.loss_dice: 0.7230  mix_decode.d4.loss_cls: 0.2941  mix_decode.d4.loss_mask: 0.6147  mix_decode.d4.loss_dice: 0.7361  mix_decode.d5.loss_cls: 0.3492  mix_decode.d5.loss_mask: 0.5914  mix_decode.d5.loss_dice: 0.7516  mix_decode.d6.loss_cls: 0.3127  mix_decode.d6.loss_mask: 0.5978  mix_decode.d6.loss_dice: 0.7447  mix_decode.d7.loss_cls: 0.3305  mix_decode.d7.loss_mask: 0.6020  mix_decode.d7.loss_dice: 0.7486  mix_decode.d8.loss_cls: 0.3290  mix_decode.d8.loss_mask: 0.6167  mix_decode.d8.loss_dice: 0.7534
2025/03/29 16:26:19 - mmengine - INFO - Iter(train) [14400/20000]  base_lr: 3.1803e-05 lr: 3.1803e-05  eta: 1:31:56  time: 1.1559  data_time: 0.0231  memory: 11215  loss: 56.8624  decode.loss_cls: 0.3859  decode.loss_mask: 1.8178  decode.loss_dice: 1.8724  decode.d0.loss_cls: 0.5121  decode.d0.loss_mask: 1.8188  decode.d0.loss_dice: 1.8771  decode.d1.loss_cls: 0.4812  decode.d1.loss_mask: 1.7988  decode.d1.loss_dice: 1.8442  decode.d2.loss_cls: 0.4201  decode.d2.loss_mask: 1.8475  decode.d2.loss_dice: 1.8723  decode.d3.loss_cls: 0.4193  decode.d3.loss_mask: 1.7974  decode.d3.loss_dice: 1.8919  decode.d4.loss_cls: 0.3910  decode.d4.loss_mask: 1.8553  decode.d4.loss_dice: 1.9210  decode.d5.loss_cls: 0.3594  decode.d5.loss_mask: 1.8390  decode.d5.loss_dice: 1.8960  decode.d6.loss_cls: 0.3982  decode.d6.loss_mask: 1.8513  decode.d6.loss_dice: 1.8865  decode.d7.loss_cls: 0.3760  decode.d7.loss_mask: 1.8429  decode.d7.loss_dice: 1.8854  decode.d8.loss_cls: 0.3749  decode.d8.loss_mask: 1.8089  decode.d8.loss_dice: 1.8707  mix_decode.loss_cls: 0.1902  mix_decode.loss_mask: 0.5706  mix_decode.loss_dice: 0.7806  mix_decode.d0.loss_cls: 0.2571  mix_decode.d0.loss_mask: 0.5819  mix_decode.d0.loss_dice: 0.8336  mix_decode.d1.loss_cls: 0.2687  mix_decode.d1.loss_mask: 0.5320  mix_decode.d1.loss_dice: 0.7373  mix_decode.d2.loss_cls: 0.2598  mix_decode.d2.loss_mask: 0.5489  mix_decode.d2.loss_dice: 0.7417  mix_decode.d3.loss_cls: 0.2456  mix_decode.d3.loss_mask: 0.5519  mix_decode.d3.loss_dice: 0.7558  mix_decode.d4.loss_cls: 0.2206  mix_decode.d4.loss_mask: 0.5669  mix_decode.d4.loss_dice: 0.7596  mix_decode.d5.loss_cls: 0.2152  mix_decode.d5.loss_mask: 0.5731  mix_decode.d5.loss_dice: 0.7811  mix_decode.d6.loss_cls: 0.2133  mix_decode.d6.loss_mask: 0.5714  mix_decode.d6.loss_dice: 0.7707  mix_decode.d7.loss_cls: 0.2477  mix_decode.d7.loss_mask: 0.5576  mix_decode.d7.loss_dice: 0.7680  mix_decode.d8.loss_cls: 0.2203  mix_decode.d8.loss_mask: 0.5605  mix_decode.d8.loss_dice: 0.7677
2025/03/29 16:27:16 - mmengine - INFO - Iter(train) [14450/20000]  base_lr: 3.1547e-05 lr: 3.1547e-05  eta: 1:31:10  time: 1.1512  data_time: 0.0230  memory: 11212  loss: 56.0757  decode.loss_cls: 0.4145  decode.loss_mask: 1.7741  decode.loss_dice: 1.6678  decode.d0.loss_cls: 0.4411  decode.d0.loss_mask: 1.7665  decode.d0.loss_dice: 1.7123  decode.d1.loss_cls: 0.4412  decode.d1.loss_mask: 1.7319  decode.d1.loss_dice: 1.6908  decode.d2.loss_cls: 0.4133  decode.d2.loss_mask: 1.7625  decode.d2.loss_dice: 1.7097  decode.d3.loss_cls: 0.3894  decode.d3.loss_mask: 1.7523  decode.d3.loss_dice: 1.7006  decode.d4.loss_cls: 0.3937  decode.d4.loss_mask: 1.7745  decode.d4.loss_dice: 1.7231  decode.d5.loss_cls: 0.3321  decode.d5.loss_mask: 1.8172  decode.d5.loss_dice: 1.7521  decode.d6.loss_cls: 0.3456  decode.d6.loss_mask: 1.7693  decode.d6.loss_dice: 1.7158  decode.d7.loss_cls: 0.3287  decode.d7.loss_mask: 1.8074  decode.d7.loss_dice: 1.7065  decode.d8.loss_cls: 0.3854  decode.d8.loss_mask: 1.8243  decode.d8.loss_dice: 1.6891  mix_decode.loss_cls: 0.2504  mix_decode.loss_mask: 0.7087  mix_decode.loss_dice: 0.7734  mix_decode.d0.loss_cls: 0.2908  mix_decode.d0.loss_mask: 0.6955  mix_decode.d0.loss_dice: 0.8293  mix_decode.d1.loss_cls: 0.2417  mix_decode.d1.loss_mask: 0.7176  mix_decode.d1.loss_dice: 0.7828  mix_decode.d2.loss_cls: 0.2334  mix_decode.d2.loss_mask: 0.7355  mix_decode.d2.loss_dice: 0.7795  mix_decode.d3.loss_cls: 0.2394  mix_decode.d3.loss_mask: 0.6966  mix_decode.d3.loss_dice: 0.7760  mix_decode.d4.loss_cls: 0.2327  mix_decode.d4.loss_mask: 0.7056  mix_decode.d4.loss_dice: 0.7770  mix_decode.d5.loss_cls: 0.2162  mix_decode.d5.loss_mask: 0.7069  mix_decode.d5.loss_dice: 0.7831  mix_decode.d6.loss_cls: 0.2495  mix_decode.d6.loss_mask: 0.6895  mix_decode.d6.loss_dice: 0.7703  mix_decode.d7.loss_cls: 0.2312  mix_decode.d7.loss_mask: 0.7117  mix_decode.d7.loss_dice: 0.7789  mix_decode.d8.loss_cls: 0.2543  mix_decode.d8.loss_mask: 0.7257  mix_decode.d8.loss_dice: 0.7602
2025/03/29 16:28:14 - mmengine - INFO - Iter(train) [14500/20000]  base_lr: 3.1291e-05 lr: 3.1291e-05  eta: 1:30:24  time: 1.1502  data_time: 0.0225  memory: 11216  loss: 50.2402  decode.loss_cls: 0.5528  decode.loss_mask: 1.3609  decode.loss_dice: 1.6188  decode.d0.loss_cls: 0.6038  decode.d0.loss_mask: 1.4409  decode.d0.loss_dice: 1.6924  decode.d1.loss_cls: 0.6016  decode.d1.loss_mask: 1.4151  decode.d1.loss_dice: 1.5986  decode.d2.loss_cls: 0.4724  decode.d2.loss_mask: 1.4322  decode.d2.loss_dice: 1.6181  decode.d3.loss_cls: 0.5410  decode.d3.loss_mask: 1.4224  decode.d3.loss_dice: 1.6196  decode.d4.loss_cls: 0.5682  decode.d4.loss_mask: 1.4201  decode.d4.loss_dice: 1.6198  decode.d5.loss_cls: 0.5759  decode.d5.loss_mask: 1.3945  decode.d5.loss_dice: 1.6073  decode.d6.loss_cls: 0.5622  decode.d6.loss_mask: 1.4090  decode.d6.loss_dice: 1.6215  decode.d7.loss_cls: 0.5416  decode.d7.loss_mask: 1.4144  decode.d7.loss_dice: 1.5574  decode.d8.loss_cls: 0.5829  decode.d8.loss_mask: 1.3769  decode.d8.loss_dice: 1.5536  mix_decode.loss_cls: 0.2369  mix_decode.loss_mask: 0.5417  mix_decode.loss_dice: 0.6565  mix_decode.d0.loss_cls: 0.2435  mix_decode.d0.loss_mask: 0.5471  mix_decode.d0.loss_dice: 0.7042  mix_decode.d1.loss_cls: 0.2201  mix_decode.d1.loss_mask: 0.5661  mix_decode.d1.loss_dice: 0.6572  mix_decode.d2.loss_cls: 0.2084  mix_decode.d2.loss_mask: 0.5682  mix_decode.d2.loss_dice: 0.6711  mix_decode.d3.loss_cls: 0.1954  mix_decode.d3.loss_mask: 0.5668  mix_decode.d3.loss_dice: 0.6651  mix_decode.d4.loss_cls: 0.1952  mix_decode.d4.loss_mask: 0.5649  mix_decode.d4.loss_dice: 0.6695  mix_decode.d5.loss_cls: 0.2076  mix_decode.d5.loss_mask: 0.5433  mix_decode.d5.loss_dice: 0.6764  mix_decode.d6.loss_cls: 0.2139  mix_decode.d6.loss_mask: 0.5745  mix_decode.d6.loss_dice: 0.6835  mix_decode.d7.loss_cls: 0.2099  mix_decode.d7.loss_mask: 0.5560  mix_decode.d7.loss_dice: 0.6710  mix_decode.d8.loss_cls: 0.2208  mix_decode.d8.loss_mask: 0.5422  mix_decode.d8.loss_dice: 0.6672
2025/03/29 16:29:12 - mmengine - INFO - Iter(train) [14550/20000]  base_lr: 3.1035e-05 lr: 3.1035e-05  eta: 1:29:38  time: 1.1558  data_time: 0.0227  memory: 11216  loss: 57.3786  decode.loss_cls: 0.2436  decode.loss_mask: 2.1280  decode.loss_dice: 1.7036  decode.d0.loss_cls: 0.3088  decode.d0.loss_mask: 2.1617  decode.d0.loss_dice: 1.7188  decode.d1.loss_cls: 0.2579  decode.d1.loss_mask: 2.1524  decode.d1.loss_dice: 1.6958  decode.d2.loss_cls: 0.2521  decode.d2.loss_mask: 2.1126  decode.d2.loss_dice: 1.6963  decode.d3.loss_cls: 0.2710  decode.d3.loss_mask: 2.0887  decode.d3.loss_dice: 1.6963  decode.d4.loss_cls: 0.2516  decode.d4.loss_mask: 2.1119  decode.d4.loss_dice: 1.7172  decode.d5.loss_cls: 0.2211  decode.d5.loss_mask: 2.1329  decode.d5.loss_dice: 1.7096  decode.d6.loss_cls: 0.2475  decode.d6.loss_mask: 2.1372  decode.d6.loss_dice: 1.7268  decode.d7.loss_cls: 0.2137  decode.d7.loss_mask: 2.1424  decode.d7.loss_dice: 1.6735  decode.d8.loss_cls: 0.2153  decode.d8.loss_mask: 2.1508  decode.d8.loss_dice: 1.6995  mix_decode.loss_cls: 0.2096  mix_decode.loss_mask: 0.6547  mix_decode.loss_dice: 0.7352  mix_decode.d0.loss_cls: 0.2364  mix_decode.d0.loss_mask: 0.6582  mix_decode.d0.loss_dice: 0.8299  mix_decode.d1.loss_cls: 0.2103  mix_decode.d1.loss_mask: 0.6796  mix_decode.d1.loss_dice: 0.7658  mix_decode.d2.loss_cls: 0.2200  mix_decode.d2.loss_mask: 0.6722  mix_decode.d2.loss_dice: 0.7646  mix_decode.d3.loss_cls: 0.2104  mix_decode.d3.loss_mask: 0.6702  mix_decode.d3.loss_dice: 0.7452  mix_decode.d4.loss_cls: 0.2443  mix_decode.d4.loss_mask: 0.6571  mix_decode.d4.loss_dice: 0.7442  mix_decode.d5.loss_cls: 0.2517  mix_decode.d5.loss_mask: 0.6484  mix_decode.d5.loss_dice: 0.7652  mix_decode.d6.loss_cls: 0.2299  mix_decode.d6.loss_mask: 0.6491  mix_decode.d6.loss_dice: 0.7696  mix_decode.d7.loss_cls: 0.2300  mix_decode.d7.loss_mask: 0.6776  mix_decode.d7.loss_dice: 0.7588  mix_decode.d8.loss_cls: 0.2193  mix_decode.d8.loss_mask: 0.6734  mix_decode.d8.loss_dice: 0.7592
2025/03/29 16:30:10 - mmengine - INFO - Iter(train) [14600/20000]  base_lr: 3.0778e-05 lr: 3.0778e-05  eta: 1:28:52  time: 1.1568  data_time: 0.0228  memory: 11213  loss: 52.1143  decode.loss_cls: 0.4110  decode.loss_mask: 1.7074  decode.loss_dice: 1.6102  decode.d0.loss_cls: 0.4940  decode.d0.loss_mask: 1.7312  decode.d0.loss_dice: 1.7011  decode.d1.loss_cls: 0.4495  decode.d1.loss_mask: 1.6796  decode.d1.loss_dice: 1.5783  decode.d2.loss_cls: 0.5381  decode.d2.loss_mask: 1.6811  decode.d2.loss_dice: 1.6202  decode.d3.loss_cls: 0.4208  decode.d3.loss_mask: 1.7578  decode.d3.loss_dice: 1.6123  decode.d4.loss_cls: 0.3963  decode.d4.loss_mask: 1.6962  decode.d4.loss_dice: 1.6290  decode.d5.loss_cls: 0.4611  decode.d5.loss_mask: 1.7292  decode.d5.loss_dice: 1.6255  decode.d6.loss_cls: 0.4498  decode.d6.loss_mask: 1.6983  decode.d6.loss_dice: 1.6543  decode.d7.loss_cls: 0.4145  decode.d7.loss_mask: 1.7001  decode.d7.loss_dice: 1.6278  decode.d8.loss_cls: 0.3837  decode.d8.loss_mask: 1.7774  decode.d8.loss_dice: 1.6236  mix_decode.loss_cls: 0.1836  mix_decode.loss_mask: 0.5369  mix_decode.loss_dice: 0.6944  mix_decode.d0.loss_cls: 0.2510  mix_decode.d0.loss_mask: 0.5320  mix_decode.d0.loss_dice: 0.7642  mix_decode.d1.loss_cls: 0.1604  mix_decode.d1.loss_mask: 0.5355  mix_decode.d1.loss_dice: 0.7075  mix_decode.d2.loss_cls: 0.1750  mix_decode.d2.loss_mask: 0.5515  mix_decode.d2.loss_dice: 0.7002  mix_decode.d3.loss_cls: 0.1804  mix_decode.d3.loss_mask: 0.5462  mix_decode.d3.loss_dice: 0.6819  mix_decode.d4.loss_cls: 0.1617  mix_decode.d4.loss_mask: 0.5503  mix_decode.d4.loss_dice: 0.7100  mix_decode.d5.loss_cls: 0.1634  mix_decode.d5.loss_mask: 0.5404  mix_decode.d5.loss_dice: 0.7149  mix_decode.d6.loss_cls: 0.1679  mix_decode.d6.loss_mask: 0.5332  mix_decode.d6.loss_dice: 0.7102  mix_decode.d7.loss_cls: 0.1879  mix_decode.d7.loss_mask: 0.5273  mix_decode.d7.loss_dice: 0.7006  mix_decode.d8.loss_cls: 0.1567  mix_decode.d8.loss_mask: 0.5326  mix_decode.d8.loss_dice: 0.6971
2025/03/29 16:31:08 - mmengine - INFO - Iter(train) [14650/20000]  base_lr: 3.0522e-05 lr: 3.0522e-05  eta: 1:28:05  time: 1.1703  data_time: 0.0228  memory: 11203  loss: 55.3032  decode.loss_cls: 0.3020  decode.loss_mask: 1.9739  decode.loss_dice: 1.6240  decode.d0.loss_cls: 0.3864  decode.d0.loss_mask: 1.9357  decode.d0.loss_dice: 1.6426  decode.d1.loss_cls: 0.3445  decode.d1.loss_mask: 1.9234  decode.d1.loss_dice: 1.6185  decode.d2.loss_cls: 0.3242  decode.d2.loss_mask: 1.9705  decode.d2.loss_dice: 1.6288  decode.d3.loss_cls: 0.3258  decode.d3.loss_mask: 1.9734  decode.d3.loss_dice: 1.6434  decode.d4.loss_cls: 0.3723  decode.d4.loss_mask: 1.9391  decode.d4.loss_dice: 1.6208  decode.d5.loss_cls: 0.3339  decode.d5.loss_mask: 1.9073  decode.d5.loss_dice: 1.5831  decode.d6.loss_cls: 0.3322  decode.d6.loss_mask: 1.9152  decode.d6.loss_dice: 1.6402  decode.d7.loss_cls: 0.3242  decode.d7.loss_mask: 1.9596  decode.d7.loss_dice: 1.5892  decode.d8.loss_cls: 0.3029  decode.d8.loss_mask: 1.9389  decode.d8.loss_dice: 1.6365  mix_decode.loss_cls: 0.2478  mix_decode.loss_mask: 0.6734  mix_decode.loss_dice: 0.7192  mix_decode.d0.loss_cls: 0.3201  mix_decode.d0.loss_mask: 0.6395  mix_decode.d0.loss_dice: 0.7651  mix_decode.d1.loss_cls: 0.2201  mix_decode.d1.loss_mask: 0.6675  mix_decode.d1.loss_dice: 0.7115  mix_decode.d2.loss_cls: 0.2523  mix_decode.d2.loss_mask: 0.6324  mix_decode.d2.loss_dice: 0.6891  mix_decode.d3.loss_cls: 0.2301  mix_decode.d3.loss_mask: 0.6575  mix_decode.d3.loss_dice: 0.7146  mix_decode.d4.loss_cls: 0.2509  mix_decode.d4.loss_mask: 0.6537  mix_decode.d4.loss_dice: 0.6972  mix_decode.d5.loss_cls: 0.2782  mix_decode.d5.loss_mask: 0.6337  mix_decode.d5.loss_dice: 0.7360  mix_decode.d6.loss_cls: 0.2858  mix_decode.d6.loss_mask: 0.6303  mix_decode.d6.loss_dice: 0.6968  mix_decode.d7.loss_cls: 0.2472  mix_decode.d7.loss_mask: 0.6714  mix_decode.d7.loss_dice: 0.7247  mix_decode.d8.loss_cls: 0.2606  mix_decode.d8.loss_mask: 0.6607  mix_decode.d8.loss_dice: 0.7229
2025/03/29 16:32:05 - mmengine - INFO - Iter(train) [14700/20000]  base_lr: 3.0265e-05 lr: 3.0265e-05  eta: 1:27:19  time: 1.1617  data_time: 0.0240  memory: 11216  loss: 53.3018  decode.loss_cls: 0.2973  decode.loss_mask: 1.8471  decode.loss_dice: 1.7191  decode.d0.loss_cls: 0.3606  decode.d0.loss_mask: 1.8573  decode.d0.loss_dice: 1.7587  decode.d1.loss_cls: 0.3449  decode.d1.loss_mask: 1.7991  decode.d1.loss_dice: 1.7209  decode.d2.loss_cls: 0.3043  decode.d2.loss_mask: 1.8581  decode.d2.loss_dice: 1.7540  decode.d3.loss_cls: 0.2999  decode.d3.loss_mask: 1.8587  decode.d3.loss_dice: 1.7177  decode.d4.loss_cls: 0.3323  decode.d4.loss_mask: 1.8405  decode.d4.loss_dice: 1.7223  decode.d5.loss_cls: 0.3100  decode.d5.loss_mask: 1.8320  decode.d5.loss_dice: 1.7246  decode.d6.loss_cls: 0.3073  decode.d6.loss_mask: 1.8215  decode.d6.loss_dice: 1.7021  decode.d7.loss_cls: 0.3306  decode.d7.loss_mask: 1.8270  decode.d7.loss_dice: 1.7077  decode.d8.loss_cls: 0.3075  decode.d8.loss_mask: 1.8466  decode.d8.loss_dice: 1.6979  mix_decode.loss_cls: 0.1616  mix_decode.loss_mask: 0.5688  mix_decode.loss_dice: 0.6804  mix_decode.d0.loss_cls: 0.2579  mix_decode.d0.loss_mask: 0.5742  mix_decode.d0.loss_dice: 0.7522  mix_decode.d1.loss_cls: 0.1831  mix_decode.d1.loss_mask: 0.5800  mix_decode.d1.loss_dice: 0.7047  mix_decode.d2.loss_cls: 0.1725  mix_decode.d2.loss_mask: 0.5753  mix_decode.d2.loss_dice: 0.6724  mix_decode.d3.loss_cls: 0.1713  mix_decode.d3.loss_mask: 0.5674  mix_decode.d3.loss_dice: 0.6861  mix_decode.d4.loss_cls: 0.1728  mix_decode.d4.loss_mask: 0.5759  mix_decode.d4.loss_dice: 0.6874  mix_decode.d5.loss_cls: 0.1591  mix_decode.d5.loss_mask: 0.5603  mix_decode.d5.loss_dice: 0.7088  mix_decode.d6.loss_cls: 0.1741  mix_decode.d6.loss_mask: 0.5631  mix_decode.d6.loss_dice: 0.6800  mix_decode.d7.loss_cls: 0.1885  mix_decode.d7.loss_mask: 0.5843  mix_decode.d7.loss_dice: 0.7005  mix_decode.d8.loss_cls: 0.1693  mix_decode.d8.loss_mask: 0.5708  mix_decode.d8.loss_dice: 0.6913
2025/03/29 16:33:03 - mmengine - INFO - Iter(train) [14750/20000]  base_lr: 3.0008e-05 lr: 3.0008e-05  eta: 1:26:33  time: 1.1564  data_time: 0.0236  memory: 11231  loss: 60.5102  decode.loss_cls: 0.5507  decode.loss_mask: 1.7259  decode.loss_dice: 1.8088  decode.d0.loss_cls: 0.5194  decode.d0.loss_mask: 1.7317  decode.d0.loss_dice: 1.8770  decode.d1.loss_cls: 0.5052  decode.d1.loss_mask: 1.7290  decode.d1.loss_dice: 1.8295  decode.d2.loss_cls: 0.5142  decode.d2.loss_mask: 1.6988  decode.d2.loss_dice: 1.8192  decode.d3.loss_cls: 0.4937  decode.d3.loss_mask: 1.7168  decode.d3.loss_dice: 1.7857  decode.d4.loss_cls: 0.5412  decode.d4.loss_mask: 1.7138  decode.d4.loss_dice: 1.7765  decode.d5.loss_cls: 0.5477  decode.d5.loss_mask: 1.7280  decode.d5.loss_dice: 1.8177  decode.d6.loss_cls: 0.5622  decode.d6.loss_mask: 1.7210  decode.d6.loss_dice: 1.8028  decode.d7.loss_cls: 0.4734  decode.d7.loss_mask: 1.7330  decode.d7.loss_dice: 1.8076  decode.d8.loss_cls: 0.4894  decode.d8.loss_mask: 1.6882  decode.d8.loss_dice: 1.8411  mix_decode.loss_cls: 0.3204  mix_decode.loss_mask: 0.7435  mix_decode.loss_dice: 0.9139  mix_decode.d0.loss_cls: 0.3256  mix_decode.d0.loss_mask: 0.7205  mix_decode.d0.loss_dice: 0.9812  mix_decode.d1.loss_cls: 0.3223  mix_decode.d1.loss_mask: 0.7221  mix_decode.d1.loss_dice: 0.9282  mix_decode.d2.loss_cls: 0.3395  mix_decode.d2.loss_mask: 0.7350  mix_decode.d2.loss_dice: 0.9242  mix_decode.d3.loss_cls: 0.3660  mix_decode.d3.loss_mask: 0.7282  mix_decode.d3.loss_dice: 0.8952  mix_decode.d4.loss_cls: 0.3051  mix_decode.d4.loss_mask: 0.7355  mix_decode.d4.loss_dice: 0.9043  mix_decode.d5.loss_cls: 0.3090  mix_decode.d5.loss_mask: 0.7736  mix_decode.d5.loss_dice: 0.9463  mix_decode.d6.loss_cls: 0.3632  mix_decode.d6.loss_mask: 0.7420  mix_decode.d6.loss_dice: 0.9368  mix_decode.d7.loss_cls: 0.3355  mix_decode.d7.loss_mask: 0.7396  mix_decode.d7.loss_dice: 0.9248  mix_decode.d8.loss_cls: 0.3214  mix_decode.d8.loss_mask: 0.7321  mix_decode.d8.loss_dice: 0.9259
2025/03/29 16:34:01 - mmengine - INFO - Iter(train) [14800/20000]  base_lr: 2.9751e-05 lr: 2.9751e-05  eta: 1:25:46  time: 1.1496  data_time: 0.0225  memory: 11212  loss: 47.8865  decode.loss_cls: 0.2625  decode.loss_mask: 1.7417  decode.loss_dice: 1.4836  decode.d0.loss_cls: 0.3761  decode.d0.loss_mask: 1.8057  decode.d0.loss_dice: 1.5748  decode.d1.loss_cls: 0.2331  decode.d1.loss_mask: 1.7871  decode.d1.loss_dice: 1.5473  decode.d2.loss_cls: 0.2294  decode.d2.loss_mask: 1.7885  decode.d2.loss_dice: 1.5240  decode.d3.loss_cls: 0.2163  decode.d3.loss_mask: 1.7493  decode.d3.loss_dice: 1.5189  decode.d4.loss_cls: 0.2193  decode.d4.loss_mask: 1.7889  decode.d4.loss_dice: 1.5371  decode.d5.loss_cls: 0.2016  decode.d5.loss_mask: 1.7286  decode.d5.loss_dice: 1.5156  decode.d6.loss_cls: 0.2016  decode.d6.loss_mask: 1.7517  decode.d6.loss_dice: 1.4998  decode.d7.loss_cls: 0.1899  decode.d7.loss_mask: 1.7670  decode.d7.loss_dice: 1.5088  decode.d8.loss_cls: 0.2361  decode.d8.loss_mask: 1.7490  decode.d8.loss_dice: 1.4889  mix_decode.loss_cls: 0.2027  mix_decode.loss_mask: 0.4603  mix_decode.loss_dice: 0.6156  mix_decode.d0.loss_cls: 0.2606  mix_decode.d0.loss_mask: 0.4344  mix_decode.d0.loss_dice: 0.6494  mix_decode.d1.loss_cls: 0.2070  mix_decode.d1.loss_mask: 0.4461  mix_decode.d1.loss_dice: 0.6010  mix_decode.d2.loss_cls: 0.1853  mix_decode.d2.loss_mask: 0.4458  mix_decode.d2.loss_dice: 0.6030  mix_decode.d3.loss_cls: 0.1665  mix_decode.d3.loss_mask: 0.4624  mix_decode.d3.loss_dice: 0.6209  mix_decode.d4.loss_cls: 0.1721  mix_decode.d4.loss_mask: 0.4396  mix_decode.d4.loss_dice: 0.6200  mix_decode.d5.loss_cls: 0.2051  mix_decode.d5.loss_mask: 0.4438  mix_decode.d5.loss_dice: 0.6237  mix_decode.d6.loss_cls: 0.1976  mix_decode.d6.loss_mask: 0.4380  mix_decode.d6.loss_dice: 0.6117  mix_decode.d7.loss_cls: 0.2098  mix_decode.d7.loss_mask: 0.4407  mix_decode.d7.loss_dice: 0.6126  mix_decode.d8.loss_cls: 0.2116  mix_decode.d8.loss_mask: 0.4737  mix_decode.d8.loss_dice: 0.6033
2025/03/29 16:34:59 - mmengine - INFO - Iter(train) [14850/20000]  base_lr: 2.9493e-05 lr: 2.9493e-05  eta: 1:24:59  time: 1.1587  data_time: 0.0235  memory: 11214  loss: 52.4643  decode.loss_cls: 0.2916  decode.loss_mask: 1.8353  decode.loss_dice: 1.7426  decode.d0.loss_cls: 0.4343  decode.d0.loss_mask: 1.7994  decode.d0.loss_dice: 1.7676  decode.d1.loss_cls: 0.4210  decode.d1.loss_mask: 1.7713  decode.d1.loss_dice: 1.7033  decode.d2.loss_cls: 0.3603  decode.d2.loss_mask: 1.7941  decode.d2.loss_dice: 1.6842  decode.d3.loss_cls: 0.3367  decode.d3.loss_mask: 1.8101  decode.d3.loss_dice: 1.7375  decode.d4.loss_cls: 0.3507  decode.d4.loss_mask: 1.8071  decode.d4.loss_dice: 1.7333  decode.d5.loss_cls: 0.3468  decode.d5.loss_mask: 1.7864  decode.d5.loss_dice: 1.7485  decode.d6.loss_cls: 0.3835  decode.d6.loss_mask: 1.7914  decode.d6.loss_dice: 1.7476  decode.d7.loss_cls: 0.2712  decode.d7.loss_mask: 1.8536  decode.d7.loss_dice: 1.7565  decode.d8.loss_cls: 0.3279  decode.d8.loss_mask: 1.8358  decode.d8.loss_dice: 1.7718  mix_decode.loss_cls: 0.1560  mix_decode.loss_mask: 0.5138  mix_decode.loss_dice: 0.6663  mix_decode.d0.loss_cls: 0.2566  mix_decode.d0.loss_mask: 0.4790  mix_decode.d0.loss_dice: 0.7285  mix_decode.d1.loss_cls: 0.1326  mix_decode.d1.loss_mask: 0.5268  mix_decode.d1.loss_dice: 0.6786  mix_decode.d2.loss_cls: 0.1546  mix_decode.d2.loss_mask: 0.5042  mix_decode.d2.loss_dice: 0.6508  mix_decode.d3.loss_cls: 0.1319  mix_decode.d3.loss_mask: 0.5213  mix_decode.d3.loss_dice: 0.6545  mix_decode.d4.loss_cls: 0.1157  mix_decode.d4.loss_mask: 0.5518  mix_decode.d4.loss_dice: 0.6776  mix_decode.d5.loss_cls: 0.1370  mix_decode.d5.loss_mask: 0.5019  mix_decode.d5.loss_dice: 0.6658  mix_decode.d6.loss_cls: 0.1572  mix_decode.d6.loss_mask: 0.5083  mix_decode.d6.loss_dice: 0.6541  mix_decode.d7.loss_cls: 0.1590  mix_decode.d7.loss_mask: 0.5181  mix_decode.d7.loss_dice: 0.6630  mix_decode.d8.loss_cls: 0.1796  mix_decode.d8.loss_mask: 0.5484  mix_decode.d8.loss_dice: 0.6701
2025/03/29 16:35:56 - mmengine - INFO - Iter(train) [14900/20000]  base_lr: 2.9235e-05 lr: 2.9235e-05  eta: 1:24:13  time: 1.1571  data_time: 0.0231  memory: 11219  loss: 54.4331  decode.loss_cls: 0.4427  decode.loss_mask: 1.6828  decode.loss_dice: 1.6005  decode.d0.loss_cls: 0.5166  decode.d0.loss_mask: 1.7326  decode.d0.loss_dice: 1.6713  decode.d1.loss_cls: 0.4073  decode.d1.loss_mask: 1.7662  decode.d1.loss_dice: 1.6503  decode.d2.loss_cls: 0.4017  decode.d2.loss_mask: 1.7371  decode.d2.loss_dice: 1.6188  decode.d3.loss_cls: 0.3726  decode.d3.loss_mask: 1.7421  decode.d3.loss_dice: 1.6231  decode.d4.loss_cls: 0.3998  decode.d4.loss_mask: 1.7145  decode.d4.loss_dice: 1.5910  decode.d5.loss_cls: 0.3930  decode.d5.loss_mask: 1.7481  decode.d5.loss_dice: 1.6587  decode.d6.loss_cls: 0.4061  decode.d6.loss_mask: 1.7262  decode.d6.loss_dice: 1.6321  decode.d7.loss_cls: 0.3536  decode.d7.loss_mask: 1.7782  decode.d7.loss_dice: 1.6610  decode.d8.loss_cls: 0.3642  decode.d8.loss_mask: 1.7508  decode.d8.loss_dice: 1.6487  mix_decode.loss_cls: 0.2529  mix_decode.loss_mask: 0.5993  mix_decode.loss_dice: 0.7787  mix_decode.d0.loss_cls: 0.3047  mix_decode.d0.loss_mask: 0.6093  mix_decode.d0.loss_dice: 0.8342  mix_decode.d1.loss_cls: 0.2401  mix_decode.d1.loss_mask: 0.6216  mix_decode.d1.loss_dice: 0.7925  mix_decode.d2.loss_cls: 0.2691  mix_decode.d2.loss_mask: 0.6211  mix_decode.d2.loss_dice: 0.7586  mix_decode.d3.loss_cls: 0.2704  mix_decode.d3.loss_mask: 0.6185  mix_decode.d3.loss_dice: 0.7631  mix_decode.d4.loss_cls: 0.2620  mix_decode.d4.loss_mask: 0.6079  mix_decode.d4.loss_dice: 0.7794  mix_decode.d5.loss_cls: 0.2956  mix_decode.d5.loss_mask: 0.6145  mix_decode.d5.loss_dice: 0.7835  mix_decode.d6.loss_cls: 0.2478  mix_decode.d6.loss_mask: 0.6115  mix_decode.d6.loss_dice: 0.8025  mix_decode.d7.loss_cls: 0.2856  mix_decode.d7.loss_mask: 0.6037  mix_decode.d7.loss_dice: 0.7497  mix_decode.d8.loss_cls: 0.2710  mix_decode.d8.loss_mask: 0.6174  mix_decode.d8.loss_dice: 0.7755
2025/03/29 16:36:54 - mmengine - INFO - Iter(train) [14950/20000]  base_lr: 2.8977e-05 lr: 2.8977e-05  eta: 1:23:26  time: 1.1557  data_time: 0.0227  memory: 11203  loss: 52.0126  decode.loss_cls: 0.3698  decode.loss_mask: 1.6543  decode.loss_dice: 1.6532  decode.d0.loss_cls: 0.5952  decode.d0.loss_mask: 1.6453  decode.d0.loss_dice: 1.7548  decode.d1.loss_cls: 0.4064  decode.d1.loss_mask: 1.6937  decode.d1.loss_dice: 1.6922  decode.d2.loss_cls: 0.4559  decode.d2.loss_mask: 1.6396  decode.d2.loss_dice: 1.6063  decode.d3.loss_cls: 0.3926  decode.d3.loss_mask: 1.6344  decode.d3.loss_dice: 1.6090  decode.d4.loss_cls: 0.4176  decode.d4.loss_mask: 1.6549  decode.d4.loss_dice: 1.6383  decode.d5.loss_cls: 0.4687  decode.d5.loss_mask: 1.6229  decode.d5.loss_dice: 1.6785  decode.d6.loss_cls: 0.3818  decode.d6.loss_mask: 1.6801  decode.d6.loss_dice: 1.6983  decode.d7.loss_cls: 0.4237  decode.d7.loss_mask: 1.6905  decode.d7.loss_dice: 1.6817  decode.d8.loss_cls: 0.4095  decode.d8.loss_mask: 1.6474  decode.d8.loss_dice: 1.6488  mix_decode.loss_cls: 0.3195  mix_decode.loss_mask: 0.5211  mix_decode.loss_dice: 0.6267  mix_decode.d0.loss_cls: 0.2978  mix_decode.d0.loss_mask: 0.5179  mix_decode.d0.loss_dice: 0.6747  mix_decode.d1.loss_cls: 0.2629  mix_decode.d1.loss_mask: 0.5330  mix_decode.d1.loss_dice: 0.6469  mix_decode.d2.loss_cls: 0.2522  mix_decode.d2.loss_mask: 0.5184  mix_decode.d2.loss_dice: 0.6185  mix_decode.d3.loss_cls: 0.2537  mix_decode.d3.loss_mask: 0.5182  mix_decode.d3.loss_dice: 0.6184  mix_decode.d4.loss_cls: 0.2472  mix_decode.d4.loss_mask: 0.5388  mix_decode.d4.loss_dice: 0.6494  mix_decode.d5.loss_cls: 0.3020  mix_decode.d5.loss_mask: 0.5067  mix_decode.d5.loss_dice: 0.6179  mix_decode.d6.loss_cls: 0.3248  mix_decode.d6.loss_mask: 0.5574  mix_decode.d6.loss_dice: 0.6418  mix_decode.d7.loss_cls: 0.3496  mix_decode.d7.loss_mask: 0.4923  mix_decode.d7.loss_dice: 0.6364  mix_decode.d8.loss_cls: 0.2858  mix_decode.d8.loss_mask: 0.5009  mix_decode.d8.loss_dice: 0.6360
2025/03/29 16:37:52 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 16:37:52 - mmengine - INFO - Iter(train) [15000/20000]  base_lr: 2.8719e-05 lr: 2.8719e-05  eta: 1:22:39  time: 1.1540  data_time: 0.0228  memory: 11220  loss: 52.8768  decode.loss_cls: 0.1554  decode.loss_mask: 1.8932  decode.loss_dice: 1.6619  decode.d0.loss_cls: 0.3277  decode.d0.loss_mask: 1.8732  decode.d0.loss_dice: 1.6005  decode.d1.loss_cls: 0.1490  decode.d1.loss_mask: 1.8808  decode.d1.loss_dice: 1.6491  decode.d2.loss_cls: 0.1439  decode.d2.loss_mask: 1.8790  decode.d2.loss_dice: 1.6633  decode.d3.loss_cls: 0.1424  decode.d3.loss_mask: 1.8770  decode.d3.loss_dice: 1.6356  decode.d4.loss_cls: 0.1687  decode.d4.loss_mask: 1.8947  decode.d4.loss_dice: 1.6247  decode.d5.loss_cls: 0.1760  decode.d5.loss_mask: 1.8848  decode.d5.loss_dice: 1.5762  decode.d6.loss_cls: 0.1530  decode.d6.loss_mask: 1.8603  decode.d6.loss_dice: 1.6589  decode.d7.loss_cls: 0.1823  decode.d7.loss_mask: 1.8876  decode.d7.loss_dice: 1.6280  decode.d8.loss_cls: 0.1628  decode.d8.loss_mask: 1.8876  decode.d8.loss_dice: 1.6215  mix_decode.loss_cls: 0.1962  mix_decode.loss_mask: 0.6252  mix_decode.loss_dice: 0.7619  mix_decode.d0.loss_cls: 0.2820  mix_decode.d0.loss_mask: 0.5852  mix_decode.d0.loss_dice: 0.7987  mix_decode.d1.loss_cls: 0.2346  mix_decode.d1.loss_mask: 0.6016  mix_decode.d1.loss_dice: 0.7401  mix_decode.d2.loss_cls: 0.2225  mix_decode.d2.loss_mask: 0.6085  mix_decode.d2.loss_dice: 0.7470  mix_decode.d3.loss_cls: 0.2341  mix_decode.d3.loss_mask: 0.5999  mix_decode.d3.loss_dice: 0.7568  mix_decode.d4.loss_cls: 0.2299  mix_decode.d4.loss_mask: 0.5768  mix_decode.d4.loss_dice: 0.7288  mix_decode.d5.loss_cls: 0.2413  mix_decode.d5.loss_mask: 0.6154  mix_decode.d5.loss_dice: 0.7737  mix_decode.d6.loss_cls: 0.2490  mix_decode.d6.loss_mask: 0.5789  mix_decode.d6.loss_dice: 0.7488  mix_decode.d7.loss_cls: 0.2715  mix_decode.d7.loss_mask: 0.6073  mix_decode.d7.loss_dice: 0.7590  mix_decode.d8.loss_cls: 0.2283  mix_decode.d8.loss_mask: 0.6215  mix_decode.d8.loss_dice: 0.7537
2025/03/29 16:38:50 - mmengine - INFO - Iter(train) [15050/20000]  base_lr: 2.8460e-05 lr: 2.8460e-05  eta: 1:21:52  time: 1.1588  data_time: 0.0230  memory: 11216  loss: 52.4454  decode.loss_cls: 0.2916  decode.loss_mask: 1.7527  decode.loss_dice: 1.6257  decode.d0.loss_cls: 0.4113  decode.d0.loss_mask: 1.7086  decode.d0.loss_dice: 1.5661  decode.d1.loss_cls: 0.2685  decode.d1.loss_mask: 1.7527  decode.d1.loss_dice: 1.6324  decode.d2.loss_cls: 0.2781  decode.d2.loss_mask: 1.7324  decode.d2.loss_dice: 1.5944  decode.d3.loss_cls: 0.2634  decode.d3.loss_mask: 1.7592  decode.d3.loss_dice: 1.6108  decode.d4.loss_cls: 0.2534  decode.d4.loss_mask: 1.7094  decode.d4.loss_dice: 1.5587  decode.d5.loss_cls: 0.2914  decode.d5.loss_mask: 1.7452  decode.d5.loss_dice: 1.6469  decode.d6.loss_cls: 0.2748  decode.d6.loss_mask: 1.7425  decode.d6.loss_dice: 1.6425  decode.d7.loss_cls: 0.3387  decode.d7.loss_mask: 1.7030  decode.d7.loss_dice: 1.6091  decode.d8.loss_cls: 0.3102  decode.d8.loss_mask: 1.7222  decode.d8.loss_dice: 1.6294  mix_decode.loss_cls: 0.1708  mix_decode.loss_mask: 0.6823  mix_decode.loss_dice: 0.7036  mix_decode.d0.loss_cls: 0.2198  mix_decode.d0.loss_mask: 0.6792  mix_decode.d0.loss_dice: 0.7643  mix_decode.d1.loss_cls: 0.1684  mix_decode.d1.loss_mask: 0.7321  mix_decode.d1.loss_dice: 0.7164  mix_decode.d2.loss_cls: 0.1585  mix_decode.d2.loss_mask: 0.6859  mix_decode.d2.loss_dice: 0.7145  mix_decode.d3.loss_cls: 0.1423  mix_decode.d3.loss_mask: 0.7236  mix_decode.d3.loss_dice: 0.7240  mix_decode.d4.loss_cls: 0.1607  mix_decode.d4.loss_mask: 0.7043  mix_decode.d4.loss_dice: 0.7096  mix_decode.d5.loss_cls: 0.1724  mix_decode.d5.loss_mask: 0.7339  mix_decode.d5.loss_dice: 0.7271  mix_decode.d6.loss_cls: 0.1996  mix_decode.d6.loss_mask: 0.7070  mix_decode.d6.loss_dice: 0.7220  mix_decode.d7.loss_cls: 0.1844  mix_decode.d7.loss_mask: 0.6986  mix_decode.d7.loss_dice: 0.7304  mix_decode.d8.loss_cls: 0.2077  mix_decode.d8.loss_mask: 0.6808  mix_decode.d8.loss_dice: 0.6958
2025/03/29 16:39:48 - mmengine - INFO - Iter(train) [15100/20000]  base_lr: 2.8201e-05 lr: 2.8201e-05  eta: 1:21:05  time: 1.1551  data_time: 0.0231  memory: 11226  loss: 52.6488  decode.loss_cls: 0.4305  decode.loss_mask: 1.8729  decode.loss_dice: 1.6401  decode.d0.loss_cls: 0.6411  decode.d0.loss_mask: 1.8759  decode.d0.loss_dice: 1.6481  decode.d1.loss_cls: 0.4274  decode.d1.loss_mask: 1.8879  decode.d1.loss_dice: 1.6476  decode.d2.loss_cls: 0.4550  decode.d2.loss_mask: 1.8774  decode.d2.loss_dice: 1.6585  decode.d3.loss_cls: 0.4601  decode.d3.loss_mask: 1.8755  decode.d3.loss_dice: 1.6504  decode.d4.loss_cls: 0.4406  decode.d4.loss_mask: 1.8399  decode.d4.loss_dice: 1.6437  decode.d5.loss_cls: 0.4269  decode.d5.loss_mask: 1.8628  decode.d5.loss_dice: 1.6201  decode.d6.loss_cls: 0.4750  decode.d6.loss_mask: 1.8825  decode.d6.loss_dice: 1.6248  decode.d7.loss_cls: 0.4256  decode.d7.loss_mask: 1.8822  decode.d7.loss_dice: 1.6442  decode.d8.loss_cls: 0.4504  decode.d8.loss_mask: 1.8852  decode.d8.loss_dice: 1.6471  mix_decode.loss_cls: 0.2010  mix_decode.loss_mask: 0.5299  mix_decode.loss_dice: 0.5515  mix_decode.d0.loss_cls: 0.2311  mix_decode.d0.loss_mask: 0.5317  mix_decode.d0.loss_dice: 0.5870  mix_decode.d1.loss_cls: 0.2049  mix_decode.d1.loss_mask: 0.5094  mix_decode.d1.loss_dice: 0.5390  mix_decode.d2.loss_cls: 0.1828  mix_decode.d2.loss_mask: 0.5126  mix_decode.d2.loss_dice: 0.5276  mix_decode.d3.loss_cls: 0.1691  mix_decode.d3.loss_mask: 0.5203  mix_decode.d3.loss_dice: 0.5314  mix_decode.d4.loss_cls: 0.2209  mix_decode.d4.loss_mask: 0.5043  mix_decode.d4.loss_dice: 0.5236  mix_decode.d5.loss_cls: 0.2202  mix_decode.d5.loss_mask: 0.5191  mix_decode.d5.loss_dice: 0.5561  mix_decode.d6.loss_cls: 0.2151  mix_decode.d6.loss_mask: 0.5286  mix_decode.d6.loss_dice: 0.5751  mix_decode.d7.loss_cls: 0.2077  mix_decode.d7.loss_mask: 0.5370  mix_decode.d7.loss_dice: 0.5685  mix_decode.d8.loss_cls: 0.1886  mix_decode.d8.loss_mask: 0.5706  mix_decode.d8.loss_dice: 0.5850
2025/03/29 16:40:46 - mmengine - INFO - Iter(train) [15150/20000]  base_lr: 2.7942e-05 lr: 2.7942e-05  eta: 1:20:18  time: 1.1563  data_time: 0.0232  memory: 11220  loss: 57.5130  decode.loss_cls: 0.3128  decode.loss_mask: 2.0032  decode.loss_dice: 1.7720  decode.d0.loss_cls: 0.5907  decode.d0.loss_mask: 1.9966  decode.d0.loss_dice: 1.8171  decode.d1.loss_cls: 0.4190  decode.d1.loss_mask: 2.0172  decode.d1.loss_dice: 1.7865  decode.d2.loss_cls: 0.3269  decode.d2.loss_mask: 1.9886  decode.d2.loss_dice: 1.7717  decode.d3.loss_cls: 0.3334  decode.d3.loss_mask: 2.0011  decode.d3.loss_dice: 1.7891  decode.d4.loss_cls: 0.3286  decode.d4.loss_mask: 1.9911  decode.d4.loss_dice: 1.8102  decode.d5.loss_cls: 0.3017  decode.d5.loss_mask: 2.0145  decode.d5.loss_dice: 1.8227  decode.d6.loss_cls: 0.3673  decode.d6.loss_mask: 1.9787  decode.d6.loss_dice: 1.7915  decode.d7.loss_cls: 0.3027  decode.d7.loss_mask: 2.0372  decode.d7.loss_dice: 1.8011  decode.d8.loss_cls: 0.3467  decode.d8.loss_mask: 1.9760  decode.d8.loss_dice: 1.8103  mix_decode.loss_cls: 0.2011  mix_decode.loss_mask: 0.6304  mix_decode.loss_dice: 0.6939  mix_decode.d0.loss_cls: 0.2945  mix_decode.d0.loss_mask: 0.6432  mix_decode.d0.loss_dice: 0.7724  mix_decode.d1.loss_cls: 0.2659  mix_decode.d1.loss_mask: 0.6356  mix_decode.d1.loss_dice: 0.6872  mix_decode.d2.loss_cls: 0.2667  mix_decode.d2.loss_mask: 0.6313  mix_decode.d2.loss_dice: 0.6885  mix_decode.d3.loss_cls: 0.2001  mix_decode.d3.loss_mask: 0.6411  mix_decode.d3.loss_dice: 0.7292  mix_decode.d4.loss_cls: 0.1665  mix_decode.d4.loss_mask: 0.6411  mix_decode.d4.loss_dice: 0.7402  mix_decode.d5.loss_cls: 0.2294  mix_decode.d5.loss_mask: 0.6507  mix_decode.d5.loss_dice: 0.7378  mix_decode.d6.loss_cls: 0.2611  mix_decode.d6.loss_mask: 0.6338  mix_decode.d6.loss_dice: 0.7289  mix_decode.d7.loss_cls: 0.2207  mix_decode.d7.loss_mask: 0.6389  mix_decode.d7.loss_dice: 0.7182  mix_decode.d8.loss_cls: 0.2179  mix_decode.d8.loss_mask: 0.6365  mix_decode.d8.loss_dice: 0.7041
2025/03/29 16:41:43 - mmengine - INFO - Iter(train) [15200/20000]  base_lr: 2.7683e-05 lr: 2.7683e-05  eta: 1:19:31  time: 1.1579  data_time: 0.0229  memory: 11219  loss: 56.8585  decode.loss_cls: 0.2650  decode.loss_mask: 1.8970  decode.loss_dice: 1.7447  decode.d0.loss_cls: 0.4077  decode.d0.loss_mask: 1.9495  decode.d0.loss_dice: 1.7508  decode.d1.loss_cls: 0.3601  decode.d1.loss_mask: 1.9401  decode.d1.loss_dice: 1.6967  decode.d2.loss_cls: 0.4557  decode.d2.loss_mask: 1.8548  decode.d2.loss_dice: 1.6529  decode.d3.loss_cls: 0.2661  decode.d3.loss_mask: 1.9323  decode.d3.loss_dice: 1.7481  decode.d4.loss_cls: 0.3438  decode.d4.loss_mask: 1.8966  decode.d4.loss_dice: 1.7167  decode.d5.loss_cls: 0.2998  decode.d5.loss_mask: 1.9096  decode.d5.loss_dice: 1.7501  decode.d6.loss_cls: 0.2880  decode.d6.loss_mask: 1.8979  decode.d6.loss_dice: 1.7746  decode.d7.loss_cls: 0.2818  decode.d7.loss_mask: 1.9262  decode.d7.loss_dice: 1.7408  decode.d8.loss_cls: 0.2869  decode.d8.loss_mask: 1.9477  decode.d8.loss_dice: 1.7296  mix_decode.loss_cls: 0.1661  mix_decode.loss_mask: 0.6765  mix_decode.loss_dice: 0.8295  mix_decode.d0.loss_cls: 0.2464  mix_decode.d0.loss_mask: 0.6570  mix_decode.d0.loss_dice: 0.8976  mix_decode.d1.loss_cls: 0.1676  mix_decode.d1.loss_mask: 0.7005  mix_decode.d1.loss_dice: 0.8493  mix_decode.d2.loss_cls: 0.1921  mix_decode.d2.loss_mask: 0.7026  mix_decode.d2.loss_dice: 0.8248  mix_decode.d3.loss_cls: 0.1944  mix_decode.d3.loss_mask: 0.6841  mix_decode.d3.loss_dice: 0.8112  mix_decode.d4.loss_cls: 0.1604  mix_decode.d4.loss_mask: 0.7067  mix_decode.d4.loss_dice: 0.8572  mix_decode.d5.loss_cls: 0.1847  mix_decode.d5.loss_mask: 0.6946  mix_decode.d5.loss_dice: 0.8349  mix_decode.d6.loss_cls: 0.1965  mix_decode.d6.loss_mask: 0.6785  mix_decode.d6.loss_dice: 0.8245  mix_decode.d7.loss_cls: 0.1925  mix_decode.d7.loss_mask: 0.6775  mix_decode.d7.loss_dice: 0.8312  mix_decode.d8.loss_cls: 0.1759  mix_decode.d8.loss_mask: 0.6878  mix_decode.d8.loss_dice: 0.8444
2025/03/29 16:42:42 - mmengine - INFO - Iter(train) [15250/20000]  base_lr: 2.7423e-05 lr: 2.7423e-05  eta: 1:18:44  time: 1.1711  data_time: 0.0250  memory: 11221  loss: 56.5715  decode.loss_cls: 0.4233  decode.loss_mask: 1.9582  decode.loss_dice: 1.6735  decode.d0.loss_cls: 0.4912  decode.d0.loss_mask: 1.9701  decode.d0.loss_dice: 1.8657  decode.d1.loss_cls: 0.5269  decode.d1.loss_mask: 1.9291  decode.d1.loss_dice: 1.6524  decode.d2.loss_cls: 0.4662  decode.d2.loss_mask: 1.9281  decode.d2.loss_dice: 1.6137  decode.d3.loss_cls: 0.4611  decode.d3.loss_mask: 1.9266  decode.d3.loss_dice: 1.6039  decode.d4.loss_cls: 0.4368  decode.d4.loss_mask: 1.9294  decode.d4.loss_dice: 1.6663  decode.d5.loss_cls: 0.4200  decode.d5.loss_mask: 1.9374  decode.d5.loss_dice: 1.6840  decode.d6.loss_cls: 0.4840  decode.d6.loss_mask: 1.9312  decode.d6.loss_dice: 1.6670  decode.d7.loss_cls: 0.4377  decode.d7.loss_mask: 1.9618  decode.d7.loss_dice: 1.6920  decode.d8.loss_cls: 0.4419  decode.d8.loss_mask: 1.9870  decode.d8.loss_dice: 1.6413  mix_decode.loss_cls: 0.3247  mix_decode.loss_mask: 0.5067  mix_decode.loss_dice: 0.6994  mix_decode.d0.loss_cls: 0.3464  mix_decode.d0.loss_mask: 0.5144  mix_decode.d0.loss_dice: 0.8177  mix_decode.d1.loss_cls: 0.3197  mix_decode.d1.loss_mask: 0.5251  mix_decode.d1.loss_dice: 0.6859  mix_decode.d2.loss_cls: 0.3548  mix_decode.d2.loss_mask: 0.5211  mix_decode.d2.loss_dice: 0.6633  mix_decode.d3.loss_cls: 0.3607  mix_decode.d3.loss_mask: 0.5219  mix_decode.d3.loss_dice: 0.7085  mix_decode.d4.loss_cls: 0.3061  mix_decode.d4.loss_mask: 0.5511  mix_decode.d4.loss_dice: 0.7270  mix_decode.d5.loss_cls: 0.3598  mix_decode.d5.loss_mask: 0.5160  mix_decode.d5.loss_dice: 0.6969  mix_decode.d6.loss_cls: 0.3170  mix_decode.d6.loss_mask: 0.5259  mix_decode.d6.loss_dice: 0.7432  mix_decode.d7.loss_cls: 0.3293  mix_decode.d7.loss_mask: 0.5311  mix_decode.d7.loss_dice: 0.7291  mix_decode.d8.loss_cls: 0.3638  mix_decode.d8.loss_mask: 0.5151  mix_decode.d8.loss_dice: 0.6816
2025/03/29 16:43:40 - mmengine - INFO - Iter(train) [15300/20000]  base_lr: 2.7163e-05 lr: 2.7163e-05  eta: 1:17:57  time: 1.1699  data_time: 0.0245  memory: 11209  loss: 53.0033  decode.loss_cls: 0.3709  decode.loss_mask: 1.6448  decode.loss_dice: 1.7224  decode.d0.loss_cls: 0.4174  decode.d0.loss_mask: 1.6389  decode.d0.loss_dice: 1.7355  decode.d1.loss_cls: 0.4037  decode.d1.loss_mask: 1.6146  decode.d1.loss_dice: 1.6999  decode.d2.loss_cls: 0.4008  decode.d2.loss_mask: 1.6308  decode.d2.loss_dice: 1.7259  decode.d3.loss_cls: 0.3816  decode.d3.loss_mask: 1.6197  decode.d3.loss_dice: 1.7155  decode.d4.loss_cls: 0.3581  decode.d4.loss_mask: 1.6232  decode.d4.loss_dice: 1.7497  decode.d5.loss_cls: 0.3965  decode.d5.loss_mask: 1.6209  decode.d5.loss_dice: 1.7307  decode.d6.loss_cls: 0.3970  decode.d6.loss_mask: 1.6580  decode.d6.loss_dice: 1.7419  decode.d7.loss_cls: 0.4172  decode.d7.loss_mask: 1.6157  decode.d7.loss_dice: 1.7379  decode.d8.loss_cls: 0.3959  decode.d8.loss_mask: 1.6196  decode.d8.loss_dice: 1.7256  mix_decode.loss_cls: 0.3071  mix_decode.loss_mask: 0.5198  mix_decode.loss_dice: 0.7166  mix_decode.d0.loss_cls: 0.3439  mix_decode.d0.loss_mask: 0.5352  mix_decode.d0.loss_dice: 0.7979  mix_decode.d1.loss_cls: 0.3577  mix_decode.d1.loss_mask: 0.5049  mix_decode.d1.loss_dice: 0.6887  mix_decode.d2.loss_cls: 0.3310  mix_decode.d2.loss_mask: 0.5138  mix_decode.d2.loss_dice: 0.6802  mix_decode.d3.loss_cls: 0.2514  mix_decode.d3.loss_mask: 0.5254  mix_decode.d3.loss_dice: 0.7081  mix_decode.d4.loss_cls: 0.2955  mix_decode.d4.loss_mask: 0.5163  mix_decode.d4.loss_dice: 0.7148  mix_decode.d5.loss_cls: 0.3143  mix_decode.d5.loss_mask: 0.5224  mix_decode.d5.loss_dice: 0.7037  mix_decode.d6.loss_cls: 0.3104  mix_decode.d6.loss_mask: 0.5242  mix_decode.d6.loss_dice: 0.7164  mix_decode.d7.loss_cls: 0.3723  mix_decode.d7.loss_mask: 0.5027  mix_decode.d7.loss_dice: 0.6836  mix_decode.d8.loss_cls: 0.3019  mix_decode.d8.loss_mask: 0.5230  mix_decode.d8.loss_dice: 0.7097
2025/03/29 16:44:37 - mmengine - INFO - Iter(train) [15350/20000]  base_lr: 2.6903e-05 lr: 2.6903e-05  eta: 1:17:10  time: 1.1507  data_time: 0.0225  memory: 11211  loss: 56.1084  decode.loss_cls: 0.5225  decode.loss_mask: 1.8069  decode.loss_dice: 1.8628  decode.d0.loss_cls: 0.6445  decode.d0.loss_mask: 1.7617  decode.d0.loss_dice: 1.8418  decode.d1.loss_cls: 0.5607  decode.d1.loss_mask: 1.7394  decode.d1.loss_dice: 1.7533  decode.d2.loss_cls: 0.6168  decode.d2.loss_mask: 1.7608  decode.d2.loss_dice: 1.7441  decode.d3.loss_cls: 0.5703  decode.d3.loss_mask: 1.7645  decode.d3.loss_dice: 1.8079  decode.d4.loss_cls: 0.5188  decode.d4.loss_mask: 1.7403  decode.d4.loss_dice: 1.8110  decode.d5.loss_cls: 0.5715  decode.d5.loss_mask: 1.7357  decode.d5.loss_dice: 1.8456  decode.d6.loss_cls: 0.5445  decode.d6.loss_mask: 1.7408  decode.d6.loss_dice: 1.8299  decode.d7.loss_cls: 0.5826  decode.d7.loss_mask: 1.7687  decode.d7.loss_dice: 1.8456  decode.d8.loss_cls: 0.4657  decode.d8.loss_mask: 1.8076  decode.d8.loss_dice: 1.9089  mix_decode.loss_cls: 0.2573  mix_decode.loss_mask: 0.5087  mix_decode.loss_dice: 0.7106  mix_decode.d0.loss_cls: 0.2648  mix_decode.d0.loss_mask: 0.4688  mix_decode.d0.loss_dice: 0.7943  mix_decode.d1.loss_cls: 0.3040  mix_decode.d1.loss_mask: 0.4677  mix_decode.d1.loss_dice: 0.6844  mix_decode.d2.loss_cls: 0.2669  mix_decode.d2.loss_mask: 0.4862  mix_decode.d2.loss_dice: 0.6852  mix_decode.d3.loss_cls: 0.2532  mix_decode.d3.loss_mask: 0.4958  mix_decode.d3.loss_dice: 0.6985  mix_decode.d4.loss_cls: 0.2362  mix_decode.d4.loss_mask: 0.4810  mix_decode.d4.loss_dice: 0.7169  mix_decode.d5.loss_cls: 0.2765  mix_decode.d5.loss_mask: 0.4837  mix_decode.d5.loss_dice: 0.7081  mix_decode.d6.loss_cls: 0.2532  mix_decode.d6.loss_mask: 0.4710  mix_decode.d6.loss_dice: 0.7054  mix_decode.d7.loss_cls: 0.3093  mix_decode.d7.loss_mask: 0.4638  mix_decode.d7.loss_dice: 0.7177  mix_decode.d8.loss_cls: 0.2177  mix_decode.d8.loss_mask: 0.5161  mix_decode.d8.loss_dice: 0.7300
2025/03/29 16:45:35 - mmengine - INFO - Iter(train) [15400/20000]  base_lr: 2.6642e-05 lr: 2.6642e-05  eta: 1:16:22  time: 1.1591  data_time: 0.0238  memory: 11216  loss: 60.7771  decode.loss_cls: 0.4972  decode.loss_mask: 2.0715  decode.loss_dice: 1.8043  decode.d0.loss_cls: 0.8271  decode.d0.loss_mask: 2.0472  decode.d0.loss_dice: 1.8784  decode.d1.loss_cls: 0.6055  decode.d1.loss_mask: 2.0194  decode.d1.loss_dice: 1.8038  decode.d2.loss_cls: 0.4982  decode.d2.loss_mask: 2.0443  decode.d2.loss_dice: 1.7951  decode.d3.loss_cls: 0.4702  decode.d3.loss_mask: 2.1171  decode.d3.loss_dice: 1.8277  decode.d4.loss_cls: 0.5577  decode.d4.loss_mask: 2.0353  decode.d4.loss_dice: 1.7672  decode.d5.loss_cls: 0.5148  decode.d5.loss_mask: 2.0883  decode.d5.loss_dice: 1.8822  decode.d6.loss_cls: 0.5110  decode.d6.loss_mask: 2.1100  decode.d6.loss_dice: 1.7956  decode.d7.loss_cls: 0.4987  decode.d7.loss_mask: 2.1138  decode.d7.loss_dice: 1.8644  decode.d8.loss_cls: 0.5015  decode.d8.loss_mask: 2.0398  decode.d8.loss_dice: 1.8726  mix_decode.loss_cls: 0.2873  mix_decode.loss_mask: 0.6244  mix_decode.loss_dice: 0.6817  mix_decode.d0.loss_cls: 0.3566  mix_decode.d0.loss_mask: 0.6327  mix_decode.d0.loss_dice: 0.7642  mix_decode.d1.loss_cls: 0.3022  mix_decode.d1.loss_mask: 0.6311  mix_decode.d1.loss_dice: 0.6727  mix_decode.d2.loss_cls: 0.2976  mix_decode.d2.loss_mask: 0.6540  mix_decode.d2.loss_dice: 0.6764  mix_decode.d3.loss_cls: 0.2655  mix_decode.d3.loss_mask: 0.6286  mix_decode.d3.loss_dice: 0.6664  mix_decode.d4.loss_cls: 0.2718  mix_decode.d4.loss_mask: 0.6661  mix_decode.d4.loss_dice: 0.6897  mix_decode.d5.loss_cls: 0.2523  mix_decode.d5.loss_mask: 0.6272  mix_decode.d5.loss_dice: 0.7361  mix_decode.d6.loss_cls: 0.3063  mix_decode.d6.loss_mask: 0.6309  mix_decode.d6.loss_dice: 0.6840  mix_decode.d7.loss_cls: 0.3193  mix_decode.d7.loss_mask: 0.6463  mix_decode.d7.loss_dice: 0.6999  mix_decode.d8.loss_cls: 0.3236  mix_decode.d8.loss_mask: 0.6367  mix_decode.d8.loss_dice: 0.6858
2025/03/29 16:46:33 - mmengine - INFO - Iter(train) [15450/20000]  base_lr: 2.6382e-05 lr: 2.6382e-05  eta: 1:15:35  time: 1.1555  data_time: 0.0229  memory: 11219  loss: 56.2326  decode.loss_cls: 0.4334  decode.loss_mask: 1.9400  decode.loss_dice: 1.7778  decode.d0.loss_cls: 0.5947  decode.d0.loss_mask: 1.8092  decode.d0.loss_dice: 1.7459  decode.d1.loss_cls: 0.4535  decode.d1.loss_mask: 1.8623  decode.d1.loss_dice: 1.7769  decode.d2.loss_cls: 0.3801  decode.d2.loss_mask: 1.8086  decode.d2.loss_dice: 1.8072  decode.d3.loss_cls: 0.4224  decode.d3.loss_mask: 1.8303  decode.d3.loss_dice: 1.7793  decode.d4.loss_cls: 0.4124  decode.d4.loss_mask: 1.9258  decode.d4.loss_dice: 1.8307  decode.d5.loss_cls: 0.4150  decode.d5.loss_mask: 1.8550  decode.d5.loss_dice: 1.7664  decode.d6.loss_cls: 0.4428  decode.d6.loss_mask: 1.8278  decode.d6.loss_dice: 1.7844  decode.d7.loss_cls: 0.3744  decode.d7.loss_mask: 1.8402  decode.d7.loss_dice: 1.7694  decode.d8.loss_cls: 0.4371  decode.d8.loss_mask: 1.8335  decode.d8.loss_dice: 1.7291  mix_decode.loss_cls: 0.2400  mix_decode.loss_mask: 0.6140  mix_decode.loss_dice: 0.7185  mix_decode.d0.loss_cls: 0.2369  mix_decode.d0.loss_mask: 0.6241  mix_decode.d0.loss_dice: 0.7796  mix_decode.d1.loss_cls: 0.1972  mix_decode.d1.loss_mask: 0.6169  mix_decode.d1.loss_dice: 0.7069  mix_decode.d2.loss_cls: 0.2502  mix_decode.d2.loss_mask: 0.5974  mix_decode.d2.loss_dice: 0.6650  mix_decode.d3.loss_cls: 0.1816  mix_decode.d3.loss_mask: 0.6387  mix_decode.d3.loss_dice: 0.7326  mix_decode.d4.loss_cls: 0.1976  mix_decode.d4.loss_mask: 0.6238  mix_decode.d4.loss_dice: 0.7269  mix_decode.d5.loss_cls: 0.2247  mix_decode.d5.loss_mask: 0.6128  mix_decode.d5.loss_dice: 0.7326  mix_decode.d6.loss_cls: 0.1842  mix_decode.d6.loss_mask: 0.6205  mix_decode.d6.loss_dice: 0.7306  mix_decode.d7.loss_cls: 0.1916  mix_decode.d7.loss_mask: 0.6206  mix_decode.d7.loss_dice: 0.7074  mix_decode.d8.loss_cls: 0.2298  mix_decode.d8.loss_mask: 0.6294  mix_decode.d8.loss_dice: 0.7350
2025/03/29 16:47:31 - mmengine - INFO - Iter(train) [15500/20000]  base_lr: 2.6121e-05 lr: 2.6121e-05  eta: 1:14:47  time: 1.1599  data_time: 0.0239  memory: 11219  loss: 50.0258  decode.loss_cls: 0.2495  decode.loss_mask: 1.7796  decode.loss_dice: 1.7060  decode.d0.loss_cls: 0.3575  decode.d0.loss_mask: 1.7677  decode.d0.loss_dice: 1.7209  decode.d1.loss_cls: 0.3596  decode.d1.loss_mask: 1.7644  decode.d1.loss_dice: 1.6846  decode.d2.loss_cls: 0.2417  decode.d2.loss_mask: 1.7697  decode.d2.loss_dice: 1.7188  decode.d3.loss_cls: 0.2091  decode.d3.loss_mask: 1.7985  decode.d3.loss_dice: 1.7293  decode.d4.loss_cls: 0.3349  decode.d4.loss_mask: 1.7549  decode.d4.loss_dice: 1.6747  decode.d5.loss_cls: 0.2411  decode.d5.loss_mask: 1.7892  decode.d5.loss_dice: 1.7396  decode.d6.loss_cls: 0.2179  decode.d6.loss_mask: 1.7837  decode.d6.loss_dice: 1.7514  decode.d7.loss_cls: 0.2864  decode.d7.loss_mask: 1.7833  decode.d7.loss_dice: 1.7551  decode.d8.loss_cls: 0.2311  decode.d8.loss_mask: 1.7699  decode.d8.loss_dice: 1.7041  mix_decode.loss_cls: 0.1538  mix_decode.loss_mask: 0.5012  mix_decode.loss_dice: 0.5525  mix_decode.d0.loss_cls: 0.2987  mix_decode.d0.loss_mask: 0.4822  mix_decode.d0.loss_dice: 0.5656  mix_decode.d1.loss_cls: 0.2076  mix_decode.d1.loss_mask: 0.4825  mix_decode.d1.loss_dice: 0.5250  mix_decode.d2.loss_cls: 0.1570  mix_decode.d2.loss_mask: 0.5004  mix_decode.d2.loss_dice: 0.5378  mix_decode.d3.loss_cls: 0.1521  mix_decode.d3.loss_mask: 0.5083  mix_decode.d3.loss_dice: 0.5302  mix_decode.d4.loss_cls: 0.1739  mix_decode.d4.loss_mask: 0.5077  mix_decode.d4.loss_dice: 0.5395  mix_decode.d5.loss_cls: 0.2236  mix_decode.d5.loss_mask: 0.4770  mix_decode.d5.loss_dice: 0.5390  mix_decode.d6.loss_cls: 0.1909  mix_decode.d6.loss_mask: 0.4990  mix_decode.d6.loss_dice: 0.5361  mix_decode.d7.loss_cls: 0.1934  mix_decode.d7.loss_mask: 0.5019  mix_decode.d7.loss_dice: 0.5762  mix_decode.d8.loss_cls: 0.2047  mix_decode.d8.loss_mask: 0.4960  mix_decode.d8.loss_dice: 0.5376
2025/03/29 16:48:29 - mmengine - INFO - Iter(train) [15550/20000]  base_lr: 2.5859e-05 lr: 2.5859e-05  eta: 1:14:00  time: 1.1574  data_time: 0.0232  memory: 11219  loss: 54.5454  decode.loss_cls: 0.4484  decode.loss_mask: 1.7032  decode.loss_dice: 1.6678  decode.d0.loss_cls: 0.4769  decode.d0.loss_mask: 1.7146  decode.d0.loss_dice: 1.7620  decode.d1.loss_cls: 0.4981  decode.d1.loss_mask: 1.6821  decode.d1.loss_dice: 1.6266  decode.d2.loss_cls: 0.4728  decode.d2.loss_mask: 1.6848  decode.d2.loss_dice: 1.5997  decode.d3.loss_cls: 0.4962  decode.d3.loss_mask: 1.6989  decode.d3.loss_dice: 1.6337  decode.d4.loss_cls: 0.4824  decode.d4.loss_mask: 1.6704  decode.d4.loss_dice: 1.6848  decode.d5.loss_cls: 0.4321  decode.d5.loss_mask: 1.6977  decode.d5.loss_dice: 1.6856  decode.d6.loss_cls: 0.4220  decode.d6.loss_mask: 1.6841  decode.d6.loss_dice: 1.6789  decode.d7.loss_cls: 0.4597  decode.d7.loss_mask: 1.6712  decode.d7.loss_dice: 1.6692  decode.d8.loss_cls: 0.4258  decode.d8.loss_mask: 1.6898  decode.d8.loss_dice: 1.6942  mix_decode.loss_cls: 0.2417  mix_decode.loss_mask: 0.6526  mix_decode.loss_dice: 0.7435  mix_decode.d0.loss_cls: 0.2452  mix_decode.d0.loss_mask: 0.6212  mix_decode.d0.loss_dice: 0.7564  mix_decode.d1.loss_cls: 0.2346  mix_decode.d1.loss_mask: 0.6490  mix_decode.d1.loss_dice: 0.7418  mix_decode.d2.loss_cls: 0.2316  mix_decode.d2.loss_mask: 0.6498  mix_decode.d2.loss_dice: 0.7351  mix_decode.d3.loss_cls: 0.1807  mix_decode.d3.loss_mask: 0.6587  mix_decode.d3.loss_dice: 0.7474  mix_decode.d4.loss_cls: 0.2067  mix_decode.d4.loss_mask: 0.6670  mix_decode.d4.loss_dice: 0.7712  mix_decode.d5.loss_cls: 0.2277  mix_decode.d5.loss_mask: 0.6408  mix_decode.d5.loss_dice: 0.7545  mix_decode.d6.loss_cls: 0.2179  mix_decode.d6.loss_mask: 0.6473  mix_decode.d6.loss_dice: 0.7591  mix_decode.d7.loss_cls: 0.2428  mix_decode.d7.loss_mask: 0.6605  mix_decode.d7.loss_dice: 0.7752  mix_decode.d8.loss_cls: 0.2611  mix_decode.d8.loss_mask: 0.6564  mix_decode.d8.loss_dice: 0.7546
2025/03/29 16:49:27 - mmengine - INFO - Iter(train) [15600/20000]  base_lr: 2.5598e-05 lr: 2.5598e-05  eta: 1:13:12  time: 1.1581  data_time: 0.0240  memory: 11210  loss: 56.9459  decode.loss_cls: 0.4619  decode.loss_mask: 1.8216  decode.loss_dice: 1.7252  decode.d0.loss_cls: 0.5280  decode.d0.loss_mask: 1.8904  decode.d0.loss_dice: 1.7471  decode.d1.loss_cls: 0.5029  decode.d1.loss_mask: 1.8258  decode.d1.loss_dice: 1.6764  decode.d2.loss_cls: 0.4850  decode.d2.loss_mask: 1.7859  decode.d2.loss_dice: 1.6642  decode.d3.loss_cls: 0.5031  decode.d3.loss_mask: 1.8090  decode.d3.loss_dice: 1.6721  decode.d4.loss_cls: 0.5518  decode.d4.loss_mask: 1.7688  decode.d4.loss_dice: 1.6725  decode.d5.loss_cls: 0.5490  decode.d5.loss_mask: 1.7785  decode.d5.loss_dice: 1.6667  decode.d6.loss_cls: 0.4539  decode.d6.loss_mask: 1.8640  decode.d6.loss_dice: 1.7103  decode.d7.loss_cls: 0.4400  decode.d7.loss_mask: 1.8522  decode.d7.loss_dice: 1.7241  decode.d8.loss_cls: 0.4486  decode.d8.loss_mask: 1.8571  decode.d8.loss_dice: 1.7206  mix_decode.loss_cls: 0.2554  mix_decode.loss_mask: 0.6620  mix_decode.loss_dice: 0.7268  mix_decode.d0.loss_cls: 0.2290  mix_decode.d0.loss_mask: 0.6681  mix_decode.d0.loss_dice: 0.8244  mix_decode.d1.loss_cls: 0.3514  mix_decode.d1.loss_mask: 0.6329  mix_decode.d1.loss_dice: 0.7103  mix_decode.d2.loss_cls: 0.2578  mix_decode.d2.loss_mask: 0.6763  mix_decode.d2.loss_dice: 0.7312  mix_decode.d3.loss_cls: 0.2136  mix_decode.d3.loss_mask: 0.6613  mix_decode.d3.loss_dice: 0.7511  mix_decode.d4.loss_cls: 0.2799  mix_decode.d4.loss_mask: 0.6466  mix_decode.d4.loss_dice: 0.7543  mix_decode.d5.loss_cls: 0.2890  mix_decode.d5.loss_mask: 0.6513  mix_decode.d5.loss_dice: 0.7509  mix_decode.d6.loss_cls: 0.2835  mix_decode.d6.loss_mask: 0.6457  mix_decode.d6.loss_dice: 0.7521  mix_decode.d7.loss_cls: 0.2987  mix_decode.d7.loss_mask: 0.6527  mix_decode.d7.loss_dice: 0.7554  mix_decode.d8.loss_cls: 0.2819  mix_decode.d8.loss_mask: 0.6698  mix_decode.d8.loss_dice: 0.7258
2025/03/29 16:50:24 - mmengine - INFO - Iter(train) [15650/20000]  base_lr: 2.5336e-05 lr: 2.5336e-05  eta: 1:12:24  time: 1.1521  data_time: 0.0227  memory: 11218  loss: 56.5758  decode.loss_cls: 0.6004  decode.loss_mask: 1.6640  decode.loss_dice: 1.7951  decode.d0.loss_cls: 0.5506  decode.d0.loss_mask: 1.6916  decode.d0.loss_dice: 1.9506  decode.d1.loss_cls: 0.5379  decode.d1.loss_mask: 1.6521  decode.d1.loss_dice: 1.8387  decode.d2.loss_cls: 0.6154  decode.d2.loss_mask: 1.6615  decode.d2.loss_dice: 1.7949  decode.d3.loss_cls: 0.6190  decode.d3.loss_mask: 1.6457  decode.d3.loss_dice: 1.8123  decode.d4.loss_cls: 0.5861  decode.d4.loss_mask: 1.6879  decode.d4.loss_dice: 1.8689  decode.d5.loss_cls: 0.5512  decode.d5.loss_mask: 1.6519  decode.d5.loss_dice: 1.7659  decode.d6.loss_cls: 0.5948  decode.d6.loss_mask: 1.6639  decode.d6.loss_dice: 1.7945  decode.d7.loss_cls: 0.5756  decode.d7.loss_mask: 1.6533  decode.d7.loss_dice: 1.8188  decode.d8.loss_cls: 0.6294  decode.d8.loss_mask: 1.6607  decode.d8.loss_dice: 1.8454  mix_decode.loss_cls: 0.2377  mix_decode.loss_mask: 0.6101  mix_decode.loss_dice: 0.7467  mix_decode.d0.loss_cls: 0.2216  mix_decode.d0.loss_mask: 0.5756  mix_decode.d0.loss_dice: 0.7969  mix_decode.d1.loss_cls: 0.2474  mix_decode.d1.loss_mask: 0.5908  mix_decode.d1.loss_dice: 0.7552  mix_decode.d2.loss_cls: 0.2076  mix_decode.d2.loss_mask: 0.6026  mix_decode.d2.loss_dice: 0.7678  mix_decode.d3.loss_cls: 0.1772  mix_decode.d3.loss_mask: 0.6116  mix_decode.d3.loss_dice: 0.7506  mix_decode.d4.loss_cls: 0.1972  mix_decode.d4.loss_mask: 0.5977  mix_decode.d4.loss_dice: 0.7277  mix_decode.d5.loss_cls: 0.2045  mix_decode.d5.loss_mask: 0.6046  mix_decode.d5.loss_dice: 0.7548  mix_decode.d6.loss_cls: 0.2119  mix_decode.d6.loss_mask: 0.6298  mix_decode.d6.loss_dice: 0.7697  mix_decode.d7.loss_cls: 0.2392  mix_decode.d7.loss_mask: 0.5988  mix_decode.d7.loss_dice: 0.7733  mix_decode.d8.loss_cls: 0.2266  mix_decode.d8.loss_mask: 0.6133  mix_decode.d8.loss_dice: 0.7492
2025/03/29 16:51:22 - mmengine - INFO - Iter(train) [15700/20000]  base_lr: 2.5073e-05 lr: 2.5073e-05  eta: 1:11:37  time: 1.1519  data_time: 0.0230  memory: 11209  loss: 52.8610  decode.loss_cls: 0.4522  decode.loss_mask: 1.5581  decode.loss_dice: 1.7093  decode.d0.loss_cls: 0.5382  decode.d0.loss_mask: 1.5682  decode.d0.loss_dice: 1.7154  decode.d1.loss_cls: 0.5102  decode.d1.loss_mask: 1.4520  decode.d1.loss_dice: 1.6130  decode.d2.loss_cls: 0.4746  decode.d2.loss_mask: 1.5143  decode.d2.loss_dice: 1.6320  decode.d3.loss_cls: 0.5379  decode.d3.loss_mask: 1.4931  decode.d3.loss_dice: 1.6344  decode.d4.loss_cls: 0.5631  decode.d4.loss_mask: 1.4906  decode.d4.loss_dice: 1.6623  decode.d5.loss_cls: 0.5050  decode.d5.loss_mask: 1.5082  decode.d5.loss_dice: 1.7174  decode.d6.loss_cls: 0.5529  decode.d6.loss_mask: 1.5174  decode.d6.loss_dice: 1.6682  decode.d7.loss_cls: 0.5257  decode.d7.loss_mask: 1.5182  decode.d7.loss_dice: 1.6878  decode.d8.loss_cls: 0.4961  decode.d8.loss_mask: 1.4991  decode.d8.loss_dice: 1.6668  mix_decode.loss_cls: 0.3047  mix_decode.loss_mask: 0.6087  mix_decode.loss_dice: 0.6245  mix_decode.d0.loss_cls: 0.3726  mix_decode.d0.loss_mask: 0.5916  mix_decode.d0.loss_dice: 0.6962  mix_decode.d1.loss_cls: 0.3283  mix_decode.d1.loss_mask: 0.6119  mix_decode.d1.loss_dice: 0.6750  mix_decode.d2.loss_cls: 0.3262  mix_decode.d2.loss_mask: 0.5974  mix_decode.d2.loss_dice: 0.6502  mix_decode.d3.loss_cls: 0.3028  mix_decode.d3.loss_mask: 0.6224  mix_decode.d3.loss_dice: 0.6752  mix_decode.d4.loss_cls: 0.2976  mix_decode.d4.loss_mask: 0.6162  mix_decode.d4.loss_dice: 0.6419  mix_decode.d5.loss_cls: 0.3422  mix_decode.d5.loss_mask: 0.6046  mix_decode.d5.loss_dice: 0.6388  mix_decode.d6.loss_cls: 0.3105  mix_decode.d6.loss_mask: 0.5965  mix_decode.d6.loss_dice: 0.6980  mix_decode.d7.loss_cls: 0.2901  mix_decode.d7.loss_mask: 0.6017  mix_decode.d7.loss_dice: 0.6865  mix_decode.d8.loss_cls: 0.3310  mix_decode.d8.loss_mask: 0.5868  mix_decode.d8.loss_dice: 0.6494
2025/03/29 16:52:20 - mmengine - INFO - Iter(train) [15750/20000]  base_lr: 2.4811e-05 lr: 2.4811e-05  eta: 1:10:49  time: 1.1553  data_time: 0.0227  memory: 11219  loss: 51.9781  decode.loss_cls: 0.2752  decode.loss_mask: 1.6930  decode.loss_dice: 1.6893  decode.d0.loss_cls: 0.3222  decode.d0.loss_mask: 1.6917  decode.d0.loss_dice: 1.6862  decode.d1.loss_cls: 0.3618  decode.d1.loss_mask: 1.6939  decode.d1.loss_dice: 1.6504  decode.d2.loss_cls: 0.3067  decode.d2.loss_mask: 1.7302  decode.d2.loss_dice: 1.6555  decode.d3.loss_cls: 0.2666  decode.d3.loss_mask: 1.6802  decode.d3.loss_dice: 1.6714  decode.d4.loss_cls: 0.3044  decode.d4.loss_mask: 1.7184  decode.d4.loss_dice: 1.6572  decode.d5.loss_cls: 0.2939  decode.d5.loss_mask: 1.7150  decode.d5.loss_dice: 1.6981  decode.d6.loss_cls: 0.2916  decode.d6.loss_mask: 1.6825  decode.d6.loss_dice: 1.6383  decode.d7.loss_cls: 0.2268  decode.d7.loss_mask: 1.7111  decode.d7.loss_dice: 1.6810  decode.d8.loss_cls: 0.3495  decode.d8.loss_mask: 1.6767  decode.d8.loss_dice: 1.6226  mix_decode.loss_cls: 0.2181  mix_decode.loss_mask: 0.6014  mix_decode.loss_dice: 0.6868  mix_decode.d0.loss_cls: 0.2602  mix_decode.d0.loss_mask: 0.5735  mix_decode.d0.loss_dice: 0.7467  mix_decode.d1.loss_cls: 0.2558  mix_decode.d1.loss_mask: 0.6010  mix_decode.d1.loss_dice: 0.6923  mix_decode.d2.loss_cls: 0.2230  mix_decode.d2.loss_mask: 0.6077  mix_decode.d2.loss_dice: 0.6832  mix_decode.d3.loss_cls: 0.2315  mix_decode.d3.loss_mask: 0.5994  mix_decode.d3.loss_dice: 0.6648  mix_decode.d4.loss_cls: 0.2135  mix_decode.d4.loss_mask: 0.6030  mix_decode.d4.loss_dice: 0.6935  mix_decode.d5.loss_cls: 0.2669  mix_decode.d5.loss_mask: 0.5960  mix_decode.d5.loss_dice: 0.6860  mix_decode.d6.loss_cls: 0.2375  mix_decode.d6.loss_mask: 0.6059  mix_decode.d6.loss_dice: 0.7056  mix_decode.d7.loss_cls: 0.2202  mix_decode.d7.loss_mask: 0.6157  mix_decode.d7.loss_dice: 0.7143  mix_decode.d8.loss_cls: 0.2604  mix_decode.d8.loss_mask: 0.5872  mix_decode.d8.loss_dice: 0.6855
2025/03/29 16:53:18 - mmengine - INFO - Iter(train) [15800/20000]  base_lr: 2.4548e-05 lr: 2.4548e-05  eta: 1:10:01  time: 1.1573  data_time: 0.0237  memory: 11216  loss: 55.1102  decode.loss_cls: 0.2719  decode.loss_mask: 1.9375  decode.loss_dice: 1.7588  decode.d0.loss_cls: 0.4602  decode.d0.loss_mask: 2.0183  decode.d0.loss_dice: 1.8137  decode.d1.loss_cls: 0.3091  decode.d1.loss_mask: 1.9459  decode.d1.loss_dice: 1.7425  decode.d2.loss_cls: 0.2576  decode.d2.loss_mask: 1.9408  decode.d2.loss_dice: 1.7401  decode.d3.loss_cls: 0.2841  decode.d3.loss_mask: 1.9090  decode.d3.loss_dice: 1.7790  decode.d4.loss_cls: 0.3532  decode.d4.loss_mask: 1.9042  decode.d4.loss_dice: 1.7480  decode.d5.loss_cls: 0.2923  decode.d5.loss_mask: 1.8975  decode.d5.loss_dice: 1.7623  decode.d6.loss_cls: 0.2916  decode.d6.loss_mask: 1.8956  decode.d6.loss_dice: 1.7952  decode.d7.loss_cls: 0.3260  decode.d7.loss_mask: 1.8896  decode.d7.loss_dice: 1.7841  decode.d8.loss_cls: 0.2980  decode.d8.loss_mask: 1.9256  decode.d8.loss_dice: 1.7501  mix_decode.loss_cls: 0.2550  mix_decode.loss_mask: 0.5738  mix_decode.loss_dice: 0.7215  mix_decode.d0.loss_cls: 0.2470  mix_decode.d0.loss_mask: 0.5637  mix_decode.d0.loss_dice: 0.7723  mix_decode.d1.loss_cls: 0.1916  mix_decode.d1.loss_mask: 0.5682  mix_decode.d1.loss_dice: 0.7355  mix_decode.d2.loss_cls: 0.2102  mix_decode.d2.loss_mask: 0.5692  mix_decode.d2.loss_dice: 0.7010  mix_decode.d3.loss_cls: 0.1827  mix_decode.d3.loss_mask: 0.5636  mix_decode.d3.loss_dice: 0.7164  mix_decode.d4.loss_cls: 0.2106  mix_decode.d4.loss_mask: 0.5615  mix_decode.d4.loss_dice: 0.7088  mix_decode.d5.loss_cls: 0.2075  mix_decode.d5.loss_mask: 0.5640  mix_decode.d5.loss_dice: 0.7115  mix_decode.d6.loss_cls: 0.1999  mix_decode.d6.loss_mask: 0.5672  mix_decode.d6.loss_dice: 0.7189  mix_decode.d7.loss_cls: 0.2436  mix_decode.d7.loss_mask: 0.5672  mix_decode.d7.loss_dice: 0.7100  mix_decode.d8.loss_cls: 0.2063  mix_decode.d8.loss_mask: 0.5607  mix_decode.d8.loss_dice: 0.7189
2025/03/29 16:54:16 - mmengine - INFO - Iter(train) [15850/20000]  base_lr: 2.4285e-05 lr: 2.4285e-05  eta: 1:09:13  time: 1.1564  data_time: 0.0231  memory: 11218  loss: 55.1724  decode.loss_cls: 0.3960  decode.loss_mask: 1.9038  decode.loss_dice: 1.6542  decode.d0.loss_cls: 0.4594  decode.d0.loss_mask: 1.9619  decode.d0.loss_dice: 1.7036  decode.d1.loss_cls: 0.3524  decode.d1.loss_mask: 1.9425  decode.d1.loss_dice: 1.7046  decode.d2.loss_cls: 0.3909  decode.d2.loss_mask: 1.9408  decode.d2.loss_dice: 1.6627  decode.d3.loss_cls: 0.3671  decode.d3.loss_mask: 1.9207  decode.d3.loss_dice: 1.6524  decode.d4.loss_cls: 0.3705  decode.d4.loss_mask: 1.9319  decode.d4.loss_dice: 1.6457  decode.d5.loss_cls: 0.3712  decode.d5.loss_mask: 1.9243  decode.d5.loss_dice: 1.6942  decode.d6.loss_cls: 0.3460  decode.d6.loss_mask: 1.9387  decode.d6.loss_dice: 1.6774  decode.d7.loss_cls: 0.3684  decode.d7.loss_mask: 1.8961  decode.d7.loss_dice: 1.6631  decode.d8.loss_cls: 0.3884  decode.d8.loss_mask: 1.9148  decode.d8.loss_dice: 1.6831  mix_decode.loss_cls: 0.2455  mix_decode.loss_mask: 0.5697  mix_decode.loss_dice: 0.7022  mix_decode.d0.loss_cls: 0.2674  mix_decode.d0.loss_mask: 0.6005  mix_decode.d0.loss_dice: 0.7643  mix_decode.d1.loss_cls: 0.2124  mix_decode.d1.loss_mask: 0.5740  mix_decode.d1.loss_dice: 0.7194  mix_decode.d2.loss_cls: 0.2388  mix_decode.d2.loss_mask: 0.5935  mix_decode.d2.loss_dice: 0.7062  mix_decode.d3.loss_cls: 0.1935  mix_decode.d3.loss_mask: 0.5902  mix_decode.d3.loss_dice: 0.7156  mix_decode.d4.loss_cls: 0.2504  mix_decode.d4.loss_mask: 0.5838  mix_decode.d4.loss_dice: 0.7229  mix_decode.d5.loss_cls: 0.2394  mix_decode.d5.loss_mask: 0.5953  mix_decode.d5.loss_dice: 0.7183  mix_decode.d6.loss_cls: 0.2119  mix_decode.d6.loss_mask: 0.5780  mix_decode.d6.loss_dice: 0.7086  mix_decode.d7.loss_cls: 0.2264  mix_decode.d7.loss_mask: 0.5666  mix_decode.d7.loss_dice: 0.7258  mix_decode.d8.loss_cls: 0.2487  mix_decode.d8.loss_mask: 0.5743  mix_decode.d8.loss_dice: 0.7018
2025/03/29 16:55:14 - mmengine - INFO - Iter(train) [15900/20000]  base_lr: 2.4021e-05 lr: 2.4021e-05  eta: 1:08:25  time: 1.1574  data_time: 0.0227  memory: 11215  loss: 60.6543  decode.loss_cls: 0.5030  decode.loss_mask: 2.0677  decode.loss_dice: 1.9032  decode.d0.loss_cls: 0.5563  decode.d0.loss_mask: 2.0495  decode.d0.loss_dice: 1.8879  decode.d1.loss_cls: 0.4736  decode.d1.loss_mask: 2.0540  decode.d1.loss_dice: 1.8827  decode.d2.loss_cls: 0.4925  decode.d2.loss_mask: 2.0841  decode.d2.loss_dice: 1.8312  decode.d3.loss_cls: 0.5445  decode.d3.loss_mask: 2.0835  decode.d3.loss_dice: 1.9166  decode.d4.loss_cls: 0.4838  decode.d4.loss_mask: 2.0622  decode.d4.loss_dice: 1.8793  decode.d5.loss_cls: 0.4645  decode.d5.loss_mask: 2.0412  decode.d5.loss_dice: 1.8666  decode.d6.loss_cls: 0.4724  decode.d6.loss_mask: 2.0888  decode.d6.loss_dice: 1.8831  decode.d7.loss_cls: 0.5526  decode.d7.loss_mask: 2.0522  decode.d7.loss_dice: 1.8666  decode.d8.loss_cls: 0.5268  decode.d8.loss_mask: 2.0763  decode.d8.loss_dice: 1.8910  mix_decode.loss_cls: 0.2411  mix_decode.loss_mask: 0.6090  mix_decode.loss_dice: 0.7836  mix_decode.d0.loss_cls: 0.1823  mix_decode.d0.loss_mask: 0.6123  mix_decode.d0.loss_dice: 0.8803  mix_decode.d1.loss_cls: 0.1822  mix_decode.d1.loss_mask: 0.6214  mix_decode.d1.loss_dice: 0.7917  mix_decode.d2.loss_cls: 0.1940  mix_decode.d2.loss_mask: 0.6013  mix_decode.d2.loss_dice: 0.7448  mix_decode.d3.loss_cls: 0.2117  mix_decode.d3.loss_mask: 0.6052  mix_decode.d3.loss_dice: 0.7575  mix_decode.d4.loss_cls: 0.2031  mix_decode.d4.loss_mask: 0.5986  mix_decode.d4.loss_dice: 0.7889  mix_decode.d5.loss_cls: 0.2404  mix_decode.d5.loss_mask: 0.6094  mix_decode.d5.loss_dice: 0.7819  mix_decode.d6.loss_cls: 0.2454  mix_decode.d6.loss_mask: 0.6149  mix_decode.d6.loss_dice: 0.7748  mix_decode.d7.loss_cls: 0.2469  mix_decode.d7.loss_mask: 0.6174  mix_decode.d7.loss_dice: 0.7763  mix_decode.d8.loss_cls: 0.2465  mix_decode.d8.loss_mask: 0.6009  mix_decode.d8.loss_dice: 0.7531
2025/03/29 16:56:11 - mmengine - INFO - Iter(train) [15950/20000]  base_lr: 2.3758e-05 lr: 2.3758e-05  eta: 1:07:37  time: 1.1510  data_time: 0.0227  memory: 11208  loss: 55.2496  decode.loss_cls: 0.4040  decode.loss_mask: 1.8003  decode.loss_dice: 1.6329  decode.d0.loss_cls: 0.6145  decode.d0.loss_mask: 1.8061  decode.d0.loss_dice: 1.7081  decode.d1.loss_cls: 0.4920  decode.d1.loss_mask: 1.8117  decode.d1.loss_dice: 1.7226  decode.d2.loss_cls: 0.4301  decode.d2.loss_mask: 1.8150  decode.d2.loss_dice: 1.6735  decode.d3.loss_cls: 0.4478  decode.d3.loss_mask: 1.8312  decode.d3.loss_dice: 1.6599  decode.d4.loss_cls: 0.4552  decode.d4.loss_mask: 1.8514  decode.d4.loss_dice: 1.6679  decode.d5.loss_cls: 0.4578  decode.d5.loss_mask: 1.8214  decode.d5.loss_dice: 1.6702  decode.d6.loss_cls: 0.4309  decode.d6.loss_mask: 1.8092  decode.d6.loss_dice: 1.6847  decode.d7.loss_cls: 0.4284  decode.d7.loss_mask: 1.8295  decode.d7.loss_dice: 1.6643  decode.d8.loss_cls: 0.4187  decode.d8.loss_mask: 1.8031  decode.d8.loss_dice: 1.6915  mix_decode.loss_cls: 0.2378  mix_decode.loss_mask: 0.5905  mix_decode.loss_dice: 0.7273  mix_decode.d0.loss_cls: 0.2506  mix_decode.d0.loss_mask: 0.5875  mix_decode.d0.loss_dice: 0.8317  mix_decode.d1.loss_cls: 0.2324  mix_decode.d1.loss_mask: 0.5822  mix_decode.d1.loss_dice: 0.7244  mix_decode.d2.loss_cls: 0.2299  mix_decode.d2.loss_mask: 0.5842  mix_decode.d2.loss_dice: 0.6952  mix_decode.d3.loss_cls: 0.2333  mix_decode.d3.loss_mask: 0.6017  mix_decode.d3.loss_dice: 0.7215  mix_decode.d4.loss_cls: 0.2624  mix_decode.d4.loss_mask: 0.5728  mix_decode.d4.loss_dice: 0.7302  mix_decode.d5.loss_cls: 0.3129  mix_decode.d5.loss_mask: 0.5671  mix_decode.d5.loss_dice: 0.7222  mix_decode.d6.loss_cls: 0.2705  mix_decode.d6.loss_mask: 0.5705  mix_decode.d6.loss_dice: 0.7219  mix_decode.d7.loss_cls: 0.2769  mix_decode.d7.loss_mask: 0.5826  mix_decode.d7.loss_dice: 0.7172  mix_decode.d8.loss_cls: 0.2657  mix_decode.d8.loss_mask: 0.5897  mix_decode.d8.loss_dice: 0.7231
2025/03/29 16:57:09 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 16:57:09 - mmengine - INFO - Iter(train) [16000/20000]  base_lr: 2.3493e-05 lr: 2.3493e-05  eta: 1:06:49  time: 1.1554  data_time: 0.0232  memory: 11213  loss: 53.7968  decode.loss_cls: 0.5533  decode.loss_mask: 1.6672  decode.loss_dice: 1.5104  decode.d0.loss_cls: 0.4750  decode.d0.loss_mask: 1.6519  decode.d0.loss_dice: 1.6592  decode.d1.loss_cls: 0.5714  decode.d1.loss_mask: 1.6495  decode.d1.loss_dice: 1.4606  decode.d2.loss_cls: 0.5908  decode.d2.loss_mask: 1.6546  decode.d2.loss_dice: 1.4645  decode.d3.loss_cls: 0.5829  decode.d3.loss_mask: 1.6241  decode.d3.loss_dice: 1.4410  decode.d4.loss_cls: 0.4791  decode.d4.loss_mask: 1.6582  decode.d4.loss_dice: 1.5171  decode.d5.loss_cls: 0.5862  decode.d5.loss_mask: 1.6272  decode.d5.loss_dice: 1.4988  decode.d6.loss_cls: 0.5887  decode.d6.loss_mask: 1.6452  decode.d6.loss_dice: 1.4925  decode.d7.loss_cls: 0.5439  decode.d7.loss_mask: 1.6084  decode.d7.loss_dice: 1.5005  decode.d8.loss_cls: 0.5739  decode.d8.loss_mask: 1.6264  decode.d8.loss_dice: 1.5492  mix_decode.loss_cls: 0.2616  mix_decode.loss_mask: 0.5918  mix_decode.loss_dice: 0.7580  mix_decode.d0.loss_cls: 0.3461  mix_decode.d0.loss_mask: 0.6333  mix_decode.d0.loss_dice: 0.8371  mix_decode.d1.loss_cls: 0.2913  mix_decode.d1.loss_mask: 0.6223  mix_decode.d1.loss_dice: 0.7462  mix_decode.d2.loss_cls: 0.2618  mix_decode.d2.loss_mask: 0.6245  mix_decode.d2.loss_dice: 0.7438  mix_decode.d3.loss_cls: 0.2130  mix_decode.d3.loss_mask: 0.6749  mix_decode.d3.loss_dice: 0.7715  mix_decode.d4.loss_cls: 0.2950  mix_decode.d4.loss_mask: 0.6230  mix_decode.d4.loss_dice: 0.7689  mix_decode.d5.loss_cls: 0.2728  mix_decode.d5.loss_mask: 0.6312  mix_decode.d5.loss_dice: 0.7591  mix_decode.d6.loss_cls: 0.2713  mix_decode.d6.loss_mask: 0.6309  mix_decode.d6.loss_dice: 0.7669  mix_decode.d7.loss_cls: 0.3151  mix_decode.d7.loss_mask: 0.6331  mix_decode.d7.loss_dice: 0.7708  mix_decode.d8.loss_cls: 0.2479  mix_decode.d8.loss_mask: 0.6099  mix_decode.d8.loss_dice: 0.7720
2025/03/29 16:57:09 - mmengine - INFO - Saving checkpoint at 16000 iterations
2025/03/29 16:57:15 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:06:00  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:20 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:55  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:24 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:50  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 16:57:29 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:45  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:33 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:40  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:38 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:35  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:42 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:31  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:47 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:26  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:52 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:22  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 16:57:56 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:17  time: 0.0914  data_time: 0.0017  memory: 3080  
2025/03/29 16:58:01 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:13  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:05 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:08  time: 0.0914  data_time: 0.0017  memory: 3080  
2025/03/29 16:58:10 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:03  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:14 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:59  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:19 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:54  time: 0.0954  data_time: 0.0020  memory: 3080  
2025/03/29 16:58:24 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:50  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:28 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:45  time: 0.0918  data_time: 0.0017  memory: 3080  
2025/03/29 16:58:33 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:41  time: 0.0929  data_time: 0.0019  memory: 3080  
2025/03/29 16:58:38 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:36  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:42 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:32  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:47 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:27  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:51 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:23  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 16:58:56 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:18  time: 0.0914  data_time: 0.0017  memory: 3080  
2025/03/29 16:59:01 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:13  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:05 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:09  time: 0.0928  data_time: 0.0019  memory: 3080  
2025/03/29 16:59:10 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:04  time: 0.0918  data_time: 0.0017  memory: 3080  
2025/03/29 16:59:14 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:04:00  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:19 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:55  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:24 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:51  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:28 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:46  time: 0.0922  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:33 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:42  time: 0.0922  data_time: 0.0019  memory: 3080  
2025/03/29 16:59:37 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:37  time: 0.0923  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:42 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:47 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:28  time: 0.0922  data_time: 0.0018  memory: 3080  
2025/03/29 16:59:51 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:23  time: 0.0921  data_time: 0.0017  memory: 3080  
2025/03/29 16:59:56 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:19  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:00:01 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:14  time: 0.0920  data_time: 0.0019  memory: 3080  
2025/03/29 17:00:05 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:10  time: 0.0923  data_time: 0.0018  memory: 3080  
2025/03/29 17:00:10 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:05  time: 0.0922  data_time: 0.0018  memory: 3080  
2025/03/29 17:00:14 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0942  data_time: 0.0019  memory: 3080  
2025/03/29 17:00:19 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:56  time: 0.0928  data_time: 0.0020  memory: 3080  
2025/03/29 17:00:24 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:51  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:00:28 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:47  time: 0.0949  data_time: 0.0024  memory: 3080  
2025/03/29 17:00:33 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:42  time: 0.0920  data_time: 0.0017  memory: 3080  
2025/03/29 17:00:38 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:00:42 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:33  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:00:47 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0918  data_time: 0.0017  memory: 3080  
2025/03/29 17:00:51 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:24  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:00:56 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:19  time: 0.0922  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:01 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:15  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:05 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:10  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:10 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:15 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:01  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:19 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:24 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:52  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:01:28 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:47  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:33 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:38 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:38  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:42 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0918  data_time: 0.0017  memory: 3080  
2025/03/29 17:01:47 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:29  time: 0.0921  data_time: 0.0017  memory: 3080  
2025/03/29 17:01:51 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 17:01:56 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:01 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:15  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:05 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:10 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:06  time: 0.0922  data_time: 0.0019  memory: 3080  
2025/03/29 17:02:15 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:19 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:24 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:28 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:02:33 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:43  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:38 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0927  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:42 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:47 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0942  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:52 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:02:56 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:20  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:03:01 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 17:03:05 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 17:03:10 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0919  data_time: 0.0017  memory: 3080  
2025/03/29 17:03:15 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 17:03:16 - mmengine - INFO - per class results:
2025/03/29 17:03:16 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 57.45 | 86.23 |
|   building   | 57.07 | 70.99 |
|     road     | 44.16 | 47.67 |
|    water     | 66.94 | 74.24 |
|    barren    | 16.61 | 24.04 |
|    forest    | 25.55 | 27.52 |
| agricultural | 56.36 |  62.5 |
+--------------+-------+-------+
2025/03/29 17:03:16 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 70.8100  mIoU: 46.3100  mAcc: 56.1700  data_time: 0.0018  time: 0.0920
2025/03/29 17:04:14 - mmengine - INFO - Iter(train) [16050/20000]  base_lr: 2.3229e-05 lr: 2.3229e-05  eta: 1:06:00  time: 1.1573  data_time: 0.0240  memory: 11216  loss: 53.6182  decode.loss_cls: 0.3990  decode.loss_mask: 1.8733  decode.loss_dice: 1.6996  decode.d0.loss_cls: 0.4990  decode.d0.loss_mask: 1.8162  decode.d0.loss_dice: 1.6358  decode.d1.loss_cls: 0.4125  decode.d1.loss_mask: 1.8192  decode.d1.loss_dice: 1.7064  decode.d2.loss_cls: 0.4022  decode.d2.loss_mask: 1.8216  decode.d2.loss_dice: 1.6822  decode.d3.loss_cls: 0.3755  decode.d3.loss_mask: 1.8371  decode.d3.loss_dice: 1.7157  decode.d4.loss_cls: 0.3641  decode.d4.loss_mask: 1.8176  decode.d4.loss_dice: 1.6944  decode.d5.loss_cls: 0.3634  decode.d5.loss_mask: 1.8621  decode.d5.loss_dice: 1.7250  decode.d6.loss_cls: 0.3451  decode.d6.loss_mask: 1.8377  decode.d6.loss_dice: 1.7191  decode.d7.loss_cls: 0.4089  decode.d7.loss_mask: 1.8575  decode.d7.loss_dice: 1.7166  decode.d8.loss_cls: 0.3437  decode.d8.loss_mask: 1.8592  decode.d8.loss_dice: 1.7098  mix_decode.loss_cls: 0.1976  mix_decode.loss_mask: 0.5959  mix_decode.loss_dice: 0.6080  mix_decode.d0.loss_cls: 0.2526  mix_decode.d0.loss_mask: 0.5967  mix_decode.d0.loss_dice: 0.6701  mix_decode.d1.loss_cls: 0.1949  mix_decode.d1.loss_mask: 0.5885  mix_decode.d1.loss_dice: 0.6336  mix_decode.d2.loss_cls: 0.2049  mix_decode.d2.loss_mask: 0.5947  mix_decode.d2.loss_dice: 0.6161  mix_decode.d3.loss_cls: 0.2021  mix_decode.d3.loss_mask: 0.5793  mix_decode.d3.loss_dice: 0.6065  mix_decode.d4.loss_cls: 0.2365  mix_decode.d4.loss_mask: 0.5921  mix_decode.d4.loss_dice: 0.6308  mix_decode.d5.loss_cls: 0.1994  mix_decode.d5.loss_mask: 0.6121  mix_decode.d5.loss_dice: 0.6201  mix_decode.d6.loss_cls: 0.2111  mix_decode.d6.loss_mask: 0.5873  mix_decode.d6.loss_dice: 0.6097  mix_decode.d7.loss_cls: 0.2229  mix_decode.d7.loss_mask: 0.5997  mix_decode.d7.loss_dice: 0.6205  mix_decode.d8.loss_cls: 0.2223  mix_decode.d8.loss_mask: 0.5969  mix_decode.d8.loss_dice: 0.5960
2025/03/29 17:05:12 - mmengine - INFO - Iter(train) [16100/20000]  base_lr: 2.2964e-05 lr: 2.2964e-05  eta: 1:05:12  time: 1.1570  data_time: 0.0234  memory: 11215  loss: 54.5280  decode.loss_cls: 0.2941  decode.loss_mask: 1.8320  decode.loss_dice: 1.8001  decode.d0.loss_cls: 0.4788  decode.d0.loss_mask: 1.8655  decode.d0.loss_dice: 1.7809  decode.d1.loss_cls: 0.2969  decode.d1.loss_mask: 1.8357  decode.d1.loss_dice: 1.7872  decode.d2.loss_cls: 0.3771  decode.d2.loss_mask: 1.7948  decode.d2.loss_dice: 1.7845  decode.d3.loss_cls: 0.3393  decode.d3.loss_mask: 1.8248  decode.d3.loss_dice: 1.7879  decode.d4.loss_cls: 0.3146  decode.d4.loss_mask: 1.8148  decode.d4.loss_dice: 1.7888  decode.d5.loss_cls: 0.3315  decode.d5.loss_mask: 1.8044  decode.d5.loss_dice: 1.7934  decode.d6.loss_cls: 0.3280  decode.d6.loss_mask: 1.8368  decode.d6.loss_dice: 1.8057  decode.d7.loss_cls: 0.2695  decode.d7.loss_mask: 1.8073  decode.d7.loss_dice: 1.7865  decode.d8.loss_cls: 0.3146  decode.d8.loss_mask: 1.8376  decode.d8.loss_dice: 1.7928  mix_decode.loss_cls: 0.2044  mix_decode.loss_mask: 0.5923  mix_decode.loss_dice: 0.6921  mix_decode.d0.loss_cls: 0.2305  mix_decode.d0.loss_mask: 0.5925  mix_decode.d0.loss_dice: 0.7242  mix_decode.d1.loss_cls: 0.1684  mix_decode.d1.loss_mask: 0.6379  mix_decode.d1.loss_dice: 0.7088  mix_decode.d2.loss_cls: 0.1716  mix_decode.d2.loss_mask: 0.5897  mix_decode.d2.loss_dice: 0.7040  mix_decode.d3.loss_cls: 0.1922  mix_decode.d3.loss_mask: 0.5947  mix_decode.d3.loss_dice: 0.6900  mix_decode.d4.loss_cls: 0.2074  mix_decode.d4.loss_mask: 0.5959  mix_decode.d4.loss_dice: 0.6700  mix_decode.d5.loss_cls: 0.1960  mix_decode.d5.loss_mask: 0.5863  mix_decode.d5.loss_dice: 0.6908  mix_decode.d6.loss_cls: 0.2586  mix_decode.d6.loss_mask: 0.5990  mix_decode.d6.loss_dice: 0.6982  mix_decode.d7.loss_cls: 0.1906  mix_decode.d7.loss_mask: 0.5976  mix_decode.d7.loss_dice: 0.7038  mix_decode.d8.loss_cls: 0.2231  mix_decode.d8.loss_mask: 0.6039  mix_decode.d8.loss_dice: 0.7072
2025/03/29 17:06:11 - mmengine - INFO - Iter(train) [16150/20000]  base_lr: 2.2699e-05 lr: 2.2699e-05  eta: 1:04:24  time: 1.1623  data_time: 0.0242  memory: 11205  loss: 46.1449  decode.loss_cls: 0.3109  decode.loss_mask: 1.7048  decode.loss_dice: 1.4153  decode.d0.loss_cls: 0.5912  decode.d0.loss_mask: 1.6877  decode.d0.loss_dice: 1.4446  decode.d1.loss_cls: 0.4020  decode.d1.loss_mask: 1.6973  decode.d1.loss_dice: 1.3735  decode.d2.loss_cls: 0.3411  decode.d2.loss_mask: 1.6826  decode.d2.loss_dice: 1.3454  decode.d3.loss_cls: 0.3381  decode.d3.loss_mask: 1.7333  decode.d3.loss_dice: 1.3478  decode.d4.loss_cls: 0.3491  decode.d4.loss_mask: 1.7038  decode.d4.loss_dice: 1.3590  decode.d5.loss_cls: 0.3542  decode.d5.loss_mask: 1.7606  decode.d5.loss_dice: 1.3996  decode.d6.loss_cls: 0.4367  decode.d6.loss_mask: 1.7204  decode.d6.loss_dice: 1.3926  decode.d7.loss_cls: 0.3195  decode.d7.loss_mask: 1.7806  decode.d7.loss_dice: 1.3964  decode.d8.loss_cls: 0.4073  decode.d8.loss_mask: 1.6771  decode.d8.loss_dice: 1.3965  mix_decode.loss_cls: 0.0912  mix_decode.loss_mask: 0.5001  mix_decode.loss_dice: 0.4910  mix_decode.d0.loss_cls: 0.1897  mix_decode.d0.loss_mask: 0.5277  mix_decode.d0.loss_dice: 0.5177  mix_decode.d1.loss_cls: 0.1315  mix_decode.d1.loss_mask: 0.4928  mix_decode.d1.loss_dice: 0.4973  mix_decode.d2.loss_cls: 0.1135  mix_decode.d2.loss_mask: 0.5070  mix_decode.d2.loss_dice: 0.4977  mix_decode.d3.loss_cls: 0.1239  mix_decode.d3.loss_mask: 0.4902  mix_decode.d3.loss_dice: 0.4966  mix_decode.d4.loss_cls: 0.1320  mix_decode.d4.loss_mask: 0.4876  mix_decode.d4.loss_dice: 0.4655  mix_decode.d5.loss_cls: 0.1450  mix_decode.d5.loss_mask: 0.5133  mix_decode.d5.loss_dice: 0.5177  mix_decode.d6.loss_cls: 0.1331  mix_decode.d6.loss_mask: 0.5063  mix_decode.d6.loss_dice: 0.5058  mix_decode.d7.loss_cls: 0.1166  mix_decode.d7.loss_mask: 0.5031  mix_decode.d7.loss_dice: 0.4887  mix_decode.d8.loss_cls: 0.1260  mix_decode.d8.loss_mask: 0.4897  mix_decode.d8.loss_dice: 0.4776
2025/03/29 17:07:09 - mmengine - INFO - Iter(train) [16200/20000]  base_lr: 2.2434e-05 lr: 2.2434e-05  eta: 1:03:36  time: 1.1588  data_time: 0.0240  memory: 11216  loss: 56.4049  decode.loss_cls: 0.3749  decode.loss_mask: 1.9112  decode.loss_dice: 1.8229  decode.d0.loss_cls: 0.4992  decode.d0.loss_mask: 1.9517  decode.d0.loss_dice: 1.8317  decode.d1.loss_cls: 0.3956  decode.d1.loss_mask: 1.9037  decode.d1.loss_dice: 1.7872  decode.d2.loss_cls: 0.4423  decode.d2.loss_mask: 1.9064  decode.d2.loss_dice: 1.7880  decode.d3.loss_cls: 0.4498  decode.d3.loss_mask: 1.9586  decode.d3.loss_dice: 1.7992  decode.d4.loss_cls: 0.4170  decode.d4.loss_mask: 1.9308  decode.d4.loss_dice: 1.8377  decode.d5.loss_cls: 0.4079  decode.d5.loss_mask: 1.9394  decode.d5.loss_dice: 1.8416  decode.d6.loss_cls: 0.3852  decode.d6.loss_mask: 1.9653  decode.d6.loss_dice: 1.8454  decode.d7.loss_cls: 0.3029  decode.d7.loss_mask: 1.9403  decode.d7.loss_dice: 1.8503  decode.d8.loss_cls: 0.4016  decode.d8.loss_mask: 1.8885  decode.d8.loss_dice: 1.8315  mix_decode.loss_cls: 0.1670  mix_decode.loss_mask: 0.6149  mix_decode.loss_dice: 0.6571  mix_decode.d0.loss_cls: 0.2586  mix_decode.d0.loss_mask: 0.6151  mix_decode.d0.loss_dice: 0.6918  mix_decode.d1.loss_cls: 0.1691  mix_decode.d1.loss_mask: 0.6144  mix_decode.d1.loss_dice: 0.6929  mix_decode.d2.loss_cls: 0.2015  mix_decode.d2.loss_mask: 0.6053  mix_decode.d2.loss_dice: 0.6739  mix_decode.d3.loss_cls: 0.2003  mix_decode.d3.loss_mask: 0.6047  mix_decode.d3.loss_dice: 0.6608  mix_decode.d4.loss_cls: 0.1777  mix_decode.d4.loss_mask: 0.6062  mix_decode.d4.loss_dice: 0.6748  mix_decode.d5.loss_cls: 0.1930  mix_decode.d5.loss_mask: 0.6089  mix_decode.d5.loss_dice: 0.6680  mix_decode.d6.loss_cls: 0.1914  mix_decode.d6.loss_mask: 0.6120  mix_decode.d6.loss_dice: 0.6583  mix_decode.d7.loss_cls: 0.1807  mix_decode.d7.loss_mask: 0.6267  mix_decode.d7.loss_dice: 0.6906  mix_decode.d8.loss_cls: 0.1623  mix_decode.d8.loss_mask: 0.6368  mix_decode.d8.loss_dice: 0.6827
2025/03/29 17:08:07 - mmengine - INFO - Iter(train) [16250/20000]  base_lr: 2.2168e-05 lr: 2.2168e-05  eta: 1:02:47  time: 1.1558  data_time: 0.0231  memory: 11222  loss: 51.2264  decode.loss_cls: 0.2856  decode.loss_mask: 1.8262  decode.loss_dice: 1.5106  decode.d0.loss_cls: 0.3934  decode.d0.loss_mask: 1.8454  decode.d0.loss_dice: 1.5241  decode.d1.loss_cls: 0.3223  decode.d1.loss_mask: 1.7720  decode.d1.loss_dice: 1.5330  decode.d2.loss_cls: 0.2804  decode.d2.loss_mask: 1.8244  decode.d2.loss_dice: 1.5480  decode.d3.loss_cls: 0.2501  decode.d3.loss_mask: 1.7699  decode.d3.loss_dice: 1.5284  decode.d4.loss_cls: 0.2957  decode.d4.loss_mask: 1.7846  decode.d4.loss_dice: 1.4800  decode.d5.loss_cls: 0.2996  decode.d5.loss_mask: 1.7992  decode.d5.loss_dice: 1.4975  decode.d6.loss_cls: 0.2765  decode.d6.loss_mask: 1.8648  decode.d6.loss_dice: 1.5150  decode.d7.loss_cls: 0.3046  decode.d7.loss_mask: 1.8360  decode.d7.loss_dice: 1.4965  decode.d8.loss_cls: 0.2790  decode.d8.loss_mask: 1.8178  decode.d8.loss_dice: 1.5081  mix_decode.loss_cls: 0.2174  mix_decode.loss_mask: 0.5692  mix_decode.loss_dice: 0.6596  mix_decode.d0.loss_cls: 0.2926  mix_decode.d0.loss_mask: 0.5934  mix_decode.d0.loss_dice: 0.7186  mix_decode.d1.loss_cls: 0.2469  mix_decode.d1.loss_mask: 0.5780  mix_decode.d1.loss_dice: 0.6715  mix_decode.d2.loss_cls: 0.2411  mix_decode.d2.loss_mask: 0.5845  mix_decode.d2.loss_dice: 0.6865  mix_decode.d3.loss_cls: 0.1865  mix_decode.d3.loss_mask: 0.5921  mix_decode.d3.loss_dice: 0.6841  mix_decode.d4.loss_cls: 0.2227  mix_decode.d4.loss_mask: 0.5952  mix_decode.d4.loss_dice: 0.6748  mix_decode.d5.loss_cls: 0.2404  mix_decode.d5.loss_mask: 0.5741  mix_decode.d5.loss_dice: 0.6801  mix_decode.d6.loss_cls: 0.2157  mix_decode.d6.loss_mask: 0.5771  mix_decode.d6.loss_dice: 0.6819  mix_decode.d7.loss_cls: 0.2524  mix_decode.d7.loss_mask: 0.5753  mix_decode.d7.loss_dice: 0.6777  mix_decode.d8.loss_cls: 0.2027  mix_decode.d8.loss_mask: 0.5867  mix_decode.d8.loss_dice: 0.6790
2025/03/29 17:09:04 - mmengine - INFO - Iter(train) [16300/20000]  base_lr: 2.1902e-05 lr: 2.1902e-05  eta: 1:01:59  time: 1.1548  data_time: 0.0235  memory: 11222  loss: 48.2197  decode.loss_cls: 0.1856  decode.loss_mask: 1.7299  decode.loss_dice: 1.4891  decode.d0.loss_cls: 0.3525  decode.d0.loss_mask: 1.7478  decode.d0.loss_dice: 1.5336  decode.d1.loss_cls: 0.2677  decode.d1.loss_mask: 1.7607  decode.d1.loss_dice: 1.4976  decode.d2.loss_cls: 0.2403  decode.d2.loss_mask: 1.7209  decode.d2.loss_dice: 1.5144  decode.d3.loss_cls: 0.3279  decode.d3.loss_mask: 1.6604  decode.d3.loss_dice: 1.4958  decode.d4.loss_cls: 0.2336  decode.d4.loss_mask: 1.6992  decode.d4.loss_dice: 1.5355  decode.d5.loss_cls: 0.2768  decode.d5.loss_mask: 1.7057  decode.d5.loss_dice: 1.5007  decode.d6.loss_cls: 0.2748  decode.d6.loss_mask: 1.7202  decode.d6.loss_dice: 1.4705  decode.d7.loss_cls: 0.2302  decode.d7.loss_mask: 1.7453  decode.d7.loss_dice: 1.4975  decode.d8.loss_cls: 0.2236  decode.d8.loss_mask: 1.7252  decode.d8.loss_dice: 1.5102  mix_decode.loss_cls: 0.1639  mix_decode.loss_mask: 0.5662  mix_decode.loss_dice: 0.5864  mix_decode.d0.loss_cls: 0.2458  mix_decode.d0.loss_mask: 0.5474  mix_decode.d0.loss_dice: 0.6322  mix_decode.d1.loss_cls: 0.1935  mix_decode.d1.loss_mask: 0.5646  mix_decode.d1.loss_dice: 0.5754  mix_decode.d2.loss_cls: 0.1479  mix_decode.d2.loss_mask: 0.5761  mix_decode.d2.loss_dice: 0.5807  mix_decode.d3.loss_cls: 0.1498  mix_decode.d3.loss_mask: 0.5669  mix_decode.d3.loss_dice: 0.5891  mix_decode.d4.loss_cls: 0.1603  mix_decode.d4.loss_mask: 0.5641  mix_decode.d4.loss_dice: 0.6011  mix_decode.d5.loss_cls: 0.1894  mix_decode.d5.loss_mask: 0.5586  mix_decode.d5.loss_dice: 0.5950  mix_decode.d6.loss_cls: 0.1512  mix_decode.d6.loss_mask: 0.5747  mix_decode.d6.loss_dice: 0.5770  mix_decode.d7.loss_cls: 0.1773  mix_decode.d7.loss_mask: 0.5735  mix_decode.d7.loss_dice: 0.6018  mix_decode.d8.loss_cls: 0.1888  mix_decode.d8.loss_mask: 0.5629  mix_decode.d8.loss_dice: 0.5850
2025/03/29 17:10:02 - mmengine - INFO - Iter(train) [16350/20000]  base_lr: 2.1635e-05 lr: 2.1635e-05  eta: 1:01:10  time: 1.1502  data_time: 0.0223  memory: 11214  loss: 59.0803  decode.loss_cls: 0.3223  decode.loss_mask: 1.9741  decode.loss_dice: 1.8491  decode.d0.loss_cls: 0.4701  decode.d0.loss_mask: 1.9916  decode.d0.loss_dice: 1.8535  decode.d1.loss_cls: 0.3280  decode.d1.loss_mask: 1.9588  decode.d1.loss_dice: 1.8875  decode.d2.loss_cls: 0.3753  decode.d2.loss_mask: 1.9595  decode.d2.loss_dice: 1.8650  decode.d3.loss_cls: 0.3702  decode.d3.loss_mask: 1.9579  decode.d3.loss_dice: 1.8184  decode.d4.loss_cls: 0.3656  decode.d4.loss_mask: 1.9811  decode.d4.loss_dice: 1.8258  decode.d5.loss_cls: 0.2769  decode.d5.loss_mask: 1.9662  decode.d5.loss_dice: 1.8474  decode.d6.loss_cls: 0.3540  decode.d6.loss_mask: 1.9519  decode.d6.loss_dice: 1.8206  decode.d7.loss_cls: 0.3140  decode.d7.loss_mask: 1.9942  decode.d7.loss_dice: 1.8687  decode.d8.loss_cls: 0.3342  decode.d8.loss_mask: 1.9549  decode.d8.loss_dice: 1.8596  mix_decode.loss_cls: 0.3708  mix_decode.loss_mask: 0.5967  mix_decode.loss_dice: 0.8178  mix_decode.d0.loss_cls: 0.3445  mix_decode.d0.loss_mask: 0.5662  mix_decode.d0.loss_dice: 0.8913  mix_decode.d1.loss_cls: 0.3599  mix_decode.d1.loss_mask: 0.5188  mix_decode.d1.loss_dice: 0.7848  mix_decode.d2.loss_cls: 0.4063  mix_decode.d2.loss_mask: 0.5321  mix_decode.d2.loss_dice: 0.7779  mix_decode.d3.loss_cls: 0.3796  mix_decode.d3.loss_mask: 0.5291  mix_decode.d3.loss_dice: 0.7911  mix_decode.d4.loss_cls: 0.3612  mix_decode.d4.loss_mask: 0.5279  mix_decode.d4.loss_dice: 0.8293  mix_decode.d5.loss_cls: 0.3813  mix_decode.d5.loss_mask: 0.5213  mix_decode.d5.loss_dice: 0.8202  mix_decode.d6.loss_cls: 0.3437  mix_decode.d6.loss_mask: 0.5673  mix_decode.d6.loss_dice: 0.8025  mix_decode.d7.loss_cls: 0.3582  mix_decode.d7.loss_mask: 0.5726  mix_decode.d7.loss_dice: 0.8584  mix_decode.d8.loss_cls: 0.3925  mix_decode.d8.loss_mask: 0.5591  mix_decode.d8.loss_dice: 0.8214
2025/03/29 17:11:00 - mmengine - INFO - Iter(train) [16400/20000]  base_lr: 2.1368e-05 lr: 2.1368e-05  eta: 1:00:21  time: 1.1542  data_time: 0.0224  memory: 11213  loss: 53.1846  decode.loss_cls: 0.2809  decode.loss_mask: 1.7008  decode.loss_dice: 1.6525  decode.d0.loss_cls: 0.3694  decode.d0.loss_mask: 1.7672  decode.d0.loss_dice: 1.6770  decode.d1.loss_cls: 0.2986  decode.d1.loss_mask: 1.6955  decode.d1.loss_dice: 1.6571  decode.d2.loss_cls: 0.3260  decode.d2.loss_mask: 1.6866  decode.d2.loss_dice: 1.6558  decode.d3.loss_cls: 0.3530  decode.d3.loss_mask: 1.6825  decode.d3.loss_dice: 1.5840  decode.d4.loss_cls: 0.3220  decode.d4.loss_mask: 1.6986  decode.d4.loss_dice: 1.6385  decode.d5.loss_cls: 0.3246  decode.d5.loss_mask: 1.7257  decode.d5.loss_dice: 1.6688  decode.d6.loss_cls: 0.3000  decode.d6.loss_mask: 1.7221  decode.d6.loss_dice: 1.5869  decode.d7.loss_cls: 0.2826  decode.d7.loss_mask: 1.7364  decode.d7.loss_dice: 1.6156  decode.d8.loss_cls: 0.3228  decode.d8.loss_mask: 1.7352  decode.d8.loss_dice: 1.6147  mix_decode.loss_cls: 0.2172  mix_decode.loss_mask: 0.6895  mix_decode.loss_dice: 0.7131  mix_decode.d0.loss_cls: 0.2583  mix_decode.d0.loss_mask: 0.6772  mix_decode.d0.loss_dice: 0.7732  mix_decode.d1.loss_cls: 0.2293  mix_decode.d1.loss_mask: 0.6980  mix_decode.d1.loss_dice: 0.7092  mix_decode.d2.loss_cls: 0.2314  mix_decode.d2.loss_mask: 0.6778  mix_decode.d2.loss_dice: 0.7142  mix_decode.d3.loss_cls: 0.2187  mix_decode.d3.loss_mask: 0.6953  mix_decode.d3.loss_dice: 0.7269  mix_decode.d4.loss_cls: 0.2122  mix_decode.d4.loss_mask: 0.6975  mix_decode.d4.loss_dice: 0.7265  mix_decode.d5.loss_cls: 0.2440  mix_decode.d5.loss_mask: 0.6858  mix_decode.d5.loss_dice: 0.7198  mix_decode.d6.loss_cls: 0.2750  mix_decode.d6.loss_mask: 0.6889  mix_decode.d6.loss_dice: 0.7211  mix_decode.d7.loss_cls: 0.2273  mix_decode.d7.loss_mask: 0.7109  mix_decode.d7.loss_dice: 0.7452  mix_decode.d8.loss_cls: 0.1813  mix_decode.d8.loss_mask: 0.7016  mix_decode.d8.loss_dice: 0.7369
2025/03/29 17:11:58 - mmengine - INFO - Iter(train) [16450/20000]  base_lr: 2.1101e-05 lr: 2.1101e-05  eta: 0:59:33  time: 1.1524  data_time: 0.0227  memory: 11219  loss: 52.3889  decode.loss_cls: 0.2059  decode.loss_mask: 1.8198  decode.loss_dice: 1.6927  decode.d0.loss_cls: 0.2800  decode.d0.loss_mask: 1.9049  decode.d0.loss_dice: 1.7023  decode.d1.loss_cls: 0.2082  decode.d1.loss_mask: 1.8385  decode.d1.loss_dice: 1.6695  decode.d2.loss_cls: 0.1974  decode.d2.loss_mask: 1.8061  decode.d2.loss_dice: 1.6460  decode.d3.loss_cls: 0.2166  decode.d3.loss_mask: 1.8188  decode.d3.loss_dice: 1.6637  decode.d4.loss_cls: 0.2216  decode.d4.loss_mask: 1.8618  decode.d4.loss_dice: 1.6683  decode.d5.loss_cls: 0.1785  decode.d5.loss_mask: 1.8538  decode.d5.loss_dice: 1.7155  decode.d6.loss_cls: 0.1988  decode.d6.loss_mask: 1.8119  decode.d6.loss_dice: 1.6871  decode.d7.loss_cls: 0.1566  decode.d7.loss_mask: 1.8156  decode.d7.loss_dice: 1.6990  decode.d8.loss_cls: 0.2100  decode.d8.loss_mask: 1.8125  decode.d8.loss_dice: 1.6783  mix_decode.loss_cls: 0.1691  mix_decode.loss_mask: 0.6123  mix_decode.loss_dice: 0.7234  mix_decode.d0.loss_cls: 0.1794  mix_decode.d0.loss_mask: 0.6092  mix_decode.d0.loss_dice: 0.8148  mix_decode.d1.loss_cls: 0.1678  mix_decode.d1.loss_mask: 0.6104  mix_decode.d1.loss_dice: 0.7193  mix_decode.d2.loss_cls: 0.1638  mix_decode.d2.loss_mask: 0.5965  mix_decode.d2.loss_dice: 0.7197  mix_decode.d3.loss_cls: 0.2133  mix_decode.d3.loss_mask: 0.5816  mix_decode.d3.loss_dice: 0.7116  mix_decode.d4.loss_cls: 0.1750  mix_decode.d4.loss_mask: 0.5916  mix_decode.d4.loss_dice: 0.7149  mix_decode.d5.loss_cls: 0.1851  mix_decode.d5.loss_mask: 0.6061  mix_decode.d5.loss_dice: 0.7135  mix_decode.d6.loss_cls: 0.1661  mix_decode.d6.loss_mask: 0.6092  mix_decode.d6.loss_dice: 0.7492  mix_decode.d7.loss_cls: 0.1903  mix_decode.d7.loss_mask: 0.6071  mix_decode.d7.loss_dice: 0.7352  mix_decode.d8.loss_cls: 0.1915  mix_decode.d8.loss_mask: 0.5996  mix_decode.d8.loss_dice: 0.7227
2025/03/29 17:12:55 - mmengine - INFO - Iter(train) [16500/20000]  base_lr: 2.0833e-05 lr: 2.0833e-05  eta: 0:58:44  time: 1.1528  data_time: 0.0228  memory: 11208  loss: 57.0971  decode.loss_cls: 0.4135  decode.loss_mask: 1.9335  decode.loss_dice: 1.8141  decode.d0.loss_cls: 0.5691  decode.d0.loss_mask: 1.9333  decode.d0.loss_dice: 1.8185  decode.d1.loss_cls: 0.4742  decode.d1.loss_mask: 1.9283  decode.d1.loss_dice: 1.7897  decode.d2.loss_cls: 0.5189  decode.d2.loss_mask: 1.8849  decode.d2.loss_dice: 1.7596  decode.d3.loss_cls: 0.4725  decode.d3.loss_mask: 1.8913  decode.d3.loss_dice: 1.7787  decode.d4.loss_cls: 0.5210  decode.d4.loss_mask: 1.8954  decode.d4.loss_dice: 1.7252  decode.d5.loss_cls: 0.4760  decode.d5.loss_mask: 1.9038  decode.d5.loss_dice: 1.7930  decode.d6.loss_cls: 0.4412  decode.d6.loss_mask: 1.8672  decode.d6.loss_dice: 1.7846  decode.d7.loss_cls: 0.4612  decode.d7.loss_mask: 1.8892  decode.d7.loss_dice: 1.8232  decode.d8.loss_cls: 0.4373  decode.d8.loss_mask: 1.9483  decode.d8.loss_dice: 1.8148  mix_decode.loss_cls: 0.2893  mix_decode.loss_mask: 0.5387  mix_decode.loss_dice: 0.6619  mix_decode.d0.loss_cls: 0.2941  mix_decode.d0.loss_mask: 0.5531  mix_decode.d0.loss_dice: 0.7444  mix_decode.d1.loss_cls: 0.3161  mix_decode.d1.loss_mask: 0.5287  mix_decode.d1.loss_dice: 0.6400  mix_decode.d2.loss_cls: 0.3081  mix_decode.d2.loss_mask: 0.5539  mix_decode.d2.loss_dice: 0.6736  mix_decode.d3.loss_cls: 0.2895  mix_decode.d3.loss_mask: 0.5601  mix_decode.d3.loss_dice: 0.6621  mix_decode.d4.loss_cls: 0.3128  mix_decode.d4.loss_mask: 0.5405  mix_decode.d4.loss_dice: 0.6658  mix_decode.d5.loss_cls: 0.3307  mix_decode.d5.loss_mask: 0.5490  mix_decode.d5.loss_dice: 0.6584  mix_decode.d6.loss_cls: 0.3367  mix_decode.d6.loss_mask: 0.5363  mix_decode.d6.loss_dice: 0.6907  mix_decode.d7.loss_cls: 0.3473  mix_decode.d7.loss_mask: 0.5475  mix_decode.d7.loss_dice: 0.6838  mix_decode.d8.loss_cls: 0.3065  mix_decode.d8.loss_mask: 0.5472  mix_decode.d8.loss_dice: 0.6689
2025/03/29 17:13:53 - mmengine - INFO - Iter(train) [16550/20000]  base_lr: 2.0565e-05 lr: 2.0565e-05  eta: 0:57:55  time: 1.1553  data_time: 0.0229  memory: 11210  loss: 52.2231  decode.loss_cls: 0.3747  decode.loss_mask: 1.6184  decode.loss_dice: 1.7006  decode.d0.loss_cls: 0.4949  decode.d0.loss_mask: 1.6323  decode.d0.loss_dice: 1.7421  decode.d1.loss_cls: 0.3909  decode.d1.loss_mask: 1.6375  decode.d1.loss_dice: 1.7318  decode.d2.loss_cls: 0.4231  decode.d2.loss_mask: 1.5812  decode.d2.loss_dice: 1.6756  decode.d3.loss_cls: 0.4907  decode.d3.loss_mask: 1.6064  decode.d3.loss_dice: 1.6747  decode.d4.loss_cls: 0.4222  decode.d4.loss_mask: 1.6199  decode.d4.loss_dice: 1.6897  decode.d5.loss_cls: 0.3635  decode.d5.loss_mask: 1.6459  decode.d5.loss_dice: 1.7348  decode.d6.loss_cls: 0.3600  decode.d6.loss_mask: 1.6355  decode.d6.loss_dice: 1.6886  decode.d7.loss_cls: 0.3542  decode.d7.loss_mask: 1.6540  decode.d7.loss_dice: 1.7171  decode.d8.loss_cls: 0.4541  decode.d8.loss_mask: 1.5929  decode.d8.loss_dice: 1.6753  mix_decode.loss_cls: 0.1985  mix_decode.loss_mask: 0.5431  mix_decode.loss_dice: 0.6852  mix_decode.d0.loss_cls: 0.2727  mix_decode.d0.loss_mask: 0.5819  mix_decode.d0.loss_dice: 0.7414  mix_decode.d1.loss_cls: 0.2252  mix_decode.d1.loss_mask: 0.5652  mix_decode.d1.loss_dice: 0.6854  mix_decode.d2.loss_cls: 0.2519  mix_decode.d2.loss_mask: 0.5487  mix_decode.d2.loss_dice: 0.6730  mix_decode.d3.loss_cls: 0.2159  mix_decode.d3.loss_mask: 0.5539  mix_decode.d3.loss_dice: 0.6784  mix_decode.d4.loss_cls: 0.2158  mix_decode.d4.loss_mask: 0.5644  mix_decode.d4.loss_dice: 0.6882  mix_decode.d5.loss_cls: 0.2220  mix_decode.d5.loss_mask: 0.5757  mix_decode.d5.loss_dice: 0.6958  mix_decode.d6.loss_cls: 0.2401  mix_decode.d6.loss_mask: 0.5673  mix_decode.d6.loss_dice: 0.6967  mix_decode.d7.loss_cls: 0.2399  mix_decode.d7.loss_mask: 0.5690  mix_decode.d7.loss_dice: 0.6792  mix_decode.d8.loss_cls: 0.1972  mix_decode.d8.loss_mask: 0.5699  mix_decode.d8.loss_dice: 0.6987
2025/03/29 17:14:51 - mmengine - INFO - Iter(train) [16600/20000]  base_lr: 2.0297e-05 lr: 2.0297e-05  eta: 0:57:06  time: 1.1524  data_time: 0.0226  memory: 11214  loss: 51.4099  decode.loss_cls: 0.2896  decode.loss_mask: 1.6775  decode.loss_dice: 1.6182  decode.d0.loss_cls: 0.4294  decode.d0.loss_mask: 1.6430  decode.d0.loss_dice: 1.6326  decode.d1.loss_cls: 0.3927  decode.d1.loss_mask: 1.6409  decode.d1.loss_dice: 1.5775  decode.d2.loss_cls: 0.3647  decode.d2.loss_mask: 1.6085  decode.d2.loss_dice: 1.5397  decode.d3.loss_cls: 0.2989  decode.d3.loss_mask: 1.6686  decode.d3.loss_dice: 1.5993  decode.d4.loss_cls: 0.3321  decode.d4.loss_mask: 1.6594  decode.d4.loss_dice: 1.6149  decode.d5.loss_cls: 0.2921  decode.d5.loss_mask: 1.6570  decode.d5.loss_dice: 1.6495  decode.d6.loss_cls: 0.3122  decode.d6.loss_mask: 1.6681  decode.d6.loss_dice: 1.6139  decode.d7.loss_cls: 0.3041  decode.d7.loss_mask: 1.6642  decode.d7.loss_dice: 1.6422  decode.d8.loss_cls: 0.3334  decode.d8.loss_mask: 1.6490  decode.d8.loss_dice: 1.6124  mix_decode.loss_cls: 0.1898  mix_decode.loss_mask: 0.6255  mix_decode.loss_dice: 0.6939  mix_decode.d0.loss_cls: 0.2187  mix_decode.d0.loss_mask: 0.6021  mix_decode.d0.loss_dice: 0.7932  mix_decode.d1.loss_cls: 0.2798  mix_decode.d1.loss_mask: 0.6024  mix_decode.d1.loss_dice: 0.6797  mix_decode.d2.loss_cls: 0.2603  mix_decode.d2.loss_mask: 0.5890  mix_decode.d2.loss_dice: 0.7026  mix_decode.d3.loss_cls: 0.2763  mix_decode.d3.loss_mask: 0.5860  mix_decode.d3.loss_dice: 0.6735  mix_decode.d4.loss_cls: 0.2850  mix_decode.d4.loss_mask: 0.5762  mix_decode.d4.loss_dice: 0.6820  mix_decode.d5.loss_cls: 0.2761  mix_decode.d5.loss_mask: 0.5650  mix_decode.d5.loss_dice: 0.6928  mix_decode.d6.loss_cls: 0.2503  mix_decode.d6.loss_mask: 0.5762  mix_decode.d6.loss_dice: 0.6856  mix_decode.d7.loss_cls: 0.2634  mix_decode.d7.loss_mask: 0.5805  mix_decode.d7.loss_dice: 0.6812  mix_decode.d8.loss_cls: 0.2574  mix_decode.d8.loss_mask: 0.5949  mix_decode.d8.loss_dice: 0.6848
2025/03/29 17:15:49 - mmengine - INFO - Iter(train) [16650/20000]  base_lr: 2.0028e-05 lr: 2.0028e-05  eta: 0:56:17  time: 1.1596  data_time: 0.0229  memory: 11222  loss: 54.9582  decode.loss_cls: 0.3365  decode.loss_mask: 1.9920  decode.loss_dice: 1.6800  decode.d0.loss_cls: 0.4058  decode.d0.loss_mask: 2.0062  decode.d0.loss_dice: 1.7563  decode.d1.loss_cls: 0.3930  decode.d1.loss_mask: 1.9522  decode.d1.loss_dice: 1.6832  decode.d2.loss_cls: 0.3902  decode.d2.loss_mask: 1.9642  decode.d2.loss_dice: 1.6943  decode.d3.loss_cls: 0.3451  decode.d3.loss_mask: 1.9450  decode.d3.loss_dice: 1.6871  decode.d4.loss_cls: 0.3840  decode.d4.loss_mask: 1.9611  decode.d4.loss_dice: 1.6945  decode.d5.loss_cls: 0.4155  decode.d5.loss_mask: 1.9894  decode.d5.loss_dice: 1.6923  decode.d6.loss_cls: 0.3710  decode.d6.loss_mask: 1.9523  decode.d6.loss_dice: 1.6692  decode.d7.loss_cls: 0.3751  decode.d7.loss_mask: 1.9713  decode.d7.loss_dice: 1.7044  decode.d8.loss_cls: 0.3336  decode.d8.loss_mask: 1.9699  decode.d8.loss_dice: 1.6995  mix_decode.loss_cls: 0.2203  mix_decode.loss_mask: 0.5683  mix_decode.loss_dice: 0.6393  mix_decode.d0.loss_cls: 0.2426  mix_decode.d0.loss_mask: 0.5795  mix_decode.d0.loss_dice: 0.6911  mix_decode.d1.loss_cls: 0.2243  mix_decode.d1.loss_mask: 0.5671  mix_decode.d1.loss_dice: 0.6325  mix_decode.d2.loss_cls: 0.2127  mix_decode.d2.loss_mask: 0.5722  mix_decode.d2.loss_dice: 0.6310  mix_decode.d3.loss_cls: 0.2082  mix_decode.d3.loss_mask: 0.5738  mix_decode.d3.loss_dice: 0.6635  mix_decode.d4.loss_cls: 0.2931  mix_decode.d4.loss_mask: 0.5632  mix_decode.d4.loss_dice: 0.6205  mix_decode.d5.loss_cls: 0.2464  mix_decode.d5.loss_mask: 0.5720  mix_decode.d5.loss_dice: 0.6505  mix_decode.d6.loss_cls: 0.2743  mix_decode.d6.loss_mask: 0.5601  mix_decode.d6.loss_dice: 0.6318  mix_decode.d7.loss_cls: 0.2509  mix_decode.d7.loss_mask: 0.5620  mix_decode.d7.loss_dice: 0.6371  mix_decode.d8.loss_cls: 0.2463  mix_decode.d8.loss_mask: 0.5711  mix_decode.d8.loss_dice: 0.6381
2025/03/29 17:16:47 - mmengine - INFO - Iter(train) [16700/20000]  base_lr: 1.9759e-05 lr: 1.9759e-05  eta: 0:55:28  time: 1.1526  data_time: 0.0226  memory: 11208  loss: 54.4633  decode.loss_cls: 0.5677  decode.loss_mask: 1.7561  decode.loss_dice: 1.7322  decode.d0.loss_cls: 0.4464  decode.d0.loss_mask: 1.8618  decode.d0.loss_dice: 1.9110  decode.d1.loss_cls: 0.5703  decode.d1.loss_mask: 1.7655  decode.d1.loss_dice: 1.7085  decode.d2.loss_cls: 0.4796  decode.d2.loss_mask: 1.8094  decode.d2.loss_dice: 1.7463  decode.d3.loss_cls: 0.6088  decode.d3.loss_mask: 1.8258  decode.d3.loss_dice: 1.7562  decode.d4.loss_cls: 0.5003  decode.d4.loss_mask: 1.7980  decode.d4.loss_dice: 1.8010  decode.d5.loss_cls: 0.4829  decode.d5.loss_mask: 1.7994  decode.d5.loss_dice: 1.8052  decode.d6.loss_cls: 0.5178  decode.d6.loss_mask: 1.7910  decode.d6.loss_dice: 1.7739  decode.d7.loss_cls: 0.5386  decode.d7.loss_mask: 1.7993  decode.d7.loss_dice: 1.7700  decode.d8.loss_cls: 0.5127  decode.d8.loss_mask: 1.8097  decode.d8.loss_dice: 1.7849  mix_decode.loss_cls: 0.2591  mix_decode.loss_mask: 0.4852  mix_decode.loss_dice: 0.6256  mix_decode.d0.loss_cls: 0.2254  mix_decode.d0.loss_mask: 0.4901  mix_decode.d0.loss_dice: 0.6983  mix_decode.d1.loss_cls: 0.2142  mix_decode.d1.loss_mask: 0.4693  mix_decode.d1.loss_dice: 0.6303  mix_decode.d2.loss_cls: 0.2043  mix_decode.d2.loss_mask: 0.4721  mix_decode.d2.loss_dice: 0.6341  mix_decode.d3.loss_cls: 0.2005  mix_decode.d3.loss_mask: 0.4681  mix_decode.d3.loss_dice: 0.6189  mix_decode.d4.loss_cls: 0.2233  mix_decode.d4.loss_mask: 0.4712  mix_decode.d4.loss_dice: 0.6307  mix_decode.d5.loss_cls: 0.2521  mix_decode.d5.loss_mask: 0.4816  mix_decode.d5.loss_dice: 0.6391  mix_decode.d6.loss_cls: 0.2141  mix_decode.d6.loss_mask: 0.4721  mix_decode.d6.loss_dice: 0.6354  mix_decode.d7.loss_cls: 0.2339  mix_decode.d7.loss_mask: 0.4820  mix_decode.d7.loss_dice: 0.6526  mix_decode.d8.loss_cls: 0.2305  mix_decode.d8.loss_mask: 0.4738  mix_decode.d8.loss_dice: 0.6453
2025/03/29 17:17:45 - mmengine - INFO - Iter(train) [16750/20000]  base_lr: 1.9489e-05 lr: 1.9489e-05  eta: 0:54:40  time: 1.1567  data_time: 0.0229  memory: 11211  loss: 53.3239  decode.loss_cls: 0.3790  decode.loss_mask: 1.7368  decode.loss_dice: 1.6386  decode.d0.loss_cls: 0.5118  decode.d0.loss_mask: 1.7787  decode.d0.loss_dice: 1.6231  decode.d1.loss_cls: 0.3545  decode.d1.loss_mask: 1.7304  decode.d1.loss_dice: 1.6608  decode.d2.loss_cls: 0.3816  decode.d2.loss_mask: 1.7372  decode.d2.loss_dice: 1.6566  decode.d3.loss_cls: 0.3614  decode.d3.loss_mask: 1.7379  decode.d3.loss_dice: 1.6595  decode.d4.loss_cls: 0.3375  decode.d4.loss_mask: 1.7987  decode.d4.loss_dice: 1.6638  decode.d5.loss_cls: 0.4091  decode.d5.loss_mask: 1.7355  decode.d5.loss_dice: 1.6633  decode.d6.loss_cls: 0.4005  decode.d6.loss_mask: 1.7700  decode.d6.loss_dice: 1.6850  decode.d7.loss_cls: 0.3671  decode.d7.loss_mask: 1.7568  decode.d7.loss_dice: 1.6715  decode.d8.loss_cls: 0.3572  decode.d8.loss_mask: 1.7865  decode.d8.loss_dice: 1.6469  mix_decode.loss_cls: 0.2989  mix_decode.loss_mask: 0.5111  mix_decode.loss_dice: 0.7065  mix_decode.d0.loss_cls: 0.3192  mix_decode.d0.loss_mask: 0.5290  mix_decode.d0.loss_dice: 0.7481  mix_decode.d1.loss_cls: 0.3025  mix_decode.d1.loss_mask: 0.5113  mix_decode.d1.loss_dice: 0.6988  mix_decode.d2.loss_cls: 0.2724  mix_decode.d2.loss_mask: 0.5132  mix_decode.d2.loss_dice: 0.7333  mix_decode.d3.loss_cls: 0.2708  mix_decode.d3.loss_mask: 0.5099  mix_decode.d3.loss_dice: 0.7164  mix_decode.d4.loss_cls: 0.2819  mix_decode.d4.loss_mask: 0.5138  mix_decode.d4.loss_dice: 0.7342  mix_decode.d5.loss_cls: 0.2904  mix_decode.d5.loss_mask: 0.5200  mix_decode.d5.loss_dice: 0.7222  mix_decode.d6.loss_cls: 0.3080  mix_decode.d6.loss_mask: 0.5074  mix_decode.d6.loss_dice: 0.7208  mix_decode.d7.loss_cls: 0.3143  mix_decode.d7.loss_mask: 0.5096  mix_decode.d7.loss_dice: 0.7130  mix_decode.d8.loss_cls: 0.3256  mix_decode.d8.loss_mask: 0.5009  mix_decode.d8.loss_dice: 0.7231
2025/03/29 17:18:43 - mmengine - INFO - Iter(train) [16800/20000]  base_lr: 1.9219e-05 lr: 1.9219e-05  eta: 0:53:51  time: 1.1535  data_time: 0.0233  memory: 11218  loss: 50.1778  decode.loss_cls: 0.2152  decode.loss_mask: 1.6761  decode.loss_dice: 1.4668  decode.d0.loss_cls: 0.4106  decode.d0.loss_mask: 1.6711  decode.d0.loss_dice: 1.4714  decode.d1.loss_cls: 0.2449  decode.d1.loss_mask: 1.7011  decode.d1.loss_dice: 1.4783  decode.d2.loss_cls: 0.1839  decode.d2.loss_mask: 1.7038  decode.d2.loss_dice: 1.4942  decode.d3.loss_cls: 0.2209  decode.d3.loss_mask: 1.7098  decode.d3.loss_dice: 1.4525  decode.d4.loss_cls: 0.2314  decode.d4.loss_mask: 1.6662  decode.d4.loss_dice: 1.4416  decode.d5.loss_cls: 0.1996  decode.d5.loss_mask: 1.7275  decode.d5.loss_dice: 1.4965  decode.d6.loss_cls: 0.1872  decode.d6.loss_mask: 1.7078  decode.d6.loss_dice: 1.4741  decode.d7.loss_cls: 0.2161  decode.d7.loss_mask: 1.7063  decode.d7.loss_dice: 1.4830  decode.d8.loss_cls: 0.2208  decode.d8.loss_mask: 1.7038  decode.d8.loss_dice: 1.4655  mix_decode.loss_cls: 0.2160  mix_decode.loss_mask: 0.6350  mix_decode.loss_dice: 0.7380  mix_decode.d0.loss_cls: 0.2734  mix_decode.d0.loss_mask: 0.6404  mix_decode.d0.loss_dice: 0.7662  mix_decode.d1.loss_cls: 0.2204  mix_decode.d1.loss_mask: 0.6373  mix_decode.d1.loss_dice: 0.7374  mix_decode.d2.loss_cls: 0.2716  mix_decode.d2.loss_mask: 0.6044  mix_decode.d2.loss_dice: 0.7043  mix_decode.d3.loss_cls: 0.2273  mix_decode.d3.loss_mask: 0.6444  mix_decode.d3.loss_dice: 0.7399  mix_decode.d4.loss_cls: 0.2225  mix_decode.d4.loss_mask: 0.6483  mix_decode.d4.loss_dice: 0.7401  mix_decode.d5.loss_cls: 0.2735  mix_decode.d5.loss_mask: 0.6076  mix_decode.d5.loss_dice: 0.6994  mix_decode.d6.loss_cls: 0.2637  mix_decode.d6.loss_mask: 0.6303  mix_decode.d6.loss_dice: 0.7218  mix_decode.d7.loss_cls: 0.2804  mix_decode.d7.loss_mask: 0.6167  mix_decode.d7.loss_dice: 0.7344  mix_decode.d8.loss_cls: 0.2553  mix_decode.d8.loss_mask: 0.6439  mix_decode.d8.loss_dice: 0.7557
2025/03/29 17:19:40 - mmengine - INFO - Iter(train) [16850/20000]  base_lr: 1.8948e-05 lr: 1.8948e-05  eta: 0:53:01  time: 1.1538  data_time: 0.0229  memory: 11211  loss: 58.7678  decode.loss_cls: 0.4793  decode.loss_mask: 1.9817  decode.loss_dice: 1.7662  decode.d0.loss_cls: 0.6218  decode.d0.loss_mask: 1.9456  decode.d0.loss_dice: 1.7005  decode.d1.loss_cls: 0.5620  decode.d1.loss_mask: 1.9097  decode.d1.loss_dice: 1.7826  decode.d2.loss_cls: 0.5274  decode.d2.loss_mask: 2.0194  decode.d2.loss_dice: 1.7804  decode.d3.loss_cls: 0.5012  decode.d3.loss_mask: 1.9543  decode.d3.loss_dice: 1.6789  decode.d4.loss_cls: 0.5406  decode.d4.loss_mask: 1.9533  decode.d4.loss_dice: 1.7779  decode.d5.loss_cls: 0.4985  decode.d5.loss_mask: 1.9701  decode.d5.loss_dice: 1.8166  decode.d6.loss_cls: 0.5320  decode.d6.loss_mask: 1.9382  decode.d6.loss_dice: 1.7829  decode.d7.loss_cls: 0.5090  decode.d7.loss_mask: 2.0185  decode.d7.loss_dice: 1.7320  decode.d8.loss_cls: 0.4838  decode.d8.loss_mask: 2.0120  decode.d8.loss_dice: 1.7532  mix_decode.loss_cls: 0.2880  mix_decode.loss_mask: 0.6827  mix_decode.loss_dice: 0.6755  mix_decode.d0.loss_cls: 0.2763  mix_decode.d0.loss_mask: 0.6583  mix_decode.d0.loss_dice: 0.7328  mix_decode.d1.loss_cls: 0.2862  mix_decode.d1.loss_mask: 0.6574  mix_decode.d1.loss_dice: 0.6804  mix_decode.d2.loss_cls: 0.3024  mix_decode.d2.loss_mask: 0.6366  mix_decode.d2.loss_dice: 0.6070  mix_decode.d3.loss_cls: 0.3031  mix_decode.d3.loss_mask: 0.6338  mix_decode.d3.loss_dice: 0.6250  mix_decode.d4.loss_cls: 0.2955  mix_decode.d4.loss_mask: 0.6514  mix_decode.d4.loss_dice: 0.6466  mix_decode.d5.loss_cls: 0.3403  mix_decode.d5.loss_mask: 0.6749  mix_decode.d5.loss_dice: 0.6322  mix_decode.d6.loss_cls: 0.3358  mix_decode.d6.loss_mask: 0.6481  mix_decode.d6.loss_dice: 0.6476  mix_decode.d7.loss_cls: 0.3505  mix_decode.d7.loss_mask: 0.6511  mix_decode.d7.loss_dice: 0.6785  mix_decode.d8.loss_cls: 0.3215  mix_decode.d8.loss_mask: 0.6721  mix_decode.d8.loss_dice: 0.6467
2025/03/29 17:20:38 - mmengine - INFO - Iter(train) [16900/20000]  base_lr: 1.8677e-05 lr: 1.8677e-05  eta: 0:52:12  time: 1.1568  data_time: 0.0231  memory: 11225  loss: 54.3901  decode.loss_cls: 0.3865  decode.loss_mask: 1.8764  decode.loss_dice: 1.7180  decode.d0.loss_cls: 0.4700  decode.d0.loss_mask: 1.8930  decode.d0.loss_dice: 1.7279  decode.d1.loss_cls: 0.4246  decode.d1.loss_mask: 1.8712  decode.d1.loss_dice: 1.7301  decode.d2.loss_cls: 0.3362  decode.d2.loss_mask: 1.9095  decode.d2.loss_dice: 1.7375  decode.d3.loss_cls: 0.3499  decode.d3.loss_mask: 1.9007  decode.d3.loss_dice: 1.6881  decode.d4.loss_cls: 0.3198  decode.d4.loss_mask: 1.9010  decode.d4.loss_dice: 1.7111  decode.d5.loss_cls: 0.2631  decode.d5.loss_mask: 1.9127  decode.d5.loss_dice: 1.7464  decode.d6.loss_cls: 0.3627  decode.d6.loss_mask: 1.8740  decode.d6.loss_dice: 1.6913  decode.d7.loss_cls: 0.3633  decode.d7.loss_mask: 1.9017  decode.d7.loss_dice: 1.7210  decode.d8.loss_cls: 0.3582  decode.d8.loss_mask: 1.8855  decode.d8.loss_dice: 1.7180  mix_decode.loss_cls: 0.2275  mix_decode.loss_mask: 0.6325  mix_decode.loss_dice: 0.6084  mix_decode.d0.loss_cls: 0.2098  mix_decode.d0.loss_mask: 0.6154  mix_decode.d0.loss_dice: 0.6883  mix_decode.d1.loss_cls: 0.2029  mix_decode.d1.loss_mask: 0.5878  mix_decode.d1.loss_dice: 0.6588  mix_decode.d2.loss_cls: 0.2112  mix_decode.d2.loss_mask: 0.5959  mix_decode.d2.loss_dice: 0.6272  mix_decode.d3.loss_cls: 0.1937  mix_decode.d3.loss_mask: 0.6198  mix_decode.d3.loss_dice: 0.6602  mix_decode.d4.loss_cls: 0.1589  mix_decode.d4.loss_mask: 0.6318  mix_decode.d4.loss_dice: 0.6566  mix_decode.d5.loss_cls: 0.1904  mix_decode.d5.loss_mask: 0.6285  mix_decode.d5.loss_dice: 0.6316  mix_decode.d6.loss_cls: 0.1828  mix_decode.d6.loss_mask: 0.6267  mix_decode.d6.loss_dice: 0.6531  mix_decode.d7.loss_cls: 0.1961  mix_decode.d7.loss_mask: 0.6258  mix_decode.d7.loss_dice: 0.6541  mix_decode.d8.loss_cls: 0.1837  mix_decode.d8.loss_mask: 0.6406  mix_decode.d8.loss_dice: 0.6406
2025/03/29 17:21:36 - mmengine - INFO - Iter(train) [16950/20000]  base_lr: 1.8406e-05 lr: 1.8406e-05  eta: 0:51:23  time: 1.1504  data_time: 0.0227  memory: 11216  loss: 51.6880  decode.loss_cls: 0.3439  decode.loss_mask: 1.7123  decode.loss_dice: 1.5885  decode.d0.loss_cls: 0.4290  decode.d0.loss_mask: 1.8193  decode.d0.loss_dice: 1.7506  decode.d1.loss_cls: 0.3978  decode.d1.loss_mask: 1.6631  decode.d1.loss_dice: 1.6629  decode.d2.loss_cls: 0.3488  decode.d2.loss_mask: 1.6770  decode.d2.loss_dice: 1.6347  decode.d3.loss_cls: 0.3731  decode.d3.loss_mask: 1.6891  decode.d3.loss_dice: 1.6957  decode.d4.loss_cls: 0.3008  decode.d4.loss_mask: 1.8049  decode.d4.loss_dice: 1.7070  decode.d5.loss_cls: 0.3314  decode.d5.loss_mask: 1.7703  decode.d5.loss_dice: 1.6637  decode.d6.loss_cls: 0.3251  decode.d6.loss_mask: 1.7689  decode.d6.loss_dice: 1.6695  decode.d7.loss_cls: 0.3263  decode.d7.loss_mask: 1.7573  decode.d7.loss_dice: 1.7075  decode.d8.loss_cls: 0.2505  decode.d8.loss_mask: 1.8048  decode.d8.loss_dice: 1.6325  mix_decode.loss_cls: 0.1783  mix_decode.loss_mask: 0.5374  mix_decode.loss_dice: 0.6655  mix_decode.d0.loss_cls: 0.1765  mix_decode.d0.loss_mask: 0.5729  mix_decode.d0.loss_dice: 0.7627  mix_decode.d1.loss_cls: 0.1763  mix_decode.d1.loss_mask: 0.5403  mix_decode.d1.loss_dice: 0.6930  mix_decode.d2.loss_cls: 0.1553  mix_decode.d2.loss_mask: 0.5620  mix_decode.d2.loss_dice: 0.6812  mix_decode.d3.loss_cls: 0.1720  mix_decode.d3.loss_mask: 0.5628  mix_decode.d3.loss_dice: 0.6754  mix_decode.d4.loss_cls: 0.2020  mix_decode.d4.loss_mask: 0.5250  mix_decode.d4.loss_dice: 0.6664  mix_decode.d5.loss_cls: 0.1591  mix_decode.d5.loss_mask: 0.5581  mix_decode.d5.loss_dice: 0.6921  mix_decode.d6.loss_cls: 0.1501  mix_decode.d6.loss_mask: 0.5425  mix_decode.d6.loss_dice: 0.6805  mix_decode.d7.loss_cls: 0.1522  mix_decode.d7.loss_mask: 0.5623  mix_decode.d7.loss_dice: 0.6976  mix_decode.d8.loss_cls: 0.2099  mix_decode.d8.loss_mask: 0.5175  mix_decode.d8.loss_dice: 0.6552
2025/03/29 17:22:33 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 17:22:33 - mmengine - INFO - Iter(train) [17000/20000]  base_lr: 1.8134e-05 lr: 1.8134e-05  eta: 0:50:34  time: 1.1569  data_time: 0.0228  memory: 11209  loss: 53.5497  decode.loss_cls: 0.5750  decode.loss_mask: 1.5928  decode.loss_dice: 1.6939  decode.d0.loss_cls: 0.6815  decode.d0.loss_mask: 1.5977  decode.d0.loss_dice: 1.7961  decode.d1.loss_cls: 0.6244  decode.d1.loss_mask: 1.5780  decode.d1.loss_dice: 1.7050  decode.d2.loss_cls: 0.5594  decode.d2.loss_mask: 1.5815  decode.d2.loss_dice: 1.6853  decode.d3.loss_cls: 0.6203  decode.d3.loss_mask: 1.6016  decode.d3.loss_dice: 1.7310  decode.d4.loss_cls: 0.6361  decode.d4.loss_mask: 1.5505  decode.d4.loss_dice: 1.6842  decode.d5.loss_cls: 0.6522  decode.d5.loss_mask: 1.5567  decode.d5.loss_dice: 1.7149  decode.d6.loss_cls: 0.6694  decode.d6.loss_mask: 1.5708  decode.d6.loss_dice: 1.7052  decode.d7.loss_cls: 0.6074  decode.d7.loss_mask: 1.5641  decode.d7.loss_dice: 1.6837  decode.d8.loss_cls: 0.6087  decode.d8.loss_mask: 1.5815  decode.d8.loss_dice: 1.7103  mix_decode.loss_cls: 0.2765  mix_decode.loss_mask: 0.5143  mix_decode.loss_dice: 0.6230  mix_decode.d0.loss_cls: 0.3059  mix_decode.d0.loss_mask: 0.5247  mix_decode.d0.loss_dice: 0.7302  mix_decode.d1.loss_cls: 0.3018  mix_decode.d1.loss_mask: 0.5109  mix_decode.d1.loss_dice: 0.6214  mix_decode.d2.loss_cls: 0.2994  mix_decode.d2.loss_mask: 0.5215  mix_decode.d2.loss_dice: 0.6126  mix_decode.d3.loss_cls: 0.2415  mix_decode.d3.loss_mask: 0.5282  mix_decode.d3.loss_dice: 0.6199  mix_decode.d4.loss_cls: 0.2465  mix_decode.d4.loss_mask: 0.5240  mix_decode.d4.loss_dice: 0.6348  mix_decode.d5.loss_cls: 0.2957  mix_decode.d5.loss_mask: 0.5234  mix_decode.d5.loss_dice: 0.6242  mix_decode.d6.loss_cls: 0.2830  mix_decode.d6.loss_mask: 0.5199  mix_decode.d6.loss_dice: 0.6545  mix_decode.d7.loss_cls: 0.3123  mix_decode.d7.loss_mask: 0.5239  mix_decode.d7.loss_dice: 0.6207  mix_decode.d8.loss_cls: 0.2947  mix_decode.d8.loss_mask: 0.5193  mix_decode.d8.loss_dice: 0.6220
2025/03/29 17:23:31 - mmengine - INFO - Iter(train) [17050/20000]  base_lr: 1.7862e-05 lr: 1.7862e-05  eta: 0:49:44  time: 1.1675  data_time: 0.0252  memory: 11219  loss: 56.3619  decode.loss_cls: 0.3076  decode.loss_mask: 1.9629  decode.loss_dice: 1.5957  decode.d0.loss_cls: 0.4716  decode.d0.loss_mask: 1.9868  decode.d0.loss_dice: 1.6192  decode.d1.loss_cls: 0.3899  decode.d1.loss_mask: 1.9400  decode.d1.loss_dice: 1.5843  decode.d2.loss_cls: 0.3017  decode.d2.loss_mask: 1.9974  decode.d2.loss_dice: 1.5833  decode.d3.loss_cls: 0.3373  decode.d3.loss_mask: 1.9866  decode.d3.loss_dice: 1.5833  decode.d4.loss_cls: 0.3261  decode.d4.loss_mask: 1.9728  decode.d4.loss_dice: 1.6177  decode.d5.loss_cls: 0.2813  decode.d5.loss_mask: 1.9892  decode.d5.loss_dice: 1.6403  decode.d6.loss_cls: 0.3255  decode.d6.loss_mask: 1.9737  decode.d6.loss_dice: 1.6064  decode.d7.loss_cls: 0.3327  decode.d7.loss_mask: 1.9565  decode.d7.loss_dice: 1.6090  decode.d8.loss_cls: 0.3621  decode.d8.loss_mask: 1.9787  decode.d8.loss_dice: 1.5991  mix_decode.loss_cls: 0.2995  mix_decode.loss_mask: 0.6890  mix_decode.loss_dice: 0.7474  mix_decode.d0.loss_cls: 0.2830  mix_decode.d0.loss_mask: 0.7052  mix_decode.d0.loss_dice: 0.8368  mix_decode.d1.loss_cls: 0.2958  mix_decode.d1.loss_mask: 0.5946  mix_decode.d1.loss_dice: 0.7400  mix_decode.d2.loss_cls: 0.2500  mix_decode.d2.loss_mask: 0.6289  mix_decode.d2.loss_dice: 0.7403  mix_decode.d3.loss_cls: 0.2916  mix_decode.d3.loss_mask: 0.6098  mix_decode.d3.loss_dice: 0.7186  mix_decode.d4.loss_cls: 0.2657  mix_decode.d4.loss_mask: 0.6986  mix_decode.d4.loss_dice: 0.7613  mix_decode.d5.loss_cls: 0.2704  mix_decode.d5.loss_mask: 0.7153  mix_decode.d5.loss_dice: 0.7707  mix_decode.d6.loss_cls: 0.2939  mix_decode.d6.loss_mask: 0.6460  mix_decode.d6.loss_dice: 0.7995  mix_decode.d7.loss_cls: 0.2892  mix_decode.d7.loss_mask: 0.6979  mix_decode.d7.loss_dice: 0.7710  mix_decode.d8.loss_cls: 0.2826  mix_decode.d8.loss_mask: 0.6681  mix_decode.d8.loss_dice: 0.7827
2025/03/29 17:24:29 - mmengine - INFO - Iter(train) [17100/20000]  base_lr: 1.7589e-05 lr: 1.7589e-05  eta: 0:48:55  time: 1.1517  data_time: 0.0229  memory: 11211  loss: 48.8603  decode.loss_cls: 0.2430  decode.loss_mask: 1.6341  decode.loss_dice: 1.5890  decode.d0.loss_cls: 0.3519  decode.d0.loss_mask: 1.6164  decode.d0.loss_dice: 1.6389  decode.d1.loss_cls: 0.2851  decode.d1.loss_mask: 1.5982  decode.d1.loss_dice: 1.6011  decode.d2.loss_cls: 0.2673  decode.d2.loss_mask: 1.5743  decode.d2.loss_dice: 1.5601  decode.d3.loss_cls: 0.2516  decode.d3.loss_mask: 1.6102  decode.d3.loss_dice: 1.5851  decode.d4.loss_cls: 0.2885  decode.d4.loss_mask: 1.5883  decode.d4.loss_dice: 1.5694  decode.d5.loss_cls: 0.3114  decode.d5.loss_mask: 1.5750  decode.d5.loss_dice: 1.5755  decode.d6.loss_cls: 0.3270  decode.d6.loss_mask: 1.5580  decode.d6.loss_dice: 1.5733  decode.d7.loss_cls: 0.2555  decode.d7.loss_mask: 1.6519  decode.d7.loss_dice: 1.5826  decode.d8.loss_cls: 0.2539  decode.d8.loss_mask: 1.6040  decode.d8.loss_dice: 1.5889  mix_decode.loss_cls: 0.2457  mix_decode.loss_mask: 0.5145  mix_decode.loss_dice: 0.6577  mix_decode.d0.loss_cls: 0.2488  mix_decode.d0.loss_mask: 0.5052  mix_decode.d0.loss_dice: 0.7235  mix_decode.d1.loss_cls: 0.2556  mix_decode.d1.loss_mask: 0.5075  mix_decode.d1.loss_dice: 0.6625  mix_decode.d2.loss_cls: 0.2362  mix_decode.d2.loss_mask: 0.4972  mix_decode.d2.loss_dice: 0.6674  mix_decode.d3.loss_cls: 0.2346  mix_decode.d3.loss_mask: 0.4866  mix_decode.d3.loss_dice: 0.6479  mix_decode.d4.loss_cls: 0.2093  mix_decode.d4.loss_mask: 0.5065  mix_decode.d4.loss_dice: 0.6637  mix_decode.d5.loss_cls: 0.2307  mix_decode.d5.loss_mask: 0.5006  mix_decode.d5.loss_dice: 0.6687  mix_decode.d6.loss_cls: 0.2333  mix_decode.d6.loss_mask: 0.5148  mix_decode.d6.loss_dice: 0.6674  mix_decode.d7.loss_cls: 0.2416  mix_decode.d7.loss_mask: 0.5116  mix_decode.d7.loss_dice: 0.6812  mix_decode.d8.loss_cls: 0.2627  mix_decode.d8.loss_mask: 0.5028  mix_decode.d8.loss_dice: 0.6650
2025/03/29 17:25:27 - mmengine - INFO - Iter(train) [17150/20000]  base_lr: 1.7316e-05 lr: 1.7316e-05  eta: 0:48:06  time: 1.1520  data_time: 0.0225  memory: 11212  loss: 53.9941  decode.loss_cls: 0.3388  decode.loss_mask: 1.8265  decode.loss_dice: 1.7068  decode.d0.loss_cls: 0.4912  decode.d0.loss_mask: 1.8466  decode.d0.loss_dice: 1.7040  decode.d1.loss_cls: 0.3644  decode.d1.loss_mask: 1.8353  decode.d1.loss_dice: 1.6826  decode.d2.loss_cls: 0.3045  decode.d2.loss_mask: 1.8136  decode.d2.loss_dice: 1.6741  decode.d3.loss_cls: 0.2922  decode.d3.loss_mask: 1.8524  decode.d3.loss_dice: 1.7130  decode.d4.loss_cls: 0.3005  decode.d4.loss_mask: 1.8251  decode.d4.loss_dice: 1.6970  decode.d5.loss_cls: 0.3134  decode.d5.loss_mask: 1.8029  decode.d5.loss_dice: 1.7453  decode.d6.loss_cls: 0.3410  decode.d6.loss_mask: 1.8123  decode.d6.loss_dice: 1.7375  decode.d7.loss_cls: 0.3542  decode.d7.loss_mask: 1.8259  decode.d7.loss_dice: 1.6772  decode.d8.loss_cls: 0.3475  decode.d8.loss_mask: 1.8484  decode.d8.loss_dice: 1.7089  mix_decode.loss_cls: 0.2801  mix_decode.loss_mask: 0.5693  mix_decode.loss_dice: 0.6160  mix_decode.d0.loss_cls: 0.3736  mix_decode.d0.loss_mask: 0.5691  mix_decode.d0.loss_dice: 0.7475  mix_decode.d1.loss_cls: 0.3581  mix_decode.d1.loss_mask: 0.5514  mix_decode.d1.loss_dice: 0.6462  mix_decode.d2.loss_cls: 0.3464  mix_decode.d2.loss_mask: 0.5368  mix_decode.d2.loss_dice: 0.6032  mix_decode.d3.loss_cls: 0.2962  mix_decode.d3.loss_mask: 0.5558  mix_decode.d3.loss_dice: 0.6245  mix_decode.d4.loss_cls: 0.3011  mix_decode.d4.loss_mask: 0.5399  mix_decode.d4.loss_dice: 0.6271  mix_decode.d5.loss_cls: 0.3527  mix_decode.d5.loss_mask: 0.5439  mix_decode.d5.loss_dice: 0.6314  mix_decode.d6.loss_cls: 0.3444  mix_decode.d6.loss_mask: 0.5536  mix_decode.d6.loss_dice: 0.6411  mix_decode.d7.loss_cls: 0.2889  mix_decode.d7.loss_mask: 0.5725  mix_decode.d7.loss_dice: 0.6406  mix_decode.d8.loss_cls: 0.3148  mix_decode.d8.loss_mask: 0.5527  mix_decode.d8.loss_dice: 0.6324
2025/03/29 17:26:25 - mmengine - INFO - Iter(train) [17200/20000]  base_lr: 1.7043e-05 lr: 1.7043e-05  eta: 0:47:16  time: 1.1475  data_time: 0.0225  memory: 11207  loss: 61.1734  decode.loss_cls: 0.5074  decode.loss_mask: 2.0033  decode.loss_dice: 1.7892  decode.d0.loss_cls: 0.5773  decode.d0.loss_mask: 1.9947  decode.d0.loss_dice: 1.8851  decode.d1.loss_cls: 0.5469  decode.d1.loss_mask: 1.9646  decode.d1.loss_dice: 1.7659  decode.d2.loss_cls: 0.5846  decode.d2.loss_mask: 1.9388  decode.d2.loss_dice: 1.7502  decode.d3.loss_cls: 0.4818  decode.d3.loss_mask: 1.9524  decode.d3.loss_dice: 1.7871  decode.d4.loss_cls: 0.5450  decode.d4.loss_mask: 1.9565  decode.d4.loss_dice: 1.7600  decode.d5.loss_cls: 0.4931  decode.d5.loss_mask: 2.0328  decode.d5.loss_dice: 1.7628  decode.d6.loss_cls: 0.5047  decode.d6.loss_mask: 2.0115  decode.d6.loss_dice: 1.7841  decode.d7.loss_cls: 0.5689  decode.d7.loss_mask: 1.9423  decode.d7.loss_dice: 1.7521  decode.d8.loss_cls: 0.4874  decode.d8.loss_mask: 1.9673  decode.d8.loss_dice: 1.7793  mix_decode.loss_cls: 0.2737  mix_decode.loss_mask: 0.7273  mix_decode.loss_dice: 0.8035  mix_decode.d0.loss_cls: 0.3323  mix_decode.d0.loss_mask: 0.7277  mix_decode.d0.loss_dice: 0.8418  mix_decode.d1.loss_cls: 0.2694  mix_decode.d1.loss_mask: 0.7305  mix_decode.d1.loss_dice: 0.8182  mix_decode.d2.loss_cls: 0.2497  mix_decode.d2.loss_mask: 0.7396  mix_decode.d2.loss_dice: 0.8060  mix_decode.d3.loss_cls: 0.2587  mix_decode.d3.loss_mask: 0.7340  mix_decode.d3.loss_dice: 0.8171  mix_decode.d4.loss_cls: 0.2862  mix_decode.d4.loss_mask: 0.7362  mix_decode.d4.loss_dice: 0.7895  mix_decode.d5.loss_cls: 0.2940  mix_decode.d5.loss_mask: 0.7233  mix_decode.d5.loss_dice: 0.8290  mix_decode.d6.loss_cls: 0.3329  mix_decode.d6.loss_mask: 0.7420  mix_decode.d6.loss_dice: 0.8023  mix_decode.d7.loss_cls: 0.2891  mix_decode.d7.loss_mask: 0.7464  mix_decode.d7.loss_dice: 0.7915  mix_decode.d8.loss_cls: 0.2927  mix_decode.d8.loss_mask: 0.7297  mix_decode.d8.loss_dice: 0.7822
2025/03/29 17:27:22 - mmengine - INFO - Iter(train) [17250/20000]  base_lr: 1.6768e-05 lr: 1.6768e-05  eta: 0:46:27  time: 1.1540  data_time: 0.0226  memory: 11218  loss: 49.6293  decode.loss_cls: 0.4183  decode.loss_mask: 1.5620  decode.loss_dice: 1.5285  decode.d0.loss_cls: 0.4923  decode.d0.loss_mask: 1.5535  decode.d0.loss_dice: 1.6261  decode.d1.loss_cls: 0.4776  decode.d1.loss_mask: 1.5388  decode.d1.loss_dice: 1.5328  decode.d2.loss_cls: 0.4628  decode.d2.loss_mask: 1.5403  decode.d2.loss_dice: 1.5144  decode.d3.loss_cls: 0.4273  decode.d3.loss_mask: 1.5226  decode.d3.loss_dice: 1.5202  decode.d4.loss_cls: 0.4320  decode.d4.loss_mask: 1.4897  decode.d4.loss_dice: 1.5356  decode.d5.loss_cls: 0.3806  decode.d5.loss_mask: 1.5093  decode.d5.loss_dice: 1.5561  decode.d6.loss_cls: 0.4400  decode.d6.loss_mask: 1.5352  decode.d6.loss_dice: 1.5285  decode.d7.loss_cls: 0.3569  decode.d7.loss_mask: 1.5727  decode.d7.loss_dice: 1.5206  decode.d8.loss_cls: 0.4248  decode.d8.loss_mask: 1.5385  decode.d8.loss_dice: 1.4852  mix_decode.loss_cls: 0.2231  mix_decode.loss_mask: 0.5660  mix_decode.loss_dice: 0.6561  mix_decode.d0.loss_cls: 0.2506  mix_decode.d0.loss_mask: 0.5641  mix_decode.d0.loss_dice: 0.7051  mix_decode.d1.loss_cls: 0.1826  mix_decode.d1.loss_mask: 0.5850  mix_decode.d1.loss_dice: 0.6517  mix_decode.d2.loss_cls: 0.2034  mix_decode.d2.loss_mask: 0.5907  mix_decode.d2.loss_dice: 0.6501  mix_decode.d3.loss_cls: 0.2041  mix_decode.d3.loss_mask: 0.5865  mix_decode.d3.loss_dice: 0.6611  mix_decode.d4.loss_cls: 0.1976  mix_decode.d4.loss_mask: 0.5740  mix_decode.d4.loss_dice: 0.6594  mix_decode.d5.loss_cls: 0.1991  mix_decode.d5.loss_mask: 0.6046  mix_decode.d5.loss_dice: 0.6771  mix_decode.d6.loss_cls: 0.2102  mix_decode.d6.loss_mask: 0.5928  mix_decode.d6.loss_dice: 0.6549  mix_decode.d7.loss_cls: 0.2002  mix_decode.d7.loss_mask: 0.5992  mix_decode.d7.loss_dice: 0.7028  mix_decode.d8.loss_cls: 0.2005  mix_decode.d8.loss_mask: 0.5902  mix_decode.d8.loss_dice: 0.6633
2025/03/29 17:28:20 - mmengine - INFO - Iter(train) [17300/20000]  base_lr: 1.6494e-05 lr: 1.6494e-05  eta: 0:45:37  time: 1.1533  data_time: 0.0230  memory: 11214  loss: 57.8882  decode.loss_cls: 0.3818  decode.loss_mask: 1.8721  decode.loss_dice: 1.7687  decode.d0.loss_cls: 0.5421  decode.d0.loss_mask: 1.9327  decode.d0.loss_dice: 1.8000  decode.d1.loss_cls: 0.4599  decode.d1.loss_mask: 1.8386  decode.d1.loss_dice: 1.7245  decode.d2.loss_cls: 0.4626  decode.d2.loss_mask: 1.8631  decode.d2.loss_dice: 1.7242  decode.d3.loss_cls: 0.4263  decode.d3.loss_mask: 1.8911  decode.d3.loss_dice: 1.7775  decode.d4.loss_cls: 0.4544  decode.d4.loss_mask: 1.8245  decode.d4.loss_dice: 1.7451  decode.d5.loss_cls: 0.4654  decode.d5.loss_mask: 1.8433  decode.d5.loss_dice: 1.7776  decode.d6.loss_cls: 0.4753  decode.d6.loss_mask: 1.8607  decode.d6.loss_dice: 1.7861  decode.d7.loss_cls: 0.4519  decode.d7.loss_mask: 1.8710  decode.d7.loss_dice: 1.7913  decode.d8.loss_cls: 0.3964  decode.d8.loss_mask: 1.8657  decode.d8.loss_dice: 1.7822  mix_decode.loss_cls: 0.2350  mix_decode.loss_mask: 0.6244  mix_decode.loss_dice: 0.8211  mix_decode.d0.loss_cls: 0.2607  mix_decode.d0.loss_mask: 0.6012  mix_decode.d0.loss_dice: 0.8810  mix_decode.d1.loss_cls: 0.2774  mix_decode.d1.loss_mask: 0.5873  mix_decode.d1.loss_dice: 0.8278  mix_decode.d2.loss_cls: 0.2397  mix_decode.d2.loss_mask: 0.6300  mix_decode.d2.loss_dice: 0.8330  mix_decode.d3.loss_cls: 0.2622  mix_decode.d3.loss_mask: 0.6244  mix_decode.d3.loss_dice: 0.8383  mix_decode.d4.loss_cls: 0.2813  mix_decode.d4.loss_mask: 0.5974  mix_decode.d4.loss_dice: 0.8176  mix_decode.d5.loss_cls: 0.2455  mix_decode.d5.loss_mask: 0.6103  mix_decode.d5.loss_dice: 0.8304  mix_decode.d6.loss_cls: 0.2524  mix_decode.d6.loss_mask: 0.6195  mix_decode.d6.loss_dice: 0.8401  mix_decode.d7.loss_cls: 0.2565  mix_decode.d7.loss_mask: 0.6188  mix_decode.d7.loss_dice: 0.8365  mix_decode.d8.loss_cls: 0.2434  mix_decode.d8.loss_mask: 0.6028  mix_decode.d8.loss_dice: 0.8358
2025/03/29 17:29:17 - mmengine - INFO - Iter(train) [17350/20000]  base_lr: 1.6219e-05 lr: 1.6219e-05  eta: 0:44:47  time: 1.1438  data_time: 0.0225  memory: 11209  loss: 52.7204  decode.loss_cls: 0.4346  decode.loss_mask: 1.7033  decode.loss_dice: 1.6885  decode.d0.loss_cls: 0.5079  decode.d0.loss_mask: 1.7783  decode.d0.loss_dice: 1.6678  decode.d1.loss_cls: 0.4236  decode.d1.loss_mask: 1.7593  decode.d1.loss_dice: 1.6492  decode.d2.loss_cls: 0.4383  decode.d2.loss_mask: 1.7684  decode.d2.loss_dice: 1.6567  decode.d3.loss_cls: 0.4146  decode.d3.loss_mask: 1.7686  decode.d3.loss_dice: 1.6553  decode.d4.loss_cls: 0.4142  decode.d4.loss_mask: 1.7627  decode.d4.loss_dice: 1.6293  decode.d5.loss_cls: 0.4617  decode.d5.loss_mask: 1.6908  decode.d5.loss_dice: 1.7043  decode.d6.loss_cls: 0.4605  decode.d6.loss_mask: 1.7973  decode.d6.loss_dice: 1.6438  decode.d7.loss_cls: 0.4540  decode.d7.loss_mask: 1.7858  decode.d7.loss_dice: 1.6712  decode.d8.loss_cls: 0.3908  decode.d8.loss_mask: 1.7630  decode.d8.loss_dice: 1.6551  mix_decode.loss_cls: 0.1202  mix_decode.loss_mask: 0.5993  mix_decode.loss_dice: 0.6952  mix_decode.d0.loss_cls: 0.1953  mix_decode.d0.loss_mask: 0.5795  mix_decode.d0.loss_dice: 0.6865  mix_decode.d1.loss_cls: 0.1232  mix_decode.d1.loss_mask: 0.5971  mix_decode.d1.loss_dice: 0.6696  mix_decode.d2.loss_cls: 0.1401  mix_decode.d2.loss_mask: 0.5953  mix_decode.d2.loss_dice: 0.6604  mix_decode.d3.loss_cls: 0.1390  mix_decode.d3.loss_mask: 0.5976  mix_decode.d3.loss_dice: 0.6678  mix_decode.d4.loss_cls: 0.1294  mix_decode.d4.loss_mask: 0.6027  mix_decode.d4.loss_dice: 0.6878  mix_decode.d5.loss_cls: 0.1326  mix_decode.d5.loss_mask: 0.5997  mix_decode.d5.loss_dice: 0.6771  mix_decode.d6.loss_cls: 0.1452  mix_decode.d6.loss_mask: 0.5889  mix_decode.d6.loss_dice: 0.6799  mix_decode.d7.loss_cls: 0.1363  mix_decode.d7.loss_mask: 0.5986  mix_decode.d7.loss_dice: 0.6728  mix_decode.d8.loss_cls: 0.1306  mix_decode.d8.loss_mask: 0.5971  mix_decode.d8.loss_dice: 0.6770
2025/03/29 17:30:15 - mmengine - INFO - Iter(train) [17400/20000]  base_lr: 1.5943e-05 lr: 1.5943e-05  eta: 0:43:58  time: 1.1503  data_time: 0.0228  memory: 11211  loss: 56.5653  decode.loss_cls: 0.4166  decode.loss_mask: 1.8021  decode.loss_dice: 1.6738  decode.d0.loss_cls: 0.4436  decode.d0.loss_mask: 1.8724  decode.d0.loss_dice: 1.6898  decode.d1.loss_cls: 0.4102  decode.d1.loss_mask: 1.8353  decode.d1.loss_dice: 1.6502  decode.d2.loss_cls: 0.4342  decode.d2.loss_mask: 1.7914  decode.d2.loss_dice: 1.6339  decode.d3.loss_cls: 0.4028  decode.d3.loss_mask: 1.7931  decode.d3.loss_dice: 1.6155  decode.d4.loss_cls: 0.4138  decode.d4.loss_mask: 1.8587  decode.d4.loss_dice: 1.6473  decode.d5.loss_cls: 0.4408  decode.d5.loss_mask: 1.7946  decode.d5.loss_dice: 1.6581  decode.d6.loss_cls: 0.4261  decode.d6.loss_mask: 1.8087  decode.d6.loss_dice: 1.6342  decode.d7.loss_cls: 0.4648  decode.d7.loss_mask: 1.7990  decode.d7.loss_dice: 1.6507  decode.d8.loss_cls: 0.4694  decode.d8.loss_mask: 1.8089  decode.d8.loss_dice: 1.6721  mix_decode.loss_cls: 0.2565  mix_decode.loss_mask: 0.6594  mix_decode.loss_dice: 0.8030  mix_decode.d0.loss_cls: 0.2351  mix_decode.d0.loss_mask: 0.6727  mix_decode.d0.loss_dice: 0.8745  mix_decode.d1.loss_cls: 0.3016  mix_decode.d1.loss_mask: 0.7003  mix_decode.d1.loss_dice: 0.7903  mix_decode.d2.loss_cls: 0.2358  mix_decode.d2.loss_mask: 0.7031  mix_decode.d2.loss_dice: 0.8237  mix_decode.d3.loss_cls: 0.3027  mix_decode.d3.loss_mask: 0.6527  mix_decode.d3.loss_dice: 0.7642  mix_decode.d4.loss_cls: 0.2332  mix_decode.d4.loss_mask: 0.6869  mix_decode.d4.loss_dice: 0.7968  mix_decode.d5.loss_cls: 0.2574  mix_decode.d5.loss_mask: 0.6653  mix_decode.d5.loss_dice: 0.7988  mix_decode.d6.loss_cls: 0.2546  mix_decode.d6.loss_mask: 0.7142  mix_decode.d6.loss_dice: 0.8211  mix_decode.d7.loss_cls: 0.3163  mix_decode.d7.loss_mask: 0.6954  mix_decode.d7.loss_dice: 0.7902  mix_decode.d8.loss_cls: 0.2604  mix_decode.d8.loss_mask: 0.6681  mix_decode.d8.loss_dice: 0.8189
2025/03/29 17:31:12 - mmengine - INFO - Iter(train) [17450/20000]  base_lr: 1.5667e-05 lr: 1.5667e-05  eta: 0:43:08  time: 1.1470  data_time: 0.0226  memory: 11212  loss: 52.9798  decode.loss_cls: 0.3175  decode.loss_mask: 1.8781  decode.loss_dice: 1.6860  decode.d0.loss_cls: 0.3722  decode.d0.loss_mask: 1.8990  decode.d0.loss_dice: 1.7186  decode.d1.loss_cls: 0.3294  decode.d1.loss_mask: 1.9389  decode.d1.loss_dice: 1.7185  decode.d2.loss_cls: 0.2830  decode.d2.loss_mask: 1.8814  decode.d2.loss_dice: 1.6977  decode.d3.loss_cls: 0.3414  decode.d3.loss_mask: 1.8906  decode.d3.loss_dice: 1.6941  decode.d4.loss_cls: 0.2799  decode.d4.loss_mask: 1.8981  decode.d4.loss_dice: 1.7391  decode.d5.loss_cls: 0.2759  decode.d5.loss_mask: 1.8987  decode.d5.loss_dice: 1.7537  decode.d6.loss_cls: 0.2861  decode.d6.loss_mask: 1.9016  decode.d6.loss_dice: 1.7290  decode.d7.loss_cls: 0.3461  decode.d7.loss_mask: 1.8731  decode.d7.loss_dice: 1.6954  decode.d8.loss_cls: 0.3352  decode.d8.loss_mask: 1.8653  decode.d8.loss_dice: 1.6806  mix_decode.loss_cls: 0.2259  mix_decode.loss_mask: 0.5073  mix_decode.loss_dice: 0.5908  mix_decode.d0.loss_cls: 0.2859  mix_decode.d0.loss_mask: 0.5170  mix_decode.d0.loss_dice: 0.6893  mix_decode.d1.loss_cls: 0.2202  mix_decode.d1.loss_mask: 0.5219  mix_decode.d1.loss_dice: 0.6334  mix_decode.d2.loss_cls: 0.2221  mix_decode.d2.loss_mask: 0.5105  mix_decode.d2.loss_dice: 0.5919  mix_decode.d3.loss_cls: 0.2137  mix_decode.d3.loss_mask: 0.5217  mix_decode.d3.loss_dice: 0.6007  mix_decode.d4.loss_cls: 0.2682  mix_decode.d4.loss_mask: 0.5242  mix_decode.d4.loss_dice: 0.5977  mix_decode.d5.loss_cls: 0.2424  mix_decode.d5.loss_mask: 0.5250  mix_decode.d5.loss_dice: 0.6283  mix_decode.d6.loss_cls: 0.2460  mix_decode.d6.loss_mask: 0.5213  mix_decode.d6.loss_dice: 0.6152  mix_decode.d7.loss_cls: 0.2415  mix_decode.d7.loss_mask: 0.5226  mix_decode.d7.loss_dice: 0.6141  mix_decode.d8.loss_cls: 0.2300  mix_decode.d8.loss_mask: 0.5334  mix_decode.d8.loss_dice: 0.6139
2025/03/29 17:32:10 - mmengine - INFO - Iter(train) [17500/20000]  base_lr: 1.5390e-05 lr: 1.5390e-05  eta: 0:42:18  time: 1.1538  data_time: 0.0225  memory: 11210  loss: 56.3756  decode.loss_cls: 0.4045  decode.loss_mask: 1.9117  decode.loss_dice: 1.8277  decode.d0.loss_cls: 0.4452  decode.d0.loss_mask: 1.9085  decode.d0.loss_dice: 1.8863  decode.d1.loss_cls: 0.3708  decode.d1.loss_mask: 1.8883  decode.d1.loss_dice: 1.8235  decode.d2.loss_cls: 0.3577  decode.d2.loss_mask: 1.9128  decode.d2.loss_dice: 1.8090  decode.d3.loss_cls: 0.4275  decode.d3.loss_mask: 1.8769  decode.d3.loss_dice: 1.7727  decode.d4.loss_cls: 0.4206  decode.d4.loss_mask: 1.8765  decode.d4.loss_dice: 1.7914  decode.d5.loss_cls: 0.4352  decode.d5.loss_mask: 1.8648  decode.d5.loss_dice: 1.7854  decode.d6.loss_cls: 0.3994  decode.d6.loss_mask: 1.8956  decode.d6.loss_dice: 1.8319  decode.d7.loss_cls: 0.4097  decode.d7.loss_mask: 1.9295  decode.d7.loss_dice: 1.8287  decode.d8.loss_cls: 0.4066  decode.d8.loss_mask: 1.8850  decode.d8.loss_dice: 1.8292  mix_decode.loss_cls: 0.1481  mix_decode.loss_mask: 0.6163  mix_decode.loss_dice: 0.7094  mix_decode.d0.loss_cls: 0.2087  mix_decode.d0.loss_mask: 0.5964  mix_decode.d0.loss_dice: 0.7804  mix_decode.d1.loss_cls: 0.1575  mix_decode.d1.loss_mask: 0.6322  mix_decode.d1.loss_dice: 0.7236  mix_decode.d2.loss_cls: 0.1670  mix_decode.d2.loss_mask: 0.6534  mix_decode.d2.loss_dice: 0.7243  mix_decode.d3.loss_cls: 0.1890  mix_decode.d3.loss_mask: 0.6314  mix_decode.d3.loss_dice: 0.7111  mix_decode.d4.loss_cls: 0.1401  mix_decode.d4.loss_mask: 0.6153  mix_decode.d4.loss_dice: 0.7155  mix_decode.d5.loss_cls: 0.0986  mix_decode.d5.loss_mask: 0.6617  mix_decode.d5.loss_dice: 0.7384  mix_decode.d6.loss_cls: 0.1779  mix_decode.d6.loss_mask: 0.6263  mix_decode.d6.loss_dice: 0.7235  mix_decode.d7.loss_cls: 0.1711  mix_decode.d7.loss_mask: 0.6305  mix_decode.d7.loss_dice: 0.7336  mix_decode.d8.loss_cls: 0.1622  mix_decode.d8.loss_mask: 0.6096  mix_decode.d8.loss_dice: 0.7098
2025/03/29 17:33:07 - mmengine - INFO - Iter(train) [17550/20000]  base_lr: 1.5113e-05 lr: 1.5113e-05  eta: 0:41:28  time: 1.1506  data_time: 0.0227  memory: 11206  loss: 47.3930  decode.loss_cls: 0.1911  decode.loss_mask: 1.6092  decode.loss_dice: 1.5512  decode.d0.loss_cls: 0.3715  decode.d0.loss_mask: 1.6374  decode.d0.loss_dice: 1.5947  decode.d1.loss_cls: 0.2385  decode.d1.loss_mask: 1.6009  decode.d1.loss_dice: 1.5044  decode.d2.loss_cls: 0.2090  decode.d2.loss_mask: 1.6319  decode.d2.loss_dice: 1.5142  decode.d3.loss_cls: 0.2031  decode.d3.loss_mask: 1.6040  decode.d3.loss_dice: 1.5213  decode.d4.loss_cls: 0.1836  decode.d4.loss_mask: 1.5824  decode.d4.loss_dice: 1.5354  decode.d5.loss_cls: 0.2542  decode.d5.loss_mask: 1.5855  decode.d5.loss_dice: 1.5318  decode.d6.loss_cls: 0.1534  decode.d6.loss_mask: 1.6166  decode.d6.loss_dice: 1.5760  decode.d7.loss_cls: 0.2131  decode.d7.loss_mask: 1.5783  decode.d7.loss_dice: 1.5083  decode.d8.loss_cls: 0.1779  decode.d8.loss_mask: 1.5922  decode.d8.loss_dice: 1.5497  mix_decode.loss_cls: 0.1905  mix_decode.loss_mask: 0.5587  mix_decode.loss_dice: 0.6160  mix_decode.d0.loss_cls: 0.2638  mix_decode.d0.loss_mask: 0.5031  mix_decode.d0.loss_dice: 0.6796  mix_decode.d1.loss_cls: 0.2045  mix_decode.d1.loss_mask: 0.5447  mix_decode.d1.loss_dice: 0.6218  mix_decode.d2.loss_cls: 0.1963  mix_decode.d2.loss_mask: 0.5507  mix_decode.d2.loss_dice: 0.6155  mix_decode.d3.loss_cls: 0.2074  mix_decode.d3.loss_mask: 0.5572  mix_decode.d3.loss_dice: 0.6244  mix_decode.d4.loss_cls: 0.2092  mix_decode.d4.loss_mask: 0.5338  mix_decode.d4.loss_dice: 0.6023  mix_decode.d5.loss_cls: 0.2085  mix_decode.d5.loss_mask: 0.5410  mix_decode.d5.loss_dice: 0.6336  mix_decode.d6.loss_cls: 0.1938  mix_decode.d6.loss_mask: 0.5531  mix_decode.d6.loss_dice: 0.6302  mix_decode.d7.loss_cls: 0.2246  mix_decode.d7.loss_mask: 0.5168  mix_decode.d7.loss_dice: 0.6198  mix_decode.d8.loss_cls: 0.2141  mix_decode.d8.loss_mask: 0.5383  mix_decode.d8.loss_dice: 0.6189
2025/03/29 17:34:05 - mmengine - INFO - Iter(train) [17600/20000]  base_lr: 1.4835e-05 lr: 1.4835e-05  eta: 0:40:38  time: 1.1631  data_time: 0.0238  memory: 11213  loss: 55.7053  decode.loss_cls: 0.1765  decode.loss_mask: 2.3235  decode.loss_dice: 1.7426  decode.d0.loss_cls: 0.3506  decode.d0.loss_mask: 2.3453  decode.d0.loss_dice: 1.7440  decode.d1.loss_cls: 0.1962  decode.d1.loss_mask: 2.2782  decode.d1.loss_dice: 1.7037  decode.d2.loss_cls: 0.1890  decode.d2.loss_mask: 2.2946  decode.d2.loss_dice: 1.7072  decode.d3.loss_cls: 0.1740  decode.d3.loss_mask: 2.2868  decode.d3.loss_dice: 1.7474  decode.d4.loss_cls: 0.1735  decode.d4.loss_mask: 2.2841  decode.d4.loss_dice: 1.6719  decode.d5.loss_cls: 0.1948  decode.d5.loss_mask: 2.2990  decode.d5.loss_dice: 1.7132  decode.d6.loss_cls: 0.1718  decode.d6.loss_mask: 2.3256  decode.d6.loss_dice: 1.7339  decode.d7.loss_cls: 0.1925  decode.d7.loss_mask: 2.3093  decode.d7.loss_dice: 1.7080  decode.d8.loss_cls: 0.2094  decode.d8.loss_mask: 2.2633  decode.d8.loss_dice: 1.7243  mix_decode.loss_cls: 0.1286  mix_decode.loss_mask: 0.5669  mix_decode.loss_dice: 0.6237  mix_decode.d0.loss_cls: 0.1899  mix_decode.d0.loss_mask: 0.5607  mix_decode.d0.loss_dice: 0.6958  mix_decode.d1.loss_cls: 0.1925  mix_decode.d1.loss_mask: 0.5613  mix_decode.d1.loss_dice: 0.6019  mix_decode.d2.loss_cls: 0.1639  mix_decode.d2.loss_mask: 0.5532  mix_decode.d2.loss_dice: 0.6119  mix_decode.d3.loss_cls: 0.1618  mix_decode.d3.loss_mask: 0.5515  mix_decode.d3.loss_dice: 0.5918  mix_decode.d4.loss_cls: 0.1514  mix_decode.d4.loss_mask: 0.5635  mix_decode.d4.loss_dice: 0.6214  mix_decode.d5.loss_cls: 0.1671  mix_decode.d5.loss_mask: 0.5638  mix_decode.d5.loss_dice: 0.6231  mix_decode.d6.loss_cls: 0.1475  mix_decode.d6.loss_mask: 0.5704  mix_decode.d6.loss_dice: 0.6294  mix_decode.d7.loss_cls: 0.1549  mix_decode.d7.loss_mask: 0.5636  mix_decode.d7.loss_dice: 0.6251  mix_decode.d8.loss_cls: 0.1427  mix_decode.d8.loss_mask: 0.5693  mix_decode.d8.loss_dice: 0.6226
2025/03/29 17:35:03 - mmengine - INFO - Iter(train) [17650/20000]  base_lr: 1.4556e-05 lr: 1.4556e-05  eta: 0:39:49  time: 1.1577  data_time: 0.0230  memory: 11221  loss: 59.7260  decode.loss_cls: 0.3525  decode.loss_mask: 2.0106  decode.loss_dice: 1.6986  decode.d0.loss_cls: 0.4632  decode.d0.loss_mask: 2.0486  decode.d0.loss_dice: 1.7218  decode.d1.loss_cls: 0.4554  decode.d1.loss_mask: 1.9761  decode.d1.loss_dice: 1.6991  decode.d2.loss_cls: 0.4911  decode.d2.loss_mask: 1.9763  decode.d2.loss_dice: 1.6742  decode.d3.loss_cls: 0.4455  decode.d3.loss_mask: 2.0152  decode.d3.loss_dice: 1.6870  decode.d4.loss_cls: 0.4579  decode.d4.loss_mask: 2.0319  decode.d4.loss_dice: 1.6816  decode.d5.loss_cls: 0.3831  decode.d5.loss_mask: 2.0271  decode.d5.loss_dice: 1.6999  decode.d6.loss_cls: 0.4824  decode.d6.loss_mask: 2.0092  decode.d6.loss_dice: 1.6659  decode.d7.loss_cls: 0.3986  decode.d7.loss_mask: 2.0139  decode.d7.loss_dice: 1.6846  decode.d8.loss_cls: 0.3634  decode.d8.loss_mask: 2.0188  decode.d8.loss_dice: 1.7044  mix_decode.loss_cls: 0.2567  mix_decode.loss_mask: 0.6992  mix_decode.loss_dice: 0.8425  mix_decode.d0.loss_cls: 0.2142  mix_decode.d0.loss_mask: 0.7371  mix_decode.d0.loss_dice: 0.9119  mix_decode.d1.loss_cls: 0.3177  mix_decode.d1.loss_mask: 0.6999  mix_decode.d1.loss_dice: 0.8107  mix_decode.d2.loss_cls: 0.3203  mix_decode.d2.loss_mask: 0.7151  mix_decode.d2.loss_dice: 0.7967  mix_decode.d3.loss_cls: 0.2405  mix_decode.d3.loss_mask: 0.7445  mix_decode.d3.loss_dice: 0.8426  mix_decode.d4.loss_cls: 0.2259  mix_decode.d4.loss_mask: 0.7512  mix_decode.d4.loss_dice: 0.8701  mix_decode.d5.loss_cls: 0.2642  mix_decode.d5.loss_mask: 0.7319  mix_decode.d5.loss_dice: 0.8514  mix_decode.d6.loss_cls: 0.2383  mix_decode.d6.loss_mask: 0.7574  mix_decode.d6.loss_dice: 0.8790  mix_decode.d7.loss_cls: 0.2384  mix_decode.d7.loss_mask: 0.7379  mix_decode.d7.loss_dice: 0.8643  mix_decode.d8.loss_cls: 0.2198  mix_decode.d8.loss_mask: 0.7421  mix_decode.d8.loss_dice: 0.8665
2025/03/29 17:36:01 - mmengine - INFO - Iter(train) [17700/20000]  base_lr: 1.4277e-05 lr: 1.4277e-05  eta: 0:38:59  time: 1.1554  data_time: 0.0229  memory: 11210  loss: 56.3920  decode.loss_cls: 0.4009  decode.loss_mask: 2.0570  decode.loss_dice: 1.7311  decode.d0.loss_cls: 0.4841  decode.d0.loss_mask: 2.0113  decode.d0.loss_dice: 1.7535  decode.d1.loss_cls: 0.3905  decode.d1.loss_mask: 2.0481  decode.d1.loss_dice: 1.7313  decode.d2.loss_cls: 0.4036  decode.d2.loss_mask: 2.0000  decode.d2.loss_dice: 1.6912  decode.d3.loss_cls: 0.3497  decode.d3.loss_mask: 2.0421  decode.d3.loss_dice: 1.7420  decode.d4.loss_cls: 0.4214  decode.d4.loss_mask: 1.9914  decode.d4.loss_dice: 1.7249  decode.d5.loss_cls: 0.3905  decode.d5.loss_mask: 1.9976  decode.d5.loss_dice: 1.7460  decode.d6.loss_cls: 0.3980  decode.d6.loss_mask: 2.0130  decode.d6.loss_dice: 1.7404  decode.d7.loss_cls: 0.4206  decode.d7.loss_mask: 2.0028  decode.d7.loss_dice: 1.6986  decode.d8.loss_cls: 0.4079  decode.d8.loss_mask: 2.0518  decode.d8.loss_dice: 1.7232  mix_decode.loss_cls: 0.2396  mix_decode.loss_mask: 0.5757  mix_decode.loss_dice: 0.6650  mix_decode.d0.loss_cls: 0.2007  mix_decode.d0.loss_mask: 0.5728  mix_decode.d0.loss_dice: 0.7552  mix_decode.d1.loss_cls: 0.2819  mix_decode.d1.loss_mask: 0.5625  mix_decode.d1.loss_dice: 0.6672  mix_decode.d2.loss_cls: 0.2381  mix_decode.d2.loss_mask: 0.5651  mix_decode.d2.loss_dice: 0.6580  mix_decode.d3.loss_cls: 0.2195  mix_decode.d3.loss_mask: 0.5835  mix_decode.d3.loss_dice: 0.6751  mix_decode.d4.loss_cls: 0.2429  mix_decode.d4.loss_mask: 0.5567  mix_decode.d4.loss_dice: 0.6811  mix_decode.d5.loss_cls: 0.2672  mix_decode.d5.loss_mask: 0.5823  mix_decode.d5.loss_dice: 0.6702  mix_decode.d6.loss_cls: 0.2147  mix_decode.d6.loss_mask: 0.5521  mix_decode.d6.loss_dice: 0.6772  mix_decode.d7.loss_cls: 0.2260  mix_decode.d7.loss_mask: 0.5615  mix_decode.d7.loss_dice: 0.6861  mix_decode.d8.loss_cls: 0.2004  mix_decode.d8.loss_mask: 0.5635  mix_decode.d8.loss_dice: 0.6858
2025/03/29 17:36:59 - mmengine - INFO - Iter(train) [17750/20000]  base_lr: 1.3998e-05 lr: 1.3998e-05  eta: 0:38:09  time: 1.1500  data_time: 0.0225  memory: 11220  loss: 52.2998  decode.loss_cls: 0.2195  decode.loss_mask: 1.8338  decode.loss_dice: 1.6284  decode.d0.loss_cls: 0.3092  decode.d0.loss_mask: 1.8409  decode.d0.loss_dice: 1.6644  decode.d1.loss_cls: 0.2613  decode.d1.loss_mask: 1.8188  decode.d1.loss_dice: 1.6238  decode.d2.loss_cls: 0.2547  decode.d2.loss_mask: 1.8333  decode.d2.loss_dice: 1.6399  decode.d3.loss_cls: 0.2412  decode.d3.loss_mask: 1.8198  decode.d3.loss_dice: 1.6372  decode.d4.loss_cls: 0.2318  decode.d4.loss_mask: 1.8193  decode.d4.loss_dice: 1.6378  decode.d5.loss_cls: 0.2385  decode.d5.loss_mask: 1.8188  decode.d5.loss_dice: 1.6347  decode.d6.loss_cls: 0.2284  decode.d6.loss_mask: 1.8389  decode.d6.loss_dice: 1.6358  decode.d7.loss_cls: 0.2493  decode.d7.loss_mask: 1.8271  decode.d7.loss_dice: 1.6511  decode.d8.loss_cls: 0.2438  decode.d8.loss_mask: 1.8226  decode.d8.loss_dice: 1.6507  mix_decode.loss_cls: 0.1497  mix_decode.loss_mask: 0.6475  mix_decode.loss_dice: 0.7106  mix_decode.d0.loss_cls: 0.2328  mix_decode.d0.loss_mask: 0.6407  mix_decode.d0.loss_dice: 0.7620  mix_decode.d1.loss_cls: 0.2145  mix_decode.d1.loss_mask: 0.6183  mix_decode.d1.loss_dice: 0.6858  mix_decode.d2.loss_cls: 0.1807  mix_decode.d2.loss_mask: 0.6246  mix_decode.d2.loss_dice: 0.6827  mix_decode.d3.loss_cls: 0.1662  mix_decode.d3.loss_mask: 0.6179  mix_decode.d3.loss_dice: 0.6963  mix_decode.d4.loss_cls: 0.2227  mix_decode.d4.loss_mask: 0.6076  mix_decode.d4.loss_dice: 0.6870  mix_decode.d5.loss_cls: 0.1894  mix_decode.d5.loss_mask: 0.6276  mix_decode.d5.loss_dice: 0.7002  mix_decode.d6.loss_cls: 0.1959  mix_decode.d6.loss_mask: 0.6040  mix_decode.d6.loss_dice: 0.6851  mix_decode.d7.loss_cls: 0.1885  mix_decode.d7.loss_mask: 0.6208  mix_decode.d7.loss_dice: 0.6925  mix_decode.d8.loss_cls: 0.1709  mix_decode.d8.loss_mask: 0.6226  mix_decode.d8.loss_dice: 0.6995
2025/03/29 17:37:57 - mmengine - INFO - Iter(train) [17800/20000]  base_lr: 1.3717e-05 lr: 1.3717e-05  eta: 0:37:19  time: 1.1549  data_time: 0.0237  memory: 11215  loss: 58.8626  decode.loss_cls: 0.4017  decode.loss_mask: 2.1728  decode.loss_dice: 1.7725  decode.d0.loss_cls: 0.5229  decode.d0.loss_mask: 2.1964  decode.d0.loss_dice: 1.7578  decode.d1.loss_cls: 0.4238  decode.d1.loss_mask: 2.1744  decode.d1.loss_dice: 1.7369  decode.d2.loss_cls: 0.3799  decode.d2.loss_mask: 2.1721  decode.d2.loss_dice: 1.7411  decode.d3.loss_cls: 0.4277  decode.d3.loss_mask: 2.1655  decode.d3.loss_dice: 1.7047  decode.d4.loss_cls: 0.4401  decode.d4.loss_mask: 2.1901  decode.d4.loss_dice: 1.7513  decode.d5.loss_cls: 0.4303  decode.d5.loss_mask: 2.2116  decode.d5.loss_dice: 1.7333  decode.d6.loss_cls: 0.4169  decode.d6.loss_mask: 2.1734  decode.d6.loss_dice: 1.7550  decode.d7.loss_cls: 0.4745  decode.d7.loss_mask: 2.1620  decode.d7.loss_dice: 1.7635  decode.d8.loss_cls: 0.3497  decode.d8.loss_mask: 2.2502  decode.d8.loss_dice: 1.7899  mix_decode.loss_cls: 0.2821  mix_decode.loss_mask: 0.5583  mix_decode.loss_dice: 0.6158  mix_decode.d0.loss_cls: 0.2726  mix_decode.d0.loss_mask: 0.5806  mix_decode.d0.loss_dice: 0.7823  mix_decode.d1.loss_cls: 0.2170  mix_decode.d1.loss_mask: 0.5969  mix_decode.d1.loss_dice: 0.6966  mix_decode.d2.loss_cls: 0.2792  mix_decode.d2.loss_mask: 0.5585  mix_decode.d2.loss_dice: 0.6305  mix_decode.d3.loss_cls: 0.2644  mix_decode.d3.loss_mask: 0.5596  mix_decode.d3.loss_dice: 0.6429  mix_decode.d4.loss_cls: 0.2717  mix_decode.d4.loss_mask: 0.5742  mix_decode.d4.loss_dice: 0.6772  mix_decode.d5.loss_cls: 0.2701  mix_decode.d5.loss_mask: 0.5816  mix_decode.d5.loss_dice: 0.6734  mix_decode.d6.loss_cls: 0.2399  mix_decode.d6.loss_mask: 0.6042  mix_decode.d6.loss_dice: 0.6980  mix_decode.d7.loss_cls: 0.2752  mix_decode.d7.loss_mask: 0.5961  mix_decode.d7.loss_dice: 0.6940  mix_decode.d8.loss_cls: 0.2732  mix_decode.d8.loss_mask: 0.5906  mix_decode.d8.loss_dice: 0.6638
2025/03/29 17:38:54 - mmengine - INFO - Iter(train) [17850/20000]  base_lr: 1.3437e-05 lr: 1.3437e-05  eta: 0:36:29  time: 1.1536  data_time: 0.0226  memory: 11223  loss: 50.7716  decode.loss_cls: 0.2556  decode.loss_mask: 1.6995  decode.loss_dice: 1.5353  decode.d0.loss_cls: 0.3921  decode.d0.loss_mask: 1.7533  decode.d0.loss_dice: 1.5304  decode.d1.loss_cls: 0.2719  decode.d1.loss_mask: 1.6985  decode.d1.loss_dice: 1.5635  decode.d2.loss_cls: 0.2481  decode.d2.loss_mask: 1.6824  decode.d2.loss_dice: 1.5350  decode.d3.loss_cls: 0.2407  decode.d3.loss_mask: 1.6818  decode.d3.loss_dice: 1.5391  decode.d4.loss_cls: 0.2572  decode.d4.loss_mask: 1.6750  decode.d4.loss_dice: 1.5289  decode.d5.loss_cls: 0.2690  decode.d5.loss_mask: 1.6785  decode.d5.loss_dice: 1.5449  decode.d6.loss_cls: 0.2815  decode.d6.loss_mask: 1.6834  decode.d6.loss_dice: 1.5372  decode.d7.loss_cls: 0.2878  decode.d7.loss_mask: 1.6860  decode.d7.loss_dice: 1.5349  decode.d8.loss_cls: 0.2637  decode.d8.loss_mask: 1.6817  decode.d8.loss_dice: 1.5325  mix_decode.loss_cls: 0.2562  mix_decode.loss_mask: 0.5774  mix_decode.loss_dice: 0.6835  mix_decode.d0.loss_cls: 0.2985  mix_decode.d0.loss_mask: 0.6147  mix_decode.d0.loss_dice: 0.7976  mix_decode.d1.loss_cls: 0.2783  mix_decode.d1.loss_mask: 0.5920  mix_decode.d1.loss_dice: 0.6900  mix_decode.d2.loss_cls: 0.2666  mix_decode.d2.loss_mask: 0.5777  mix_decode.d2.loss_dice: 0.6664  mix_decode.d3.loss_cls: 0.2663  mix_decode.d3.loss_mask: 0.6087  mix_decode.d3.loss_dice: 0.6873  mix_decode.d4.loss_cls: 0.2715  mix_decode.d4.loss_mask: 0.5884  mix_decode.d4.loss_dice: 0.7020  mix_decode.d5.loss_cls: 0.2850  mix_decode.d5.loss_mask: 0.5946  mix_decode.d5.loss_dice: 0.7067  mix_decode.d6.loss_cls: 0.2681  mix_decode.d6.loss_mask: 0.5873  mix_decode.d6.loss_dice: 0.6947  mix_decode.d7.loss_cls: 0.2863  mix_decode.d7.loss_mask: 0.6027  mix_decode.d7.loss_dice: 0.7064  mix_decode.d8.loss_cls: 0.2862  mix_decode.d8.loss_mask: 0.5849  mix_decode.d8.loss_dice: 0.6760
2025/03/29 17:39:52 - mmengine - INFO - Iter(train) [17900/20000]  base_lr: 1.3155e-05 lr: 1.3155e-05  eta: 0:35:38  time: 1.1490  data_time: 0.0225  memory: 11213  loss: 55.6741  decode.loss_cls: 0.4061  decode.loss_mask: 1.7791  decode.loss_dice: 1.5610  decode.d0.loss_cls: 0.6871  decode.d0.loss_mask: 1.8977  decode.d0.loss_dice: 1.6799  decode.d1.loss_cls: 0.4325  decode.d1.loss_mask: 1.8498  decode.d1.loss_dice: 1.5869  decode.d2.loss_cls: 0.4565  decode.d2.loss_mask: 1.8255  decode.d2.loss_dice: 1.5750  decode.d3.loss_cls: 0.4282  decode.d3.loss_mask: 1.8099  decode.d3.loss_dice: 1.5833  decode.d4.loss_cls: 0.4289  decode.d4.loss_mask: 1.8299  decode.d4.loss_dice: 1.5588  decode.d5.loss_cls: 0.4369  decode.d5.loss_mask: 1.8181  decode.d5.loss_dice: 1.5644  decode.d6.loss_cls: 0.4924  decode.d6.loss_mask: 1.8334  decode.d6.loss_dice: 1.5647  decode.d7.loss_cls: 0.4610  decode.d7.loss_mask: 1.8177  decode.d7.loss_dice: 1.5818  decode.d8.loss_cls: 0.4400  decode.d8.loss_mask: 1.7749  decode.d8.loss_dice: 1.5709  mix_decode.loss_cls: 0.2718  mix_decode.loss_mask: 0.6379  mix_decode.loss_dice: 0.7579  mix_decode.d0.loss_cls: 0.3412  mix_decode.d0.loss_mask: 0.6221  mix_decode.d0.loss_dice: 0.8369  mix_decode.d1.loss_cls: 0.3184  mix_decode.d1.loss_mask: 0.5878  mix_decode.d1.loss_dice: 0.7271  mix_decode.d2.loss_cls: 0.3109  mix_decode.d2.loss_mask: 0.5971  mix_decode.d2.loss_dice: 0.7311  mix_decode.d3.loss_cls: 0.3207  mix_decode.d3.loss_mask: 0.6085  mix_decode.d3.loss_dice: 0.7445  mix_decode.d4.loss_cls: 0.2592  mix_decode.d4.loss_mask: 0.6458  mix_decode.d4.loss_dice: 0.7823  mix_decode.d5.loss_cls: 0.3477  mix_decode.d5.loss_mask: 0.6133  mix_decode.d5.loss_dice: 0.7798  mix_decode.d6.loss_cls: 0.2980  mix_decode.d6.loss_mask: 0.6506  mix_decode.d6.loss_dice: 0.7778  mix_decode.d7.loss_cls: 0.3249  mix_decode.d7.loss_mask: 0.6143  mix_decode.d7.loss_dice: 0.7516  mix_decode.d8.loss_cls: 0.3257  mix_decode.d8.loss_mask: 0.6109  mix_decode.d8.loss_dice: 0.7457
2025/03/29 17:40:50 - mmengine - INFO - Iter(train) [17950/20000]  base_lr: 1.2873e-05 lr: 1.2873e-05  eta: 0:34:48  time: 1.1518  data_time: 0.0226  memory: 11217  loss: 51.9242  decode.loss_cls: 0.3150  decode.loss_mask: 1.6513  decode.loss_dice: 1.5448  decode.d0.loss_cls: 0.5302  decode.d0.loss_mask: 1.6700  decode.d0.loss_dice: 1.6096  decode.d1.loss_cls: 0.3257  decode.d1.loss_mask: 1.6616  decode.d1.loss_dice: 1.5956  decode.d2.loss_cls: 0.4639  decode.d2.loss_mask: 1.6405  decode.d2.loss_dice: 1.4947  decode.d3.loss_cls: 0.4102  decode.d3.loss_mask: 1.6433  decode.d3.loss_dice: 1.5249  decode.d4.loss_cls: 0.4086  decode.d4.loss_mask: 1.6117  decode.d4.loss_dice: 1.5594  decode.d5.loss_cls: 0.3388  decode.d5.loss_mask: 1.6378  decode.d5.loss_dice: 1.5462  decode.d6.loss_cls: 0.3862  decode.d6.loss_mask: 1.6243  decode.d6.loss_dice: 1.5544  decode.d7.loss_cls: 0.3462  decode.d7.loss_mask: 1.6623  decode.d7.loss_dice: 1.5428  decode.d8.loss_cls: 0.3693  decode.d8.loss_mask: 1.6737  decode.d8.loss_dice: 1.5989  mix_decode.loss_cls: 0.1995  mix_decode.loss_mask: 0.6494  mix_decode.loss_dice: 0.7057  mix_decode.d0.loss_cls: 0.3198  mix_decode.d0.loss_mask: 0.6491  mix_decode.d0.loss_dice: 0.7905  mix_decode.d1.loss_cls: 0.2002  mix_decode.d1.loss_mask: 0.6569  mix_decode.d1.loss_dice: 0.7409  mix_decode.d2.loss_cls: 0.2165  mix_decode.d2.loss_mask: 0.6505  mix_decode.d2.loss_dice: 0.7071  mix_decode.d3.loss_cls: 0.1750  mix_decode.d3.loss_mask: 0.6720  mix_decode.d3.loss_dice: 0.7212  mix_decode.d4.loss_cls: 0.1990  mix_decode.d4.loss_mask: 0.6562  mix_decode.d4.loss_dice: 0.7148  mix_decode.d5.loss_cls: 0.2044  mix_decode.d5.loss_mask: 0.6786  mix_decode.d5.loss_dice: 0.7225  mix_decode.d6.loss_cls: 0.2176  mix_decode.d6.loss_mask: 0.6214  mix_decode.d6.loss_dice: 0.7212  mix_decode.d7.loss_cls: 0.2489  mix_decode.d7.loss_mask: 0.6528  mix_decode.d7.loss_dice: 0.7225  mix_decode.d8.loss_cls: 0.1966  mix_decode.d8.loss_mask: 0.6459  mix_decode.d8.loss_dice: 0.7256
2025/03/29 17:41:47 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 17:41:47 - mmengine - INFO - Iter(train) [18000/20000]  base_lr: 1.2590e-05 lr: 1.2590e-05  eta: 0:33:58  time: 1.1530  data_time: 0.0227  memory: 11211  loss: 53.9999  decode.loss_cls: 0.3297  decode.loss_mask: 1.8531  decode.loss_dice: 1.5241  decode.d0.loss_cls: 0.5776  decode.d0.loss_mask: 1.8294  decode.d0.loss_dice: 1.5665  decode.d1.loss_cls: 0.3908  decode.d1.loss_mask: 1.8536  decode.d1.loss_dice: 1.5425  decode.d2.loss_cls: 0.2478  decode.d2.loss_mask: 1.9139  decode.d2.loss_dice: 1.5637  decode.d3.loss_cls: 0.3507  decode.d3.loss_mask: 1.8630  decode.d3.loss_dice: 1.5295  decode.d4.loss_cls: 0.3135  decode.d4.loss_mask: 1.8631  decode.d4.loss_dice: 1.5576  decode.d5.loss_cls: 0.3762  decode.d5.loss_mask: 1.8545  decode.d5.loss_dice: 1.5967  decode.d6.loss_cls: 0.4087  decode.d6.loss_mask: 1.8674  decode.d6.loss_dice: 1.5743  decode.d7.loss_cls: 0.3626  decode.d7.loss_mask: 1.8375  decode.d7.loss_dice: 1.6020  decode.d8.loss_cls: 0.3503  decode.d8.loss_mask: 1.8319  decode.d8.loss_dice: 1.5671  mix_decode.loss_cls: 0.2959  mix_decode.loss_mask: 0.5942  mix_decode.loss_dice: 0.6885  mix_decode.d0.loss_cls: 0.3413  mix_decode.d0.loss_mask: 0.5858  mix_decode.d0.loss_dice: 0.7851  mix_decode.d1.loss_cls: 0.3328  mix_decode.d1.loss_mask: 0.5253  mix_decode.d1.loss_dice: 0.6903  mix_decode.d2.loss_cls: 0.2648  mix_decode.d2.loss_mask: 0.5625  mix_decode.d2.loss_dice: 0.6999  mix_decode.d3.loss_cls: 0.2856  mix_decode.d3.loss_mask: 0.5836  mix_decode.d3.loss_dice: 0.7160  mix_decode.d4.loss_cls: 0.3230  mix_decode.d4.loss_mask: 0.5930  mix_decode.d4.loss_dice: 0.7173  mix_decode.d5.loss_cls: 0.3375  mix_decode.d5.loss_mask: 0.5955  mix_decode.d5.loss_dice: 0.7124  mix_decode.d6.loss_cls: 0.2983  mix_decode.d6.loss_mask: 0.6104  mix_decode.d6.loss_dice: 0.7379  mix_decode.d7.loss_cls: 0.3088  mix_decode.d7.loss_mask: 0.6000  mix_decode.d7.loss_dice: 0.6899  mix_decode.d8.loss_cls: 0.3539  mix_decode.d8.loss_mask: 0.5688  mix_decode.d8.loss_dice: 0.7025
2025/03/29 17:41:47 - mmengine - INFO - Saving checkpoint at 18000 iterations
2025/03/29 17:41:53 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:06:00  time: 0.0923  data_time: 0.0019  memory: 3081  
2025/03/29 17:41:58 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:54  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:02 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:49  time: 0.0909  data_time: 0.0017  memory: 3081  
2025/03/29 17:42:07 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:44  time: 0.0911  data_time: 0.0017  memory: 3081  
2025/03/29 17:42:11 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:39  time: 0.0911  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:16 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:34  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:20 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:30  time: 0.0911  data_time: 0.0017  memory: 3081  
2025/03/29 17:42:25 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:25  time: 0.0913  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:30 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:21  time: 0.0914  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:34 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:16  time: 0.0913  data_time: 0.0017  memory: 3081  
2025/03/29 17:42:39 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:12  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 17:42:43 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:07  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:48 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:02  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 17:42:53 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:58  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:42:57 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:54  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:02 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:49  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:06 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:45  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:11 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:40  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 17:43:15 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:35  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:43:20 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:31  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 17:43:25 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:26  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 17:43:29 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:22  time: 0.0912  data_time: 0.0017  memory: 3081  
2025/03/29 17:43:34 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:17  time: 0.0914  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:38 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:13  time: 0.0914  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:43 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:08  time: 0.0921  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:48 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:03  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:43:52 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:59  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:43:57 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:54  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:01 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:06 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:45  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:10 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:15 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:36  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:44:20 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:31  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:44:24 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:27  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:29 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:22  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:44:33 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0914  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:38 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:13  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:43 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:44:47 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:04  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:52 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:44:56 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:45:01 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:50  time: 0.0916  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:05 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 17:45:10 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:41  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:45:15 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0935  data_time: 0.0020  memory: 3081  
2025/03/29 17:45:19 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:24 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:29 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0912  data_time: 0.0016  memory: 3081  
2025/03/29 17:45:33 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:18  time: 0.0917  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:38 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0913  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:42 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:47 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0916  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:52 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0916  data_time: 0.0017  memory: 3081  
2025/03/29 17:45:56 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:01 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0917  data_time: 0.0017  memory: 3081  
2025/03/29 17:46:05 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:46  time: 0.0915  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:10 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:46:14 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0913  data_time: 0.0017  memory: 3081  
2025/03/29 17:46:19 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:24 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:28 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0965  data_time: 0.0021  memory: 3081  
2025/03/29 17:46:33 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0921  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:38 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:14  time: 0.0920  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:42 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0919  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:47 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0920  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:51 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 17:46:56 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:47:01 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:47:05 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0916  data_time: 0.0018  memory: 3081  
2025/03/29 17:47:10 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0917  data_time: 0.0018  memory: 3081  
2025/03/29 17:47:14 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0956  data_time: 0.0020  memory: 3081  
2025/03/29 17:47:19 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0921  data_time: 0.0019  memory: 3081  
2025/03/29 17:47:24 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0920  data_time: 0.0019  memory: 3081  
2025/03/29 17:47:28 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0919  data_time: 0.0018  memory: 3081  
2025/03/29 17:47:33 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:19  time: 0.0919  data_time: 0.0017  memory: 3081  
2025/03/29 17:47:38 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0915  data_time: 0.0017  memory: 3081  
2025/03/29 17:47:42 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0914  data_time: 0.0017  memory: 3081  
2025/03/29 17:47:47 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0912  data_time: 0.0018  memory: 3081  
2025/03/29 17:47:51 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0916  data_time: 0.0017  memory: 3081  
2025/03/29 17:47:53 - mmengine - INFO - per class results:
2025/03/29 17:47:53 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 57.47 | 81.79 |
|   building   | 59.98 | 67.42 |
|     road     | 47.52 | 53.73 |
|    water     | 66.99 |  78.5 |
|    barren    | 17.33 | 27.07 |
|    forest    | 25.55 | 27.85 |
| agricultural | 61.53 | 70.65 |
+--------------+-------+-------+
2025/03/29 17:47:53 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 72.0400  mIoU: 48.0500  mAcc: 58.1400  data_time: 0.0018  time: 0.0917
2025/03/29 17:48:51 - mmengine - INFO - Iter(train) [18050/20000]  base_lr: 1.2306e-05 lr: 1.2306e-05  eta: 0:33:08  time: 1.1522  data_time: 0.0228  memory: 11222  loss: 51.5453  decode.loss_cls: 0.2758  decode.loss_mask: 1.6467  decode.loss_dice: 1.6503  decode.d0.loss_cls: 0.4637  decode.d0.loss_mask: 1.7125  decode.d0.loss_dice: 1.6456  decode.d1.loss_cls: 0.5000  decode.d1.loss_mask: 1.6511  decode.d1.loss_dice: 1.6301  decode.d2.loss_cls: 0.4028  decode.d2.loss_mask: 1.6453  decode.d2.loss_dice: 1.5906  decode.d3.loss_cls: 0.3608  decode.d3.loss_mask: 1.6513  decode.d3.loss_dice: 1.6288  decode.d4.loss_cls: 0.3277  decode.d4.loss_mask: 1.6288  decode.d4.loss_dice: 1.6833  decode.d5.loss_cls: 0.4020  decode.d5.loss_mask: 1.6176  decode.d5.loss_dice: 1.6630  decode.d6.loss_cls: 0.3245  decode.d6.loss_mask: 1.6398  decode.d6.loss_dice: 1.6371  decode.d7.loss_cls: 0.3504  decode.d7.loss_mask: 1.6685  decode.d7.loss_dice: 1.6243  decode.d8.loss_cls: 0.3582  decode.d8.loss_mask: 1.6704  decode.d8.loss_dice: 1.6364  mix_decode.loss_cls: 0.2091  mix_decode.loss_mask: 0.5496  mix_decode.loss_dice: 0.7010  mix_decode.d0.loss_cls: 0.2284  mix_decode.d0.loss_mask: 0.5650  mix_decode.d0.loss_dice: 0.7352  mix_decode.d1.loss_cls: 0.1809  mix_decode.d1.loss_mask: 0.5885  mix_decode.d1.loss_dice: 0.6943  mix_decode.d2.loss_cls: 0.2366  mix_decode.d2.loss_mask: 0.5428  mix_decode.d2.loss_dice: 0.6905  mix_decode.d3.loss_cls: 0.2417  mix_decode.d3.loss_mask: 0.5674  mix_decode.d3.loss_dice: 0.6784  mix_decode.d4.loss_cls: 0.1854  mix_decode.d4.loss_mask: 0.6093  mix_decode.d4.loss_dice: 0.7208  mix_decode.d5.loss_cls: 0.1833  mix_decode.d5.loss_mask: 0.5661  mix_decode.d5.loss_dice: 0.7081  mix_decode.d6.loss_cls: 0.1954  mix_decode.d6.loss_mask: 0.5715  mix_decode.d6.loss_dice: 0.7071  mix_decode.d7.loss_cls: 0.1620  mix_decode.d7.loss_mask: 0.6165  mix_decode.d7.loss_dice: 0.7161  mix_decode.d8.loss_cls: 0.1686  mix_decode.d8.loss_mask: 0.6156  mix_decode.d8.loss_dice: 0.7229
2025/03/29 17:49:48 - mmengine - INFO - Iter(train) [18100/20000]  base_lr: 1.2022e-05 lr: 1.2022e-05  eta: 0:32:18  time: 1.1540  data_time: 0.0226  memory: 11230  loss: 55.5128  decode.loss_cls: 0.3558  decode.loss_mask: 1.9390  decode.loss_dice: 1.7120  decode.d0.loss_cls: 0.5319  decode.d0.loss_mask: 2.0491  decode.d0.loss_dice: 1.7276  decode.d1.loss_cls: 0.4618  decode.d1.loss_mask: 1.8737  decode.d1.loss_dice: 1.6099  decode.d2.loss_cls: 0.3606  decode.d2.loss_mask: 1.9383  decode.d2.loss_dice: 1.6785  decode.d3.loss_cls: 0.4002  decode.d3.loss_mask: 1.9420  decode.d3.loss_dice: 1.7149  decode.d4.loss_cls: 0.3505  decode.d4.loss_mask: 1.9108  decode.d4.loss_dice: 1.6811  decode.d5.loss_cls: 0.3539  decode.d5.loss_mask: 2.0354  decode.d5.loss_dice: 1.7073  decode.d6.loss_cls: 0.3469  decode.d6.loss_mask: 1.9908  decode.d6.loss_dice: 1.7198  decode.d7.loss_cls: 0.3502  decode.d7.loss_mask: 1.9362  decode.d7.loss_dice: 1.7138  decode.d8.loss_cls: 0.3763  decode.d8.loss_mask: 1.9998  decode.d8.loss_dice: 1.7022  mix_decode.loss_cls: 0.2446  mix_decode.loss_mask: 0.5600  mix_decode.loss_dice: 0.6617  mix_decode.d0.loss_cls: 0.3231  mix_decode.d0.loss_mask: 0.5574  mix_decode.d0.loss_dice: 0.7482  mix_decode.d1.loss_cls: 0.2312  mix_decode.d1.loss_mask: 0.5622  mix_decode.d1.loss_dice: 0.6719  mix_decode.d2.loss_cls: 0.2150  mix_decode.d2.loss_mask: 0.5592  mix_decode.d2.loss_dice: 0.6927  mix_decode.d3.loss_cls: 0.2338  mix_decode.d3.loss_mask: 0.5756  mix_decode.d3.loss_dice: 0.6857  mix_decode.d4.loss_cls: 0.2079  mix_decode.d4.loss_mask: 0.5734  mix_decode.d4.loss_dice: 0.7124  mix_decode.d5.loss_cls: 0.2538  mix_decode.d5.loss_mask: 0.5921  mix_decode.d5.loss_dice: 0.6939  mix_decode.d6.loss_cls: 0.2431  mix_decode.d6.loss_mask: 0.5844  mix_decode.d6.loss_dice: 0.6960  mix_decode.d7.loss_cls: 0.2423  mix_decode.d7.loss_mask: 0.5543  mix_decode.d7.loss_dice: 0.6885  mix_decode.d8.loss_cls: 0.2441  mix_decode.d8.loss_mask: 0.5542  mix_decode.d8.loss_dice: 0.6797
2025/03/29 17:50:46 - mmengine - INFO - Iter(train) [18150/20000]  base_lr: 1.1737e-05 lr: 1.1737e-05  eta: 0:31:27  time: 1.1515  data_time: 0.0227  memory: 11233  loss: 51.7389  decode.loss_cls: 0.3844  decode.loss_mask: 1.7265  decode.loss_dice: 1.4782  decode.d0.loss_cls: 0.5320  decode.d0.loss_mask: 1.7767  decode.d0.loss_dice: 1.4862  decode.d1.loss_cls: 0.4027  decode.d1.loss_mask: 1.7045  decode.d1.loss_dice: 1.4819  decode.d2.loss_cls: 0.3720  decode.d2.loss_mask: 1.7087  decode.d2.loss_dice: 1.4885  decode.d3.loss_cls: 0.3795  decode.d3.loss_mask: 1.6962  decode.d3.loss_dice: 1.4351  decode.d4.loss_cls: 0.3078  decode.d4.loss_mask: 1.7373  decode.d4.loss_dice: 1.4371  decode.d5.loss_cls: 0.3661  decode.d5.loss_mask: 1.7150  decode.d5.loss_dice: 1.4583  decode.d6.loss_cls: 0.3030  decode.d6.loss_mask: 1.7166  decode.d6.loss_dice: 1.4156  decode.d7.loss_cls: 0.3440  decode.d7.loss_mask: 1.6797  decode.d7.loss_dice: 1.4154  decode.d8.loss_cls: 0.3776  decode.d8.loss_mask: 1.7297  decode.d8.loss_dice: 1.4741  mix_decode.loss_cls: 0.2535  mix_decode.loss_mask: 0.6456  mix_decode.loss_dice: 0.6853  mix_decode.d0.loss_cls: 0.3426  mix_decode.d0.loss_mask: 0.6286  mix_decode.d0.loss_dice: 0.7253  mix_decode.d1.loss_cls: 0.2233  mix_decode.d1.loss_mask: 0.6495  mix_decode.d1.loss_dice: 0.6908  mix_decode.d2.loss_cls: 0.3117  mix_decode.d2.loss_mask: 0.6134  mix_decode.d2.loss_dice: 0.6533  mix_decode.d3.loss_cls: 0.2622  mix_decode.d3.loss_mask: 0.6525  mix_decode.d3.loss_dice: 0.6771  mix_decode.d4.loss_cls: 0.2768  mix_decode.d4.loss_mask: 0.6516  mix_decode.d4.loss_dice: 0.7167  mix_decode.d5.loss_cls: 0.2841  mix_decode.d5.loss_mask: 0.6619  mix_decode.d5.loss_dice: 0.7048  mix_decode.d6.loss_cls: 0.2623  mix_decode.d6.loss_mask: 0.6642  mix_decode.d6.loss_dice: 0.6917  mix_decode.d7.loss_cls: 0.2810  mix_decode.d7.loss_mask: 0.6757  mix_decode.d7.loss_dice: 0.7049  mix_decode.d8.loss_cls: 0.2456  mix_decode.d8.loss_mask: 0.6686  mix_decode.d8.loss_dice: 0.7039
2025/03/29 17:51:44 - mmengine - INFO - Iter(train) [18200/20000]  base_lr: 1.1451e-05 lr: 1.1451e-05  eta: 0:30:37  time: 1.1612  data_time: 0.0237  memory: 11208  loss: 55.9943  decode.loss_cls: 0.4406  decode.loss_mask: 1.9468  decode.loss_dice: 1.6888  decode.d0.loss_cls: 0.5460  decode.d0.loss_mask: 1.9606  decode.d0.loss_dice: 1.6831  decode.d1.loss_cls: 0.5014  decode.d1.loss_mask: 1.9700  decode.d1.loss_dice: 1.6746  decode.d2.loss_cls: 0.4888  decode.d2.loss_mask: 1.9772  decode.d2.loss_dice: 1.6723  decode.d3.loss_cls: 0.4812  decode.d3.loss_mask: 1.9689  decode.d3.loss_dice: 1.6892  decode.d4.loss_cls: 0.5277  decode.d4.loss_mask: 2.0039  decode.d4.loss_dice: 1.7367  decode.d5.loss_cls: 0.4927  decode.d5.loss_mask: 1.9990  decode.d5.loss_dice: 1.7433  decode.d6.loss_cls: 0.4608  decode.d6.loss_mask: 1.9932  decode.d6.loss_dice: 1.6687  decode.d7.loss_cls: 0.5300  decode.d7.loss_mask: 1.9922  decode.d7.loss_dice: 1.6907  decode.d8.loss_cls: 0.4770  decode.d8.loss_mask: 1.9599  decode.d8.loss_dice: 1.6772  mix_decode.loss_cls: 0.1584  mix_decode.loss_mask: 0.5955  mix_decode.loss_dice: 0.6329  mix_decode.d0.loss_cls: 0.2050  mix_decode.d0.loss_mask: 0.5774  mix_decode.d0.loss_dice: 0.7090  mix_decode.d1.loss_cls: 0.2291  mix_decode.d1.loss_mask: 0.5766  mix_decode.d1.loss_dice: 0.6330  mix_decode.d2.loss_cls: 0.2118  mix_decode.d2.loss_mask: 0.5907  mix_decode.d2.loss_dice: 0.6438  mix_decode.d3.loss_cls: 0.1762  mix_decode.d3.loss_mask: 0.6066  mix_decode.d3.loss_dice: 0.6325  mix_decode.d4.loss_cls: 0.1853  mix_decode.d4.loss_mask: 0.5753  mix_decode.d4.loss_dice: 0.6436  mix_decode.d5.loss_cls: 0.2176  mix_decode.d5.loss_mask: 0.5863  mix_decode.d5.loss_dice: 0.6470  mix_decode.d6.loss_cls: 0.2091  mix_decode.d6.loss_mask: 0.6005  mix_decode.d6.loss_dice: 0.6500  mix_decode.d7.loss_cls: 0.2154  mix_decode.d7.loss_mask: 0.5904  mix_decode.d7.loss_dice: 0.6277  mix_decode.d8.loss_cls: 0.2071  mix_decode.d8.loss_mask: 0.5805  mix_decode.d8.loss_dice: 0.6372
2025/03/29 17:52:41 - mmengine - INFO - Iter(train) [18250/20000]  base_lr: 1.1164e-05 lr: 1.1164e-05  eta: 0:29:46  time: 1.1535  data_time: 0.0231  memory: 11208  loss: 55.7069  decode.loss_cls: 0.3908  decode.loss_mask: 1.8860  decode.loss_dice: 1.6630  decode.d0.loss_cls: 0.4622  decode.d0.loss_mask: 1.8954  decode.d0.loss_dice: 1.6505  decode.d1.loss_cls: 0.3946  decode.d1.loss_mask: 1.8644  decode.d1.loss_dice: 1.6631  decode.d2.loss_cls: 0.3900  decode.d2.loss_mask: 1.8628  decode.d2.loss_dice: 1.6736  decode.d3.loss_cls: 0.3650  decode.d3.loss_mask: 1.8701  decode.d3.loss_dice: 1.6730  decode.d4.loss_cls: 0.3273  decode.d4.loss_mask: 1.8532  decode.d4.loss_dice: 1.6886  decode.d5.loss_cls: 0.3862  decode.d5.loss_mask: 1.8511  decode.d5.loss_dice: 1.6828  decode.d6.loss_cls: 0.3385  decode.d6.loss_mask: 1.8646  decode.d6.loss_dice: 1.6734  decode.d7.loss_cls: 0.3587  decode.d7.loss_mask: 1.8993  decode.d7.loss_dice: 1.6900  decode.d8.loss_cls: 0.3730  decode.d8.loss_mask: 1.9010  decode.d8.loss_dice: 1.7369  mix_decode.loss_cls: 0.2569  mix_decode.loss_mask: 0.6935  mix_decode.loss_dice: 0.6960  mix_decode.d0.loss_cls: 0.3031  mix_decode.d0.loss_mask: 0.6313  mix_decode.d0.loss_dice: 0.7436  mix_decode.d1.loss_cls: 0.3054  mix_decode.d1.loss_mask: 0.6510  mix_decode.d1.loss_dice: 0.6872  mix_decode.d2.loss_cls: 0.2977  mix_decode.d2.loss_mask: 0.6532  mix_decode.d2.loss_dice: 0.6910  mix_decode.d3.loss_cls: 0.2771  mix_decode.d3.loss_mask: 0.6628  mix_decode.d3.loss_dice: 0.7025  mix_decode.d4.loss_cls: 0.2830  mix_decode.d4.loss_mask: 0.6363  mix_decode.d4.loss_dice: 0.7191  mix_decode.d5.loss_cls: 0.2922  mix_decode.d5.loss_mask: 0.6225  mix_decode.d5.loss_dice: 0.6796  mix_decode.d6.loss_cls: 0.2680  mix_decode.d6.loss_mask: 0.6526  mix_decode.d6.loss_dice: 0.7115  mix_decode.d7.loss_cls: 0.3004  mix_decode.d7.loss_mask: 0.6478  mix_decode.d7.loss_dice: 0.6829  mix_decode.d8.loss_cls: 0.2990  mix_decode.d8.loss_mask: 0.6406  mix_decode.d8.loss_dice: 0.6900
2025/03/29 17:53:39 - mmengine - INFO - Iter(train) [18300/20000]  base_lr: 1.0877e-05 lr: 1.0877e-05  eta: 0:28:56  time: 1.1607  data_time: 0.0227  memory: 11211  loss: 57.2062  decode.loss_cls: 0.4088  decode.loss_mask: 1.9220  decode.loss_dice: 1.6776  decode.d0.loss_cls: 0.6057  decode.d0.loss_mask: 1.9438  decode.d0.loss_dice: 1.6960  decode.d1.loss_cls: 0.3905  decode.d1.loss_mask: 1.9525  decode.d1.loss_dice: 1.6965  decode.d2.loss_cls: 0.4045  decode.d2.loss_mask: 1.8987  decode.d2.loss_dice: 1.6612  decode.d3.loss_cls: 0.3573  decode.d3.loss_mask: 1.9414  decode.d3.loss_dice: 1.6660  decode.d4.loss_cls: 0.4049  decode.d4.loss_mask: 1.9549  decode.d4.loss_dice: 1.6915  decode.d5.loss_cls: 0.3957  decode.d5.loss_mask: 1.9281  decode.d5.loss_dice: 1.7291  decode.d6.loss_cls: 0.3740  decode.d6.loss_mask: 1.9612  decode.d6.loss_dice: 1.7229  decode.d7.loss_cls: 0.3911  decode.d7.loss_mask: 1.9120  decode.d7.loss_dice: 1.6774  decode.d8.loss_cls: 0.4077  decode.d8.loss_mask: 1.9845  decode.d8.loss_dice: 1.7150  mix_decode.loss_cls: 0.2011  mix_decode.loss_mask: 0.6576  mix_decode.loss_dice: 0.7416  mix_decode.d0.loss_cls: 0.3012  mix_decode.d0.loss_mask: 0.6609  mix_decode.d0.loss_dice: 0.8285  mix_decode.d1.loss_cls: 0.2496  mix_decode.d1.loss_mask: 0.6396  mix_decode.d1.loss_dice: 0.7517  mix_decode.d2.loss_cls: 0.2562  mix_decode.d2.loss_mask: 0.6499  mix_decode.d2.loss_dice: 0.7186  mix_decode.d3.loss_cls: 0.2487  mix_decode.d3.loss_mask: 0.6861  mix_decode.d3.loss_dice: 0.7134  mix_decode.d4.loss_cls: 0.2277  mix_decode.d4.loss_mask: 0.7453  mix_decode.d4.loss_dice: 0.7544  mix_decode.d5.loss_cls: 0.2798  mix_decode.d5.loss_mask: 0.6623  mix_decode.d5.loss_dice: 0.7474  mix_decode.d6.loss_cls: 0.2658  mix_decode.d6.loss_mask: 0.6519  mix_decode.d6.loss_dice: 0.7366  mix_decode.d7.loss_cls: 0.2494  mix_decode.d7.loss_mask: 0.6828  mix_decode.d7.loss_dice: 0.7626  mix_decode.d8.loss_cls: 0.2479  mix_decode.d8.loss_mask: 0.6493  mix_decode.d8.loss_dice: 0.7658
2025/03/29 17:54:37 - mmengine - INFO - Iter(train) [18350/20000]  base_lr: 1.0588e-05 lr: 1.0588e-05  eta: 0:28:06  time: 1.1538  data_time: 0.0226  memory: 11209  loss: 49.3995  decode.loss_cls: 0.1948  decode.loss_mask: 1.7395  decode.loss_dice: 1.4941  decode.d0.loss_cls: 0.3249  decode.d0.loss_mask: 1.7933  decode.d0.loss_dice: 1.5159  decode.d1.loss_cls: 0.2269  decode.d1.loss_mask: 1.7140  decode.d1.loss_dice: 1.4622  decode.d2.loss_cls: 0.2223  decode.d2.loss_mask: 1.7613  decode.d2.loss_dice: 1.4873  decode.d3.loss_cls: 0.1537  decode.d3.loss_mask: 1.7795  decode.d3.loss_dice: 1.5038  decode.d4.loss_cls: 0.2072  decode.d4.loss_mask: 1.7261  decode.d4.loss_dice: 1.5129  decode.d5.loss_cls: 0.2104  decode.d5.loss_mask: 1.7129  decode.d5.loss_dice: 1.4991  decode.d6.loss_cls: 0.1975  decode.d6.loss_mask: 1.7373  decode.d6.loss_dice: 1.4965  decode.d7.loss_cls: 0.2240  decode.d7.loss_mask: 1.7229  decode.d7.loss_dice: 1.4944  decode.d8.loss_cls: 0.1901  decode.d8.loss_mask: 1.7493  decode.d8.loss_dice: 1.5140  mix_decode.loss_cls: 0.2731  mix_decode.loss_mask: 0.5532  mix_decode.loss_dice: 0.6400  mix_decode.d0.loss_cls: 0.2190  mix_decode.d0.loss_mask: 0.5644  mix_decode.d0.loss_dice: 0.7564  mix_decode.d1.loss_cls: 0.2723  mix_decode.d1.loss_mask: 0.5498  mix_decode.d1.loss_dice: 0.6389  mix_decode.d2.loss_cls: 0.2493  mix_decode.d2.loss_mask: 0.5690  mix_decode.d2.loss_dice: 0.6114  mix_decode.d3.loss_cls: 0.2285  mix_decode.d3.loss_mask: 0.5818  mix_decode.d3.loss_dice: 0.6509  mix_decode.d4.loss_cls: 0.2824  mix_decode.d4.loss_mask: 0.5740  mix_decode.d4.loss_dice: 0.6671  mix_decode.d5.loss_cls: 0.2563  mix_decode.d5.loss_mask: 0.5643  mix_decode.d5.loss_dice: 0.6588  mix_decode.d6.loss_cls: 0.2447  mix_decode.d6.loss_mask: 0.5638  mix_decode.d6.loss_dice: 0.6624  mix_decode.d7.loss_cls: 0.2616  mix_decode.d7.loss_mask: 0.5671  mix_decode.d7.loss_dice: 0.6811  mix_decode.d8.loss_cls: 0.2699  mix_decode.d8.loss_mask: 0.5732  mix_decode.d8.loss_dice: 0.6468
2025/03/29 17:55:35 - mmengine - INFO - Iter(train) [18400/20000]  base_lr: 1.0299e-05 lr: 1.0299e-05  eta: 0:27:15  time: 1.1512  data_time: 0.0223  memory: 11202  loss: 50.7289  decode.loss_cls: 0.3469  decode.loss_mask: 1.6095  decode.loss_dice: 1.6386  decode.d0.loss_cls: 0.3908  decode.d0.loss_mask: 1.6382  decode.d0.loss_dice: 1.6789  decode.d1.loss_cls: 0.3060  decode.d1.loss_mask: 1.6005  decode.d1.loss_dice: 1.6591  decode.d2.loss_cls: 0.3442  decode.d2.loss_mask: 1.5971  decode.d2.loss_dice: 1.6532  decode.d3.loss_cls: 0.3417  decode.d3.loss_mask: 1.6078  decode.d3.loss_dice: 1.6615  decode.d4.loss_cls: 0.3144  decode.d4.loss_mask: 1.6046  decode.d4.loss_dice: 1.6716  decode.d5.loss_cls: 0.3652  decode.d5.loss_mask: 1.6244  decode.d5.loss_dice: 1.6841  decode.d6.loss_cls: 0.3330  decode.d6.loss_mask: 1.5996  decode.d6.loss_dice: 1.6766  decode.d7.loss_cls: 0.3244  decode.d7.loss_mask: 1.5900  decode.d7.loss_dice: 1.6521  decode.d8.loss_cls: 0.3358  decode.d8.loss_mask: 1.6151  decode.d8.loss_dice: 1.6431  mix_decode.loss_cls: 0.1801  mix_decode.loss_mask: 0.5134  mix_decode.loss_dice: 0.7618  mix_decode.d0.loss_cls: 0.2198  mix_decode.d0.loss_mask: 0.5141  mix_decode.d0.loss_dice: 0.8172  mix_decode.d1.loss_cls: 0.1840  mix_decode.d1.loss_mask: 0.5059  mix_decode.d1.loss_dice: 0.7584  mix_decode.d2.loss_cls: 0.1720  mix_decode.d2.loss_mask: 0.5131  mix_decode.d2.loss_dice: 0.7493  mix_decode.d3.loss_cls: 0.1603  mix_decode.d3.loss_mask: 0.5148  mix_decode.d3.loss_dice: 0.7442  mix_decode.d4.loss_cls: 0.1743  mix_decode.d4.loss_mask: 0.5196  mix_decode.d4.loss_dice: 0.7626  mix_decode.d5.loss_cls: 0.1810  mix_decode.d5.loss_mask: 0.5092  mix_decode.d5.loss_dice: 0.7692  mix_decode.d6.loss_cls: 0.2034  mix_decode.d6.loss_mask: 0.5185  mix_decode.d6.loss_dice: 0.7508  mix_decode.d7.loss_cls: 0.1985  mix_decode.d7.loss_mask: 0.5069  mix_decode.d7.loss_dice: 0.7548  mix_decode.d8.loss_cls: 0.1977  mix_decode.d8.loss_mask: 0.5080  mix_decode.d8.loss_dice: 0.7582
2025/03/29 17:56:32 - mmengine - INFO - Iter(train) [18450/20000]  base_lr: 1.0009e-05 lr: 1.0009e-05  eta: 0:26:25  time: 1.1512  data_time: 0.0224  memory: 11211  loss: 58.4270  decode.loss_cls: 0.3971  decode.loss_mask: 1.9308  decode.loss_dice: 1.8394  decode.d0.loss_cls: 0.4258  decode.d0.loss_mask: 1.9514  decode.d0.loss_dice: 1.8253  decode.d1.loss_cls: 0.4869  decode.d1.loss_mask: 1.8937  decode.d1.loss_dice: 1.8329  decode.d2.loss_cls: 0.4280  decode.d2.loss_mask: 1.9185  decode.d2.loss_dice: 1.7850  decode.d3.loss_cls: 0.4281  decode.d3.loss_mask: 1.9089  decode.d3.loss_dice: 1.7882  decode.d4.loss_cls: 0.3996  decode.d4.loss_mask: 1.8921  decode.d4.loss_dice: 1.7579  decode.d5.loss_cls: 0.4200  decode.d5.loss_mask: 1.8980  decode.d5.loss_dice: 1.8240  decode.d6.loss_cls: 0.4407  decode.d6.loss_mask: 1.8817  decode.d6.loss_dice: 1.7864  decode.d7.loss_cls: 0.4178  decode.d7.loss_mask: 1.9007  decode.d7.loss_dice: 1.8071  decode.d8.loss_cls: 0.3835  decode.d8.loss_mask: 1.9334  decode.d8.loss_dice: 1.8611  mix_decode.loss_cls: 0.2367  mix_decode.loss_mask: 0.7025  mix_decode.loss_dice: 0.7198  mix_decode.d0.loss_cls: 0.2941  mix_decode.d0.loss_mask: 0.6895  mix_decode.d0.loss_dice: 0.7887  mix_decode.d1.loss_cls: 0.2665  mix_decode.d1.loss_mask: 0.6913  mix_decode.d1.loss_dice: 0.7288  mix_decode.d2.loss_cls: 0.2492  mix_decode.d2.loss_mask: 0.7219  mix_decode.d2.loss_dice: 0.7339  mix_decode.d3.loss_cls: 0.3043  mix_decode.d3.loss_mask: 0.6864  mix_decode.d3.loss_dice: 0.6974  mix_decode.d4.loss_cls: 0.2938  mix_decode.d4.loss_mask: 0.6863  mix_decode.d4.loss_dice: 0.7124  mix_decode.d5.loss_cls: 0.2916  mix_decode.d5.loss_mask: 0.7104  mix_decode.d5.loss_dice: 0.7012  mix_decode.d6.loss_cls: 0.2836  mix_decode.d6.loss_mask: 0.6842  mix_decode.d6.loss_dice: 0.7046  mix_decode.d7.loss_cls: 0.2698  mix_decode.d7.loss_mask: 0.7123  mix_decode.d7.loss_dice: 0.7325  mix_decode.d8.loss_cls: 0.2563  mix_decode.d8.loss_mask: 0.7061  mix_decode.d8.loss_dice: 0.7269
2025/03/29 17:57:30 - mmengine - INFO - Iter(train) [18500/20000]  base_lr: 9.7180e-06 lr: 9.7180e-06  eta: 0:25:34  time: 1.1523  data_time: 0.0226  memory: 11209  loss: 59.7151  decode.loss_cls: 0.4689  decode.loss_mask: 1.9161  decode.loss_dice: 1.8450  decode.d0.loss_cls: 0.3926  decode.d0.loss_mask: 1.9626  decode.d0.loss_dice: 1.9117  decode.d1.loss_cls: 0.3888  decode.d1.loss_mask: 1.9799  decode.d1.loss_dice: 1.8706  decode.d2.loss_cls: 0.3668  decode.d2.loss_mask: 1.9985  decode.d2.loss_dice: 1.8797  decode.d3.loss_cls: 0.3386  decode.d3.loss_mask: 2.0067  decode.d3.loss_dice: 1.9098  decode.d4.loss_cls: 0.3703  decode.d4.loss_mask: 1.9959  decode.d4.loss_dice: 1.9323  decode.d5.loss_cls: 0.3883  decode.d5.loss_mask: 2.0058  decode.d5.loss_dice: 1.9100  decode.d6.loss_cls: 0.4233  decode.d6.loss_mask: 1.9893  decode.d6.loss_dice: 1.9129  decode.d7.loss_cls: 0.4394  decode.d7.loss_mask: 2.0176  decode.d7.loss_dice: 1.8902  decode.d8.loss_cls: 0.4774  decode.d8.loss_mask: 1.9695  decode.d8.loss_dice: 1.8483  mix_decode.loss_cls: 0.2726  mix_decode.loss_mask: 0.6302  mix_decode.loss_dice: 0.7745  mix_decode.d0.loss_cls: 0.2416  mix_decode.d0.loss_mask: 0.6319  mix_decode.d0.loss_dice: 0.8584  mix_decode.d1.loss_cls: 0.2393  mix_decode.d1.loss_mask: 0.6450  mix_decode.d1.loss_dice: 0.8029  mix_decode.d2.loss_cls: 0.2766  mix_decode.d2.loss_mask: 0.6403  mix_decode.d2.loss_dice: 0.7738  mix_decode.d3.loss_cls: 0.2501  mix_decode.d3.loss_mask: 0.6326  mix_decode.d3.loss_dice: 0.7764  mix_decode.d4.loss_cls: 0.2714  mix_decode.d4.loss_mask: 0.6512  mix_decode.d4.loss_dice: 0.7750  mix_decode.d5.loss_cls: 0.2356  mix_decode.d5.loss_mask: 0.6373  mix_decode.d5.loss_dice: 0.8026  mix_decode.d6.loss_cls: 0.2653  mix_decode.d6.loss_mask: 0.6490  mix_decode.d6.loss_dice: 0.7904  mix_decode.d7.loss_cls: 0.3106  mix_decode.d7.loss_mask: 0.6329  mix_decode.d7.loss_dice: 0.7631  mix_decode.d8.loss_cls: 0.2236  mix_decode.d8.loss_mask: 0.6609  mix_decode.d8.loss_dice: 0.7932
2025/03/29 17:58:28 - mmengine - INFO - Iter(train) [18550/20000]  base_lr: 9.4259e-06 lr: 9.4259e-06  eta: 0:24:43  time: 1.1532  data_time: 0.0228  memory: 11215  loss: 52.2076  decode.loss_cls: 0.2484  decode.loss_mask: 1.8061  decode.loss_dice: 1.7666  decode.d0.loss_cls: 0.4145  decode.d0.loss_mask: 1.8112  decode.d0.loss_dice: 1.7802  decode.d1.loss_cls: 0.2659  decode.d1.loss_mask: 1.8210  decode.d1.loss_dice: 1.7635  decode.d2.loss_cls: 0.2599  decode.d2.loss_mask: 1.8016  decode.d2.loss_dice: 1.7543  decode.d3.loss_cls: 0.2757  decode.d3.loss_mask: 1.7928  decode.d3.loss_dice: 1.7624  decode.d4.loss_cls: 0.2584  decode.d4.loss_mask: 1.8338  decode.d4.loss_dice: 1.7728  decode.d5.loss_cls: 0.2186  decode.d5.loss_mask: 1.8120  decode.d5.loss_dice: 1.7809  decode.d6.loss_cls: 0.2760  decode.d6.loss_mask: 1.7903  decode.d6.loss_dice: 1.7852  decode.d7.loss_cls: 0.2644  decode.d7.loss_mask: 1.7911  decode.d7.loss_dice: 1.7380  decode.d8.loss_cls: 0.1905  decode.d8.loss_mask: 1.8443  decode.d8.loss_dice: 1.7952  mix_decode.loss_cls: 0.1842  mix_decode.loss_mask: 0.5659  mix_decode.loss_dice: 0.6259  mix_decode.d0.loss_cls: 0.1804  mix_decode.d0.loss_mask: 0.5657  mix_decode.d0.loss_dice: 0.6972  mix_decode.d1.loss_cls: 0.1955  mix_decode.d1.loss_mask: 0.5476  mix_decode.d1.loss_dice: 0.6055  mix_decode.d2.loss_cls: 0.2083  mix_decode.d2.loss_mask: 0.5374  mix_decode.d2.loss_dice: 0.6075  mix_decode.d3.loss_cls: 0.1732  mix_decode.d3.loss_mask: 0.5681  mix_decode.d3.loss_dice: 0.6168  mix_decode.d4.loss_cls: 0.1892  mix_decode.d4.loss_mask: 0.5521  mix_decode.d4.loss_dice: 0.6295  mix_decode.d5.loss_cls: 0.2071  mix_decode.d5.loss_mask: 0.5474  mix_decode.d5.loss_dice: 0.6111  mix_decode.d6.loss_cls: 0.1928  mix_decode.d6.loss_mask: 0.5443  mix_decode.d6.loss_dice: 0.6020  mix_decode.d7.loss_cls: 0.2042  mix_decode.d7.loss_mask: 0.5647  mix_decode.d7.loss_dice: 0.6197  mix_decode.d8.loss_cls: 0.1692  mix_decode.d8.loss_mask: 0.5794  mix_decode.d8.loss_dice: 0.6404
2025/03/29 17:59:25 - mmengine - INFO - Iter(train) [18600/20000]  base_lr: 9.1329e-06 lr: 9.1329e-06  eta: 0:23:53  time: 1.1492  data_time: 0.0229  memory: 11208  loss: 55.1405  decode.loss_cls: 0.2744  decode.loss_mask: 2.0301  decode.loss_dice: 1.7792  decode.d0.loss_cls: 0.4251  decode.d0.loss_mask: 2.0605  decode.d0.loss_dice: 1.7539  decode.d1.loss_cls: 0.2963  decode.d1.loss_mask: 1.9833  decode.d1.loss_dice: 1.7494  decode.d2.loss_cls: 0.3011  decode.d2.loss_mask: 1.9797  decode.d2.loss_dice: 1.7248  decode.d3.loss_cls: 0.3104  decode.d3.loss_mask: 1.9771  decode.d3.loss_dice: 1.7492  decode.d4.loss_cls: 0.2828  decode.d4.loss_mask: 1.9959  decode.d4.loss_dice: 1.7519  decode.d5.loss_cls: 0.2988  decode.d5.loss_mask: 2.0145  decode.d5.loss_dice: 1.7677  decode.d6.loss_cls: 0.2881  decode.d6.loss_mask: 2.0161  decode.d6.loss_dice: 1.7548  decode.d7.loss_cls: 0.2962  decode.d7.loss_mask: 2.0097  decode.d7.loss_dice: 1.7483  decode.d8.loss_cls: 0.2717  decode.d8.loss_mask: 2.0285  decode.d8.loss_dice: 1.7391  mix_decode.loss_cls: 0.1985  mix_decode.loss_mask: 0.5775  mix_decode.loss_dice: 0.6068  mix_decode.d0.loss_cls: 0.2671  mix_decode.d0.loss_mask: 0.5927  mix_decode.d0.loss_dice: 0.7302  mix_decode.d1.loss_cls: 0.2066  mix_decode.d1.loss_mask: 0.5976  mix_decode.d1.loss_dice: 0.6399  mix_decode.d2.loss_cls: 0.2431  mix_decode.d2.loss_mask: 0.5900  mix_decode.d2.loss_dice: 0.6079  mix_decode.d3.loss_cls: 0.2143  mix_decode.d3.loss_mask: 0.5836  mix_decode.d3.loss_dice: 0.6109  mix_decode.d4.loss_cls: 0.2091  mix_decode.d4.loss_mask: 0.5937  mix_decode.d4.loss_dice: 0.6302  mix_decode.d5.loss_cls: 0.2287  mix_decode.d5.loss_mask: 0.5917  mix_decode.d5.loss_dice: 0.6335  mix_decode.d6.loss_cls: 0.2148  mix_decode.d6.loss_mask: 0.5860  mix_decode.d6.loss_dice: 0.6400  mix_decode.d7.loss_cls: 0.2315  mix_decode.d7.loss_mask: 0.5799  mix_decode.d7.loss_dice: 0.6361  mix_decode.d8.loss_cls: 0.2066  mix_decode.d8.loss_mask: 0.5885  mix_decode.d8.loss_dice: 0.6447
2025/03/29 18:00:23 - mmengine - INFO - Iter(train) [18650/20000]  base_lr: 8.8388e-06 lr: 8.8388e-06  eta: 0:23:02  time: 1.1499  data_time: 0.0224  memory: 11224  loss: 48.0338  decode.loss_cls: 0.3086  decode.loss_mask: 1.5824  decode.loss_dice: 1.4656  decode.d0.loss_cls: 0.4432  decode.d0.loss_mask: 1.5857  decode.d0.loss_dice: 1.4807  decode.d1.loss_cls: 0.4241  decode.d1.loss_mask: 1.5640  decode.d1.loss_dice: 1.4410  decode.d2.loss_cls: 0.4071  decode.d2.loss_mask: 1.5663  decode.d2.loss_dice: 1.4426  decode.d3.loss_cls: 0.3083  decode.d3.loss_mask: 1.6327  decode.d3.loss_dice: 1.4669  decode.d4.loss_cls: 0.4018  decode.d4.loss_mask: 1.5841  decode.d4.loss_dice: 1.4515  decode.d5.loss_cls: 0.3557  decode.d5.loss_mask: 1.5889  decode.d5.loss_dice: 1.4740  decode.d6.loss_cls: 0.3829  decode.d6.loss_mask: 1.5690  decode.d6.loss_dice: 1.4687  decode.d7.loss_cls: 0.3416  decode.d7.loss_mask: 1.6228  decode.d7.loss_dice: 1.4819  decode.d8.loss_cls: 0.3171  decode.d8.loss_mask: 1.5732  decode.d8.loss_dice: 1.4508  mix_decode.loss_cls: 0.2416  mix_decode.loss_mask: 0.5216  mix_decode.loss_dice: 0.5893  mix_decode.d0.loss_cls: 0.2945  mix_decode.d0.loss_mask: 0.5283  mix_decode.d0.loss_dice: 0.7028  mix_decode.d1.loss_cls: 0.2173  mix_decode.d1.loss_mask: 0.5224  mix_decode.d1.loss_dice: 0.6166  mix_decode.d2.loss_cls: 0.1852  mix_decode.d2.loss_mask: 0.5338  mix_decode.d2.loss_dice: 0.6437  mix_decode.d3.loss_cls: 0.2125  mix_decode.d3.loss_mask: 0.5592  mix_decode.d3.loss_dice: 0.6078  mix_decode.d4.loss_cls: 0.1768  mix_decode.d4.loss_mask: 0.5321  mix_decode.d4.loss_dice: 0.6152  mix_decode.d5.loss_cls: 0.2555  mix_decode.d5.loss_mask: 0.5061  mix_decode.d5.loss_dice: 0.6232  mix_decode.d6.loss_cls: 0.1751  mix_decode.d6.loss_mask: 0.5547  mix_decode.d6.loss_dice: 0.6270  mix_decode.d7.loss_cls: 0.2046  mix_decode.d7.loss_mask: 0.5507  mix_decode.d7.loss_dice: 0.6382  mix_decode.d8.loss_cls: 0.2732  mix_decode.d8.loss_mask: 0.5208  mix_decode.d8.loss_dice: 0.6206
2025/03/29 18:01:20 - mmengine - INFO - Iter(train) [18700/20000]  base_lr: 8.5436e-06 lr: 8.5436e-06  eta: 0:22:11  time: 1.1456  data_time: 0.0227  memory: 11213  loss: 54.6716  decode.loss_cls: 0.3548  decode.loss_mask: 1.9379  decode.loss_dice: 1.7028  decode.d0.loss_cls: 0.4907  decode.d0.loss_mask: 1.8853  decode.d0.loss_dice: 1.6967  decode.d1.loss_cls: 0.3515  decode.d1.loss_mask: 1.9510  decode.d1.loss_dice: 1.6454  decode.d2.loss_cls: 0.3325  decode.d2.loss_mask: 1.9994  decode.d2.loss_dice: 1.7238  decode.d3.loss_cls: 0.3231  decode.d3.loss_mask: 1.9915  decode.d3.loss_dice: 1.6701  decode.d4.loss_cls: 0.3766  decode.d4.loss_mask: 1.9401  decode.d4.loss_dice: 1.6648  decode.d5.loss_cls: 0.3539  decode.d5.loss_mask: 1.9682  decode.d5.loss_dice: 1.7093  decode.d6.loss_cls: 0.3784  decode.d6.loss_mask: 1.9234  decode.d6.loss_dice: 1.6824  decode.d7.loss_cls: 0.4164  decode.d7.loss_mask: 2.0349  decode.d7.loss_dice: 1.7023  decode.d8.loss_cls: 0.3931  decode.d8.loss_mask: 1.9679  decode.d8.loss_dice: 1.7108  mix_decode.loss_cls: 0.2083  mix_decode.loss_mask: 0.5819  mix_decode.loss_dice: 0.6208  mix_decode.d0.loss_cls: 0.2432  mix_decode.d0.loss_mask: 0.5546  mix_decode.d0.loss_dice: 0.6831  mix_decode.d1.loss_cls: 0.2036  mix_decode.d1.loss_mask: 0.5786  mix_decode.d1.loss_dice: 0.6218  mix_decode.d2.loss_cls: 0.1876  mix_decode.d2.loss_mask: 0.6057  mix_decode.d2.loss_dice: 0.6357  mix_decode.d3.loss_cls: 0.1873  mix_decode.d3.loss_mask: 0.6311  mix_decode.d3.loss_dice: 0.6338  mix_decode.d4.loss_cls: 0.1804  mix_decode.d4.loss_mask: 0.6319  mix_decode.d4.loss_dice: 0.6426  mix_decode.d5.loss_cls: 0.1839  mix_decode.d5.loss_mask: 0.6202  mix_decode.d5.loss_dice: 0.6599  mix_decode.d6.loss_cls: 0.1724  mix_decode.d6.loss_mask: 0.6457  mix_decode.d6.loss_dice: 0.6473  mix_decode.d7.loss_cls: 0.2130  mix_decode.d7.loss_mask: 0.5817  mix_decode.d7.loss_dice: 0.6295  mix_decode.d8.loss_cls: 0.1685  mix_decode.d8.loss_mask: 0.6028  mix_decode.d8.loss_dice: 0.6362
2025/03/29 18:02:18 - mmengine - INFO - Iter(train) [18750/20000]  base_lr: 8.2473e-06 lr: 8.2473e-06  eta: 0:21:20  time: 1.1498  data_time: 0.0229  memory: 11213  loss: 54.6867  decode.loss_cls: 0.3755  decode.loss_mask: 1.8473  decode.loss_dice: 1.6877  decode.d0.loss_cls: 0.4644  decode.d0.loss_mask: 1.8081  decode.d0.loss_dice: 1.7462  decode.d1.loss_cls: 0.4269  decode.d1.loss_mask: 1.8129  decode.d1.loss_dice: 1.6514  decode.d2.loss_cls: 0.4207  decode.d2.loss_mask: 1.8150  decode.d2.loss_dice: 1.6807  decode.d3.loss_cls: 0.4466  decode.d3.loss_mask: 1.8045  decode.d3.loss_dice: 1.6799  decode.d4.loss_cls: 0.3566  decode.d4.loss_mask: 1.8600  decode.d4.loss_dice: 1.7233  decode.d5.loss_cls: 0.4101  decode.d5.loss_mask: 1.8609  decode.d5.loss_dice: 1.7378  decode.d6.loss_cls: 0.3510  decode.d6.loss_mask: 1.8362  decode.d6.loss_dice: 1.7222  decode.d7.loss_cls: 0.4380  decode.d7.loss_mask: 1.8210  decode.d7.loss_dice: 1.6819  decode.d8.loss_cls: 0.3792  decode.d8.loss_mask: 1.8608  decode.d8.loss_dice: 1.7164  mix_decode.loss_cls: 0.1276  mix_decode.loss_mask: 0.6422  mix_decode.loss_dice: 0.7058  mix_decode.d0.loss_cls: 0.1970  mix_decode.d0.loss_mask: 0.6376  mix_decode.d0.loss_dice: 0.7903  mix_decode.d1.loss_cls: 0.1955  mix_decode.d1.loss_mask: 0.6080  mix_decode.d1.loss_dice: 0.7071  mix_decode.d2.loss_cls: 0.1714  mix_decode.d2.loss_mask: 0.6329  mix_decode.d2.loss_dice: 0.6858  mix_decode.d3.loss_cls: 0.1878  mix_decode.d3.loss_mask: 0.6297  mix_decode.d3.loss_dice: 0.6998  mix_decode.d4.loss_cls: 0.1663  mix_decode.d4.loss_mask: 0.6402  mix_decode.d4.loss_dice: 0.7155  mix_decode.d5.loss_cls: 0.1962  mix_decode.d5.loss_mask: 0.6190  mix_decode.d5.loss_dice: 0.7217  mix_decode.d6.loss_cls: 0.1765  mix_decode.d6.loss_mask: 0.6253  mix_decode.d6.loss_dice: 0.7286  mix_decode.d7.loss_cls: 0.2020  mix_decode.d7.loss_mask: 0.6320  mix_decode.d7.loss_dice: 0.7185  mix_decode.d8.loss_cls: 0.1520  mix_decode.d8.loss_mask: 0.6286  mix_decode.d8.loss_dice: 0.7226
2025/03/29 18:03:16 - mmengine - INFO - Iter(train) [18800/20000]  base_lr: 7.9498e-06 lr: 7.9498e-06  eta: 0:20:29  time: 1.1482  data_time: 0.0226  memory: 11222  loss: 50.6597  decode.loss_cls: 0.2858  decode.loss_mask: 1.6703  decode.loss_dice: 1.5025  decode.d0.loss_cls: 0.4526  decode.d0.loss_mask: 1.6914  decode.d0.loss_dice: 1.5340  decode.d1.loss_cls: 0.3373  decode.d1.loss_mask: 1.6801  decode.d1.loss_dice: 1.5101  decode.d2.loss_cls: 0.3119  decode.d2.loss_mask: 1.6816  decode.d2.loss_dice: 1.5080  decode.d3.loss_cls: 0.3018  decode.d3.loss_mask: 1.6678  decode.d3.loss_dice: 1.5412  decode.d4.loss_cls: 0.3277  decode.d4.loss_mask: 1.6682  decode.d4.loss_dice: 1.5060  decode.d5.loss_cls: 0.3399  decode.d5.loss_mask: 1.6827  decode.d5.loss_dice: 1.5134  decode.d6.loss_cls: 0.2959  decode.d6.loss_mask: 1.6761  decode.d6.loss_dice: 1.5433  decode.d7.loss_cls: 0.3090  decode.d7.loss_mask: 1.6738  decode.d7.loss_dice: 1.5059  decode.d8.loss_cls: 0.2970  decode.d8.loss_mask: 1.6628  decode.d8.loss_dice: 1.5001  mix_decode.loss_cls: 0.2446  mix_decode.loss_mask: 0.5470  mix_decode.loss_dice: 0.6865  mix_decode.d0.loss_cls: 0.2910  mix_decode.d0.loss_mask: 0.5788  mix_decode.d0.loss_dice: 0.7935  mix_decode.d1.loss_cls: 0.2284  mix_decode.d1.loss_mask: 0.5849  mix_decode.d1.loss_dice: 0.6934  mix_decode.d2.loss_cls: 0.2054  mix_decode.d2.loss_mask: 0.5958  mix_decode.d2.loss_dice: 0.7169  mix_decode.d3.loss_cls: 0.2531  mix_decode.d3.loss_mask: 0.5624  mix_decode.d3.loss_dice: 0.6990  mix_decode.d4.loss_cls: 0.2500  mix_decode.d4.loss_mask: 0.5668  mix_decode.d4.loss_dice: 0.7097  mix_decode.d5.loss_cls: 0.2501  mix_decode.d5.loss_mask: 0.6107  mix_decode.d5.loss_dice: 0.7591  mix_decode.d6.loss_cls: 0.2522  mix_decode.d6.loss_mask: 0.5944  mix_decode.d6.loss_dice: 0.7181  mix_decode.d7.loss_cls: 0.2450  mix_decode.d7.loss_mask: 0.5894  mix_decode.d7.loss_dice: 0.7305  mix_decode.d8.loss_cls: 0.2381  mix_decode.d8.loss_mask: 0.5738  mix_decode.d8.loss_dice: 0.7127
2025/03/29 18:04:13 - mmengine - INFO - Iter(train) [18850/20000]  base_lr: 7.6510e-06 lr: 7.6510e-06  eta: 0:19:39  time: 1.1536  data_time: 0.0232  memory: 11215  loss: 51.1237  decode.loss_cls: 0.3977  decode.loss_mask: 1.6727  decode.loss_dice: 1.7677  decode.d0.loss_cls: 0.4900  decode.d0.loss_mask: 1.6353  decode.d0.loss_dice: 1.7904  decode.d1.loss_cls: 0.4150  decode.d1.loss_mask: 1.6380  decode.d1.loss_dice: 1.7667  decode.d2.loss_cls: 0.4553  decode.d2.loss_mask: 1.6501  decode.d2.loss_dice: 1.7468  decode.d3.loss_cls: 0.3234  decode.d3.loss_mask: 1.6604  decode.d3.loss_dice: 1.8147  decode.d4.loss_cls: 0.3407  decode.d4.loss_mask: 1.6336  decode.d4.loss_dice: 1.7949  decode.d5.loss_cls: 0.3753  decode.d5.loss_mask: 1.6515  decode.d5.loss_dice: 1.7766  decode.d6.loss_cls: 0.3498  decode.d6.loss_mask: 1.6222  decode.d6.loss_dice: 1.7976  decode.d7.loss_cls: 0.4004  decode.d7.loss_mask: 1.6409  decode.d7.loss_dice: 1.7771  decode.d8.loss_cls: 0.4770  decode.d8.loss_mask: 1.6576  decode.d8.loss_dice: 1.7799  mix_decode.loss_cls: 0.2390  mix_decode.loss_mask: 0.4905  mix_decode.loss_dice: 0.5383  mix_decode.d0.loss_cls: 0.2858  mix_decode.d0.loss_mask: 0.4918  mix_decode.d0.loss_dice: 0.6167  mix_decode.d1.loss_cls: 0.2298  mix_decode.d1.loss_mask: 0.4913  mix_decode.d1.loss_dice: 0.5669  mix_decode.d2.loss_cls: 0.2130  mix_decode.d2.loss_mask: 0.4860  mix_decode.d2.loss_dice: 0.5511  mix_decode.d3.loss_cls: 0.2236  mix_decode.d3.loss_mask: 0.4877  mix_decode.d3.loss_dice: 0.5481  mix_decode.d4.loss_cls: 0.2257  mix_decode.d4.loss_mask: 0.4923  mix_decode.d4.loss_dice: 0.5640  mix_decode.d5.loss_cls: 0.2306  mix_decode.d5.loss_mask: 0.4964  mix_decode.d5.loss_dice: 0.5653  mix_decode.d6.loss_cls: 0.2160  mix_decode.d6.loss_mask: 0.4895  mix_decode.d6.loss_dice: 0.5526  mix_decode.d7.loss_cls: 0.2094  mix_decode.d7.loss_mask: 0.4979  mix_decode.d7.loss_dice: 0.5553  mix_decode.d8.loss_cls: 0.2530  mix_decode.d8.loss_mask: 0.4822  mix_decode.d8.loss_dice: 0.5345
2025/03/29 18:05:10 - mmengine - INFO - Iter(train) [18900/20000]  base_lr: 7.3510e-06 lr: 7.3510e-06  eta: 0:18:48  time: 1.1479  data_time: 0.0229  memory: 11205  loss: 53.7038  decode.loss_cls: 0.3624  decode.loss_mask: 2.0412  decode.loss_dice: 1.6680  decode.d0.loss_cls: 0.4255  decode.d0.loss_mask: 2.0411  decode.d0.loss_dice: 1.6866  decode.d1.loss_cls: 0.4040  decode.d1.loss_mask: 1.9356  decode.d1.loss_dice: 1.6418  decode.d2.loss_cls: 0.4331  decode.d2.loss_mask: 1.9293  decode.d2.loss_dice: 1.6421  decode.d3.loss_cls: 0.3971  decode.d3.loss_mask: 1.9407  decode.d3.loss_dice: 1.6458  decode.d4.loss_cls: 0.4663  decode.d4.loss_mask: 1.9816  decode.d4.loss_dice: 1.6431  decode.d5.loss_cls: 0.4230  decode.d5.loss_mask: 1.9386  decode.d5.loss_dice: 1.5928  decode.d6.loss_cls: 0.4062  decode.d6.loss_mask: 1.9669  decode.d6.loss_dice: 1.6364  decode.d7.loss_cls: 0.4883  decode.d7.loss_mask: 1.9238  decode.d7.loss_dice: 1.6336  decode.d8.loss_cls: 0.3420  decode.d8.loss_mask: 1.9591  decode.d8.loss_dice: 1.6804  mix_decode.loss_cls: 0.1432  mix_decode.loss_mask: 0.5595  mix_decode.loss_dice: 0.6282  mix_decode.d0.loss_cls: 0.2247  mix_decode.d0.loss_mask: 0.5420  mix_decode.d0.loss_dice: 0.6928  mix_decode.d1.loss_cls: 0.2138  mix_decode.d1.loss_mask: 0.5245  mix_decode.d1.loss_dice: 0.6128  mix_decode.d2.loss_cls: 0.1270  mix_decode.d2.loss_mask: 0.5837  mix_decode.d2.loss_dice: 0.6307  mix_decode.d3.loss_cls: 0.1776  mix_decode.d3.loss_mask: 0.5271  mix_decode.d3.loss_dice: 0.6138  mix_decode.d4.loss_cls: 0.1816  mix_decode.d4.loss_mask: 0.5295  mix_decode.d4.loss_dice: 0.6032  mix_decode.d5.loss_cls: 0.1543  mix_decode.d5.loss_mask: 0.5421  mix_decode.d5.loss_dice: 0.6227  mix_decode.d6.loss_cls: 0.1421  mix_decode.d6.loss_mask: 0.5565  mix_decode.d6.loss_dice: 0.6083  mix_decode.d7.loss_cls: 0.1751  mix_decode.d7.loss_mask: 0.5351  mix_decode.d7.loss_dice: 0.6285  mix_decode.d8.loss_cls: 0.1485  mix_decode.d8.loss_mask: 0.5634  mix_decode.d8.loss_dice: 0.6349
2025/03/29 18:06:08 - mmengine - INFO - Iter(train) [18950/20000]  base_lr: 7.0496e-06 lr: 7.0496e-06  eta: 0:17:57  time: 1.1495  data_time: 0.0225  memory: 11203  loss: 48.5006  decode.loss_cls: 0.2333  decode.loss_mask: 1.7312  decode.loss_dice: 1.7058  decode.d0.loss_cls: 0.2999  decode.d0.loss_mask: 1.7288  decode.d0.loss_dice: 1.6768  decode.d1.loss_cls: 0.2372  decode.d1.loss_mask: 1.7280  decode.d1.loss_dice: 1.6687  decode.d2.loss_cls: 0.2420  decode.d2.loss_mask: 1.7318  decode.d2.loss_dice: 1.6793  decode.d3.loss_cls: 0.2310  decode.d3.loss_mask: 1.7167  decode.d3.loss_dice: 1.6794  decode.d4.loss_cls: 0.2394  decode.d4.loss_mask: 1.7051  decode.d4.loss_dice: 1.6880  decode.d5.loss_cls: 0.2392  decode.d5.loss_mask: 1.7066  decode.d5.loss_dice: 1.6898  decode.d6.loss_cls: 0.2343  decode.d6.loss_mask: 1.7155  decode.d6.loss_dice: 1.6947  decode.d7.loss_cls: 0.2037  decode.d7.loss_mask: 1.7123  decode.d7.loss_dice: 1.7017  decode.d8.loss_cls: 0.2375  decode.d8.loss_mask: 1.7065  decode.d8.loss_dice: 1.6885  mix_decode.loss_cls: 0.0830  mix_decode.loss_mask: 0.5139  mix_decode.loss_dice: 0.5841  mix_decode.d0.loss_cls: 0.1744  mix_decode.d0.loss_mask: 0.5057  mix_decode.d0.loss_dice: 0.6169  mix_decode.d1.loss_cls: 0.1259  mix_decode.d1.loss_mask: 0.5087  mix_decode.d1.loss_dice: 0.5738  mix_decode.d2.loss_cls: 0.1032  mix_decode.d2.loss_mask: 0.5050  mix_decode.d2.loss_dice: 0.5749  mix_decode.d3.loss_cls: 0.0990  mix_decode.d3.loss_mask: 0.5055  mix_decode.d3.loss_dice: 0.5656  mix_decode.d4.loss_cls: 0.1039  mix_decode.d4.loss_mask: 0.5104  mix_decode.d4.loss_dice: 0.5711  mix_decode.d5.loss_cls: 0.0954  mix_decode.d5.loss_mask: 0.5090  mix_decode.d5.loss_dice: 0.5847  mix_decode.d6.loss_cls: 0.0924  mix_decode.d6.loss_mask: 0.5195  mix_decode.d6.loss_dice: 0.5941  mix_decode.d7.loss_cls: 0.1210  mix_decode.d7.loss_mask: 0.5171  mix_decode.d7.loss_dice: 0.5885  mix_decode.d8.loss_cls: 0.0913  mix_decode.d8.loss_mask: 0.5182  mix_decode.d8.loss_dice: 0.5914
2025/03/29 18:07:05 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 18:07:05 - mmengine - INFO - Iter(train) [19000/20000]  base_lr: 6.7467e-06 lr: 6.7467e-06  eta: 0:17:06  time: 1.1483  data_time: 0.0227  memory: 11230  loss: 50.1587  decode.loss_cls: 0.5733  decode.loss_mask: 1.4177  decode.loss_dice: 1.5618  decode.d0.loss_cls: 0.5957  decode.d0.loss_mask: 1.4607  decode.d0.loss_dice: 1.7068  decode.d1.loss_cls: 0.6219  decode.d1.loss_mask: 1.4219  decode.d1.loss_dice: 1.5151  decode.d2.loss_cls: 0.6513  decode.d2.loss_mask: 1.4083  decode.d2.loss_dice: 1.5190  decode.d3.loss_cls: 0.6567  decode.d3.loss_mask: 1.4300  decode.d3.loss_dice: 1.5063  decode.d4.loss_cls: 0.5370  decode.d4.loss_mask: 1.4528  decode.d4.loss_dice: 1.5425  decode.d5.loss_cls: 0.5948  decode.d5.loss_mask: 1.4523  decode.d5.loss_dice: 1.5307  decode.d6.loss_cls: 0.6173  decode.d6.loss_mask: 1.4128  decode.d6.loss_dice: 1.5357  decode.d7.loss_cls: 0.6403  decode.d7.loss_mask: 1.4055  decode.d7.loss_dice: 1.5274  decode.d8.loss_cls: 0.5899  decode.d8.loss_mask: 1.4346  decode.d8.loss_dice: 1.5227  mix_decode.loss_cls: 0.2457  mix_decode.loss_mask: 0.5178  mix_decode.loss_dice: 0.6412  mix_decode.d0.loss_cls: 0.3052  mix_decode.d0.loss_mask: 0.5053  mix_decode.d0.loss_dice: 0.6797  mix_decode.d1.loss_cls: 0.3047  mix_decode.d1.loss_mask: 0.4990  mix_decode.d1.loss_dice: 0.6398  mix_decode.d2.loss_cls: 0.2974  mix_decode.d2.loss_mask: 0.5057  mix_decode.d2.loss_dice: 0.6308  mix_decode.d3.loss_cls: 0.3116  mix_decode.d3.loss_mask: 0.4995  mix_decode.d3.loss_dice: 0.6074  mix_decode.d4.loss_cls: 0.2762  mix_decode.d4.loss_mask: 0.5207  mix_decode.d4.loss_dice: 0.6152  mix_decode.d5.loss_cls: 0.3264  mix_decode.d5.loss_mask: 0.5043  mix_decode.d5.loss_dice: 0.6213  mix_decode.d6.loss_cls: 0.2581  mix_decode.d6.loss_mask: 0.5101  mix_decode.d6.loss_dice: 0.6355  mix_decode.d7.loss_cls: 0.3185  mix_decode.d7.loss_mask: 0.5083  mix_decode.d7.loss_dice: 0.6408  mix_decode.d8.loss_cls: 0.2372  mix_decode.d8.loss_mask: 0.5140  mix_decode.d8.loss_dice: 0.6389
2025/03/29 18:08:03 - mmengine - INFO - Iter(train) [19050/20000]  base_lr: 6.4423e-06 lr: 6.4423e-06  eta: 0:16:15  time: 1.1467  data_time: 0.0226  memory: 11209  loss: 49.0481  decode.loss_cls: 0.2821  decode.loss_mask: 1.5445  decode.loss_dice: 1.4309  decode.d0.loss_cls: 0.4383  decode.d0.loss_mask: 1.5223  decode.d0.loss_dice: 1.4488  decode.d1.loss_cls: 0.3708  decode.d1.loss_mask: 1.5454  decode.d1.loss_dice: 1.4617  decode.d2.loss_cls: 0.3172  decode.d2.loss_mask: 1.5458  decode.d2.loss_dice: 1.4334  decode.d3.loss_cls: 0.3158  decode.d3.loss_mask: 1.5600  decode.d3.loss_dice: 1.4456  decode.d4.loss_cls: 0.3079  decode.d4.loss_mask: 1.5465  decode.d4.loss_dice: 1.4417  decode.d5.loss_cls: 0.2948  decode.d5.loss_mask: 1.5429  decode.d5.loss_dice: 1.5085  decode.d6.loss_cls: 0.4030  decode.d6.loss_mask: 1.5456  decode.d6.loss_dice: 1.4270  decode.d7.loss_cls: 0.3187  decode.d7.loss_mask: 1.5699  decode.d7.loss_dice: 1.4765  decode.d8.loss_cls: 0.3434  decode.d8.loss_mask: 1.5253  decode.d8.loss_dice: 1.4393  mix_decode.loss_cls: 0.2126  mix_decode.loss_mask: 0.6563  mix_decode.loss_dice: 0.6630  mix_decode.d0.loss_cls: 0.2271  mix_decode.d0.loss_mask: 0.6708  mix_decode.d0.loss_dice: 0.7117  mix_decode.d1.loss_cls: 0.2429  mix_decode.d1.loss_mask: 0.6787  mix_decode.d1.loss_dice: 0.6542  mix_decode.d2.loss_cls: 0.2191  mix_decode.d2.loss_mask: 0.6690  mix_decode.d2.loss_dice: 0.6656  mix_decode.d3.loss_cls: 0.2269  mix_decode.d3.loss_mask: 0.6608  mix_decode.d3.loss_dice: 0.6742  mix_decode.d4.loss_cls: 0.2081  mix_decode.d4.loss_mask: 0.6899  mix_decode.d4.loss_dice: 0.6752  mix_decode.d5.loss_cls: 0.1900  mix_decode.d5.loss_mask: 0.6717  mix_decode.d5.loss_dice: 0.6809  mix_decode.d6.loss_cls: 0.2087  mix_decode.d6.loss_mask: 0.6882  mix_decode.d6.loss_dice: 0.6991  mix_decode.d7.loss_cls: 0.1870  mix_decode.d7.loss_mask: 0.7224  mix_decode.d7.loss_dice: 0.6870  mix_decode.d8.loss_cls: 0.2278  mix_decode.d8.loss_mask: 0.6686  mix_decode.d8.loss_dice: 0.6570
2025/03/29 18:09:00 - mmengine - INFO - Iter(train) [19100/20000]  base_lr: 6.1364e-06 lr: 6.1364e-06  eta: 0:15:24  time: 1.1483  data_time: 0.0225  memory: 11213  loss: 53.7956  decode.loss_cls: 0.3467  decode.loss_mask: 1.8620  decode.loss_dice: 1.5929  decode.d0.loss_cls: 0.5330  decode.d0.loss_mask: 1.8747  decode.d0.loss_dice: 1.6140  decode.d1.loss_cls: 0.3474  decode.d1.loss_mask: 1.8884  decode.d1.loss_dice: 1.5814  decode.d2.loss_cls: 0.3577  decode.d2.loss_mask: 1.8716  decode.d2.loss_dice: 1.5671  decode.d3.loss_cls: 0.3526  decode.d3.loss_mask: 1.8786  decode.d3.loss_dice: 1.5738  decode.d4.loss_cls: 0.3403  decode.d4.loss_mask: 1.8382  decode.d4.loss_dice: 1.6382  decode.d5.loss_cls: 0.3718  decode.d5.loss_mask: 1.8352  decode.d5.loss_dice: 1.6065  decode.d6.loss_cls: 0.3424  decode.d6.loss_mask: 1.9029  decode.d6.loss_dice: 1.6334  decode.d7.loss_cls: 0.3301  decode.d7.loss_mask: 1.9020  decode.d7.loss_dice: 1.6208  decode.d8.loss_cls: 0.3548  decode.d8.loss_mask: 1.8636  decode.d8.loss_dice: 1.5946  mix_decode.loss_cls: 0.2655  mix_decode.loss_mask: 0.5927  mix_decode.loss_dice: 0.5974  mix_decode.d0.loss_cls: 0.3232  mix_decode.d0.loss_mask: 0.6001  mix_decode.d0.loss_dice: 0.7025  mix_decode.d1.loss_cls: 0.2408  mix_decode.d1.loss_mask: 0.6317  mix_decode.d1.loss_dice: 0.6411  mix_decode.d2.loss_cls: 0.2299  mix_decode.d2.loss_mask: 0.6365  mix_decode.d2.loss_dice: 0.6460  mix_decode.d3.loss_cls: 0.2595  mix_decode.d3.loss_mask: 0.6499  mix_decode.d3.loss_dice: 0.6343  mix_decode.d4.loss_cls: 0.2105  mix_decode.d4.loss_mask: 0.6572  mix_decode.d4.loss_dice: 0.6533  mix_decode.d5.loss_cls: 0.2649  mix_decode.d5.loss_mask: 0.6260  mix_decode.d5.loss_dice: 0.6697  mix_decode.d6.loss_cls: 0.3098  mix_decode.d6.loss_mask: 0.6343  mix_decode.d6.loss_dice: 0.6478  mix_decode.d7.loss_cls: 0.3076  mix_decode.d7.loss_mask: 0.6093  mix_decode.d7.loss_dice: 0.6288  mix_decode.d8.loss_cls: 0.3094  mix_decode.d8.loss_mask: 0.5960  mix_decode.d8.loss_dice: 0.6030
2025/03/29 18:09:58 - mmengine - INFO - Iter(train) [19150/20000]  base_lr: 5.8287e-06 lr: 5.8287e-06  eta: 0:14:33  time: 1.1482  data_time: 0.0225  memory: 11211  loss: 50.9907  decode.loss_cls: 0.3560  decode.loss_mask: 1.6932  decode.loss_dice: 1.7145  decode.d0.loss_cls: 0.3631  decode.d0.loss_mask: 1.7014  decode.d0.loss_dice: 1.7122  decode.d1.loss_cls: 0.2693  decode.d1.loss_mask: 1.7126  decode.d1.loss_dice: 1.6947  decode.d2.loss_cls: 0.3334  decode.d2.loss_mask: 1.6827  decode.d2.loss_dice: 1.6890  decode.d3.loss_cls: 0.2493  decode.d3.loss_mask: 1.6851  decode.d3.loss_dice: 1.6930  decode.d4.loss_cls: 0.1903  decode.d4.loss_mask: 1.7163  decode.d4.loss_dice: 1.7280  decode.d5.loss_cls: 0.1995  decode.d5.loss_mask: 1.7388  decode.d5.loss_dice: 1.7503  decode.d6.loss_cls: 0.3253  decode.d6.loss_mask: 1.6491  decode.d6.loss_dice: 1.6823  decode.d7.loss_cls: 0.3580  decode.d7.loss_mask: 1.7108  decode.d7.loss_dice: 1.7011  decode.d8.loss_cls: 0.2686  decode.d8.loss_mask: 1.7185  decode.d8.loss_dice: 1.7268  mix_decode.loss_cls: 0.1822  mix_decode.loss_mask: 0.5444  mix_decode.loss_dice: 0.6632  mix_decode.d0.loss_cls: 0.1871  mix_decode.d0.loss_mask: 0.5332  mix_decode.d0.loss_dice: 0.7194  mix_decode.d1.loss_cls: 0.1800  mix_decode.d1.loss_mask: 0.5400  mix_decode.d1.loss_dice: 0.6546  mix_decode.d2.loss_cls: 0.1796  mix_decode.d2.loss_mask: 0.5479  mix_decode.d2.loss_dice: 0.6724  mix_decode.d3.loss_cls: 0.1515  mix_decode.d3.loss_mask: 0.5385  mix_decode.d3.loss_dice: 0.6715  mix_decode.d4.loss_cls: 0.1606  mix_decode.d4.loss_mask: 0.5632  mix_decode.d4.loss_dice: 0.6772  mix_decode.d5.loss_cls: 0.1882  mix_decode.d5.loss_mask: 0.5441  mix_decode.d5.loss_dice: 0.6621  mix_decode.d6.loss_cls: 0.1742  mix_decode.d6.loss_mask: 0.5495  mix_decode.d6.loss_dice: 0.6629  mix_decode.d7.loss_cls: 0.1690  mix_decode.d7.loss_mask: 0.5506  mix_decode.d7.loss_dice: 0.6725  mix_decode.d8.loss_cls: 0.2084  mix_decode.d8.loss_mask: 0.5627  mix_decode.d8.loss_dice: 0.6663
2025/03/29 18:10:55 - mmengine - INFO - Iter(train) [19200/20000]  base_lr: 5.5192e-06 lr: 5.5192e-06  eta: 0:13:42  time: 1.1520  data_time: 0.0232  memory: 11225  loss: 52.1337  decode.loss_cls: 0.4470  decode.loss_mask: 1.5787  decode.loss_dice: 1.5321  decode.d0.loss_cls: 0.4458  decode.d0.loss_mask: 1.6154  decode.d0.loss_dice: 1.6075  decode.d1.loss_cls: 0.4083  decode.d1.loss_mask: 1.6283  decode.d1.loss_dice: 1.5521  decode.d2.loss_cls: 0.3951  decode.d2.loss_mask: 1.6160  decode.d2.loss_dice: 1.5590  decode.d3.loss_cls: 0.3913  decode.d3.loss_mask: 1.6187  decode.d3.loss_dice: 1.5543  decode.d4.loss_cls: 0.4213  decode.d4.loss_mask: 1.5908  decode.d4.loss_dice: 1.5373  decode.d5.loss_cls: 0.3670  decode.d5.loss_mask: 1.6566  decode.d5.loss_dice: 1.5639  decode.d6.loss_cls: 0.4429  decode.d6.loss_mask: 1.6160  decode.d6.loss_dice: 1.5359  decode.d7.loss_cls: 0.4158  decode.d7.loss_mask: 1.6263  decode.d7.loss_dice: 1.5539  decode.d8.loss_cls: 0.4130  decode.d8.loss_mask: 1.5890  decode.d8.loss_dice: 1.5753  mix_decode.loss_cls: 0.2627  mix_decode.loss_mask: 0.6258  mix_decode.loss_dice: 0.7193  mix_decode.d0.loss_cls: 0.2678  mix_decode.d0.loss_mask: 0.5910  mix_decode.d0.loss_dice: 0.8323  mix_decode.d1.loss_cls: 0.2610  mix_decode.d1.loss_mask: 0.6261  mix_decode.d1.loss_dice: 0.7298  mix_decode.d2.loss_cls: 0.2899  mix_decode.d2.loss_mask: 0.6020  mix_decode.d2.loss_dice: 0.7018  mix_decode.d3.loss_cls: 0.2786  mix_decode.d3.loss_mask: 0.6243  mix_decode.d3.loss_dice: 0.7128  mix_decode.d4.loss_cls: 0.2460  mix_decode.d4.loss_mask: 0.6165  mix_decode.d4.loss_dice: 0.7498  mix_decode.d5.loss_cls: 0.2948  mix_decode.d5.loss_mask: 0.6123  mix_decode.d5.loss_dice: 0.7279  mix_decode.d6.loss_cls: 0.3101  mix_decode.d6.loss_mask: 0.6338  mix_decode.d6.loss_dice: 0.7243  mix_decode.d7.loss_cls: 0.2866  mix_decode.d7.loss_mask: 0.6324  mix_decode.d7.loss_dice: 0.7142  mix_decode.d8.loss_cls: 0.2472  mix_decode.d8.loss_mask: 0.6358  mix_decode.d8.loss_dice: 0.7216
2025/03/29 18:11:52 - mmengine - INFO - Iter(train) [19250/20000]  base_lr: 5.2077e-06 lr: 5.2077e-06  eta: 0:12:50  time: 1.1455  data_time: 0.0223  memory: 11214  loss: 47.4456  decode.loss_cls: 0.1879  decode.loss_mask: 1.6751  decode.loss_dice: 1.4925  decode.d0.loss_cls: 0.3699  decode.d0.loss_mask: 1.6785  decode.d0.loss_dice: 1.4902  decode.d1.loss_cls: 0.2530  decode.d1.loss_mask: 1.6681  decode.d1.loss_dice: 1.4647  decode.d2.loss_cls: 0.2075  decode.d2.loss_mask: 1.6857  decode.d2.loss_dice: 1.4946  decode.d3.loss_cls: 0.2401  decode.d3.loss_mask: 1.6685  decode.d3.loss_dice: 1.4618  decode.d4.loss_cls: 0.2369  decode.d4.loss_mask: 1.6574  decode.d4.loss_dice: 1.4672  decode.d5.loss_cls: 0.2618  decode.d5.loss_mask: 1.6660  decode.d5.loss_dice: 1.4747  decode.d6.loss_cls: 0.2361  decode.d6.loss_mask: 1.6775  decode.d6.loss_dice: 1.4782  decode.d7.loss_cls: 0.2390  decode.d7.loss_mask: 1.6569  decode.d7.loss_dice: 1.4695  decode.d8.loss_cls: 0.1948  decode.d8.loss_mask: 1.6867  decode.d8.loss_dice: 1.5084  mix_decode.loss_cls: 0.1242  mix_decode.loss_mask: 0.5805  mix_decode.loss_dice: 0.6374  mix_decode.d0.loss_cls: 0.1608  mix_decode.d0.loss_mask: 0.5926  mix_decode.d0.loss_dice: 0.6980  mix_decode.d1.loss_cls: 0.1926  mix_decode.d1.loss_mask: 0.5570  mix_decode.d1.loss_dice: 0.6113  mix_decode.d2.loss_cls: 0.1372  mix_decode.d2.loss_mask: 0.5597  mix_decode.d2.loss_dice: 0.6212  mix_decode.d3.loss_cls: 0.1191  mix_decode.d3.loss_mask: 0.5854  mix_decode.d3.loss_dice: 0.6360  mix_decode.d4.loss_cls: 0.1386  mix_decode.d4.loss_mask: 0.5820  mix_decode.d4.loss_dice: 0.6236  mix_decode.d5.loss_cls: 0.1546  mix_decode.d5.loss_mask: 0.5571  mix_decode.d5.loss_dice: 0.6170  mix_decode.d6.loss_cls: 0.1692  mix_decode.d6.loss_mask: 0.5465  mix_decode.d6.loss_dice: 0.6139  mix_decode.d7.loss_cls: 0.1432  mix_decode.d7.loss_mask: 0.5582  mix_decode.d7.loss_dice: 0.6252  mix_decode.d8.loss_cls: 0.1315  mix_decode.d8.loss_mask: 0.5877  mix_decode.d8.loss_dice: 0.6350
2025/03/29 18:12:50 - mmengine - INFO - Iter(train) [19300/20000]  base_lr: 4.8942e-06 lr: 4.8942e-06  eta: 0:11:59  time: 1.1584  data_time: 0.0233  memory: 11211  loss: 45.7461  decode.loss_cls: 0.1798  decode.loss_mask: 1.5319  decode.loss_dice: 1.3740  decode.d0.loss_cls: 0.3181  decode.d0.loss_mask: 1.5685  decode.d0.loss_dice: 1.4038  decode.d1.loss_cls: 0.1805  decode.d1.loss_mask: 1.5424  decode.d1.loss_dice: 1.4054  decode.d2.loss_cls: 0.1769  decode.d2.loss_mask: 1.5619  decode.d2.loss_dice: 1.4027  decode.d3.loss_cls: 0.1806  decode.d3.loss_mask: 1.5363  decode.d3.loss_dice: 1.3784  decode.d4.loss_cls: 0.1929  decode.d4.loss_mask: 1.5531  decode.d4.loss_dice: 1.3849  decode.d5.loss_cls: 0.1962  decode.d5.loss_mask: 1.5372  decode.d5.loss_dice: 1.3922  decode.d6.loss_cls: 0.1693  decode.d6.loss_mask: 1.5679  decode.d6.loss_dice: 1.4070  decode.d7.loss_cls: 0.1627  decode.d7.loss_mask: 1.5452  decode.d7.loss_dice: 1.4156  decode.d8.loss_cls: 0.1859  decode.d8.loss_mask: 1.5375  decode.d8.loss_dice: 1.3936  mix_decode.loss_cls: 0.1594  mix_decode.loss_mask: 0.5805  mix_decode.loss_dice: 0.6576  mix_decode.d0.loss_cls: 0.1827  mix_decode.d0.loss_mask: 0.5943  mix_decode.d0.loss_dice: 0.7777  mix_decode.d1.loss_cls: 0.1854  mix_decode.d1.loss_mask: 0.5937  mix_decode.d1.loss_dice: 0.6837  mix_decode.d2.loss_cls: 0.1564  mix_decode.d2.loss_mask: 0.5963  mix_decode.d2.loss_dice: 0.6925  mix_decode.d3.loss_cls: 0.1490  mix_decode.d3.loss_mask: 0.5951  mix_decode.d3.loss_dice: 0.6753  mix_decode.d4.loss_cls: 0.1395  mix_decode.d4.loss_mask: 0.5982  mix_decode.d4.loss_dice: 0.6703  mix_decode.d5.loss_cls: 0.1431  mix_decode.d5.loss_mask: 0.5796  mix_decode.d5.loss_dice: 0.6916  mix_decode.d6.loss_cls: 0.1559  mix_decode.d6.loss_mask: 0.5816  mix_decode.d6.loss_dice: 0.6767  mix_decode.d7.loss_cls: 0.1587  mix_decode.d7.loss_mask: 0.5838  mix_decode.d7.loss_dice: 0.6873  mix_decode.d8.loss_cls: 0.1630  mix_decode.d8.loss_mask: 0.5787  mix_decode.d8.loss_dice: 0.6764
2025/03/29 18:13:48 - mmengine - INFO - Iter(train) [19350/20000]  base_lr: 4.5784e-06 lr: 4.5784e-06  eta: 0:11:08  time: 1.1531  data_time: 0.0231  memory: 11211  loss: 58.9342  decode.loss_cls: 0.4551  decode.loss_mask: 2.1262  decode.loss_dice: 1.9770  decode.d0.loss_cls: 0.5153  decode.d0.loss_mask: 2.1197  decode.d0.loss_dice: 1.9626  decode.d1.loss_cls: 0.4191  decode.d1.loss_mask: 2.1149  decode.d1.loss_dice: 1.9394  decode.d2.loss_cls: 0.4232  decode.d2.loss_mask: 2.1275  decode.d2.loss_dice: 1.9678  decode.d3.loss_cls: 0.4092  decode.d3.loss_mask: 2.1362  decode.d3.loss_dice: 1.9829  decode.d4.loss_cls: 0.4434  decode.d4.loss_mask: 2.1179  decode.d4.loss_dice: 1.9527  decode.d5.loss_cls: 0.4401  decode.d5.loss_mask: 2.1105  decode.d5.loss_dice: 1.9405  decode.d6.loss_cls: 0.4611  decode.d6.loss_mask: 2.1396  decode.d6.loss_dice: 1.9587  decode.d7.loss_cls: 0.4443  decode.d7.loss_mask: 2.1120  decode.d7.loss_dice: 1.9662  decode.d8.loss_cls: 0.4939  decode.d8.loss_mask: 2.1323  decode.d8.loss_dice: 1.9798  mix_decode.loss_cls: 0.2311  mix_decode.loss_mask: 0.5116  mix_decode.loss_dice: 0.5749  mix_decode.d0.loss_cls: 0.2727  mix_decode.d0.loss_mask: 0.5272  mix_decode.d0.loss_dice: 0.6999  mix_decode.d1.loss_cls: 0.2377  mix_decode.d1.loss_mask: 0.5165  mix_decode.d1.loss_dice: 0.5935  mix_decode.d2.loss_cls: 0.2315  mix_decode.d2.loss_mask: 0.5325  mix_decode.d2.loss_dice: 0.6034  mix_decode.d3.loss_cls: 0.2314  mix_decode.d3.loss_mask: 0.5172  mix_decode.d3.loss_dice: 0.5925  mix_decode.d4.loss_cls: 0.2351  mix_decode.d4.loss_mask: 0.5176  mix_decode.d4.loss_dice: 0.5713  mix_decode.d5.loss_cls: 0.2570  mix_decode.d5.loss_mask: 0.5079  mix_decode.d5.loss_dice: 0.5835  mix_decode.d6.loss_cls: 0.2343  mix_decode.d6.loss_mask: 0.5165  mix_decode.d6.loss_dice: 0.5845  mix_decode.d7.loss_cls: 0.2151  mix_decode.d7.loss_mask: 0.5334  mix_decode.d7.loss_dice: 0.5946  mix_decode.d8.loss_cls: 0.2519  mix_decode.d8.loss_mask: 0.5055  mix_decode.d8.loss_dice: 0.5831
2025/03/29 18:14:45 - mmengine - INFO - Iter(train) [19400/20000]  base_lr: 4.2602e-06 lr: 4.2602e-06  eta: 0:10:17  time: 1.1462  data_time: 0.0230  memory: 11210  loss: 56.2015  decode.loss_cls: 0.4518  decode.loss_mask: 1.8825  decode.loss_dice: 1.7425  decode.d0.loss_cls: 0.5749  decode.d0.loss_mask: 1.8549  decode.d0.loss_dice: 1.7960  decode.d1.loss_cls: 0.4728  decode.d1.loss_mask: 1.9218  decode.d1.loss_dice: 1.8055  decode.d2.loss_cls: 0.4265  decode.d2.loss_mask: 1.8905  decode.d2.loss_dice: 1.8035  decode.d3.loss_cls: 0.4162  decode.d3.loss_mask: 1.9325  decode.d3.loss_dice: 1.8156  decode.d4.loss_cls: 0.4388  decode.d4.loss_mask: 1.9099  decode.d4.loss_dice: 1.8255  decode.d5.loss_cls: 0.4412  decode.d5.loss_mask: 1.9216  decode.d5.loss_dice: 1.8066  decode.d6.loss_cls: 0.4694  decode.d6.loss_mask: 1.9148  decode.d6.loss_dice: 1.7732  decode.d7.loss_cls: 0.4792  decode.d7.loss_mask: 1.8705  decode.d7.loss_dice: 1.7663  decode.d8.loss_cls: 0.4547  decode.d8.loss_mask: 1.9134  decode.d8.loss_dice: 1.7397  mix_decode.loss_cls: 0.1899  mix_decode.loss_mask: 0.5984  mix_decode.loss_dice: 0.6705  mix_decode.d0.loss_cls: 0.2007  mix_decode.d0.loss_mask: 0.6230  mix_decode.d0.loss_dice: 0.7555  mix_decode.d1.loss_cls: 0.1861  mix_decode.d1.loss_mask: 0.6105  mix_decode.d1.loss_dice: 0.6643  mix_decode.d2.loss_cls: 0.2019  mix_decode.d2.loss_mask: 0.6048  mix_decode.d2.loss_dice: 0.6556  mix_decode.d3.loss_cls: 0.2071  mix_decode.d3.loss_mask: 0.5851  mix_decode.d3.loss_dice: 0.6388  mix_decode.d4.loss_cls: 0.1715  mix_decode.d4.loss_mask: 0.6000  mix_decode.d4.loss_dice: 0.6781  mix_decode.d5.loss_cls: 0.1875  mix_decode.d5.loss_mask: 0.5949  mix_decode.d5.loss_dice: 0.6763  mix_decode.d6.loss_cls: 0.1831  mix_decode.d6.loss_mask: 0.6174  mix_decode.d6.loss_dice: 0.6850  mix_decode.d7.loss_cls: 0.1893  mix_decode.d7.loss_mask: 0.5910  mix_decode.d7.loss_dice: 0.6730  mix_decode.d8.loss_cls: 0.1919  mix_decode.d8.loss_mask: 0.5935  mix_decode.d8.loss_dice: 0.6646
2025/03/29 18:15:43 - mmengine - INFO - Iter(train) [19450/20000]  base_lr: 3.9393e-06 lr: 3.9393e-06  eta: 0:09:26  time: 1.1515  data_time: 0.0227  memory: 11219  loss: 52.6099  decode.loss_cls: 0.3448  decode.loss_mask: 1.7441  decode.loss_dice: 1.5384  decode.d0.loss_cls: 0.4887  decode.d0.loss_mask: 1.7602  decode.d0.loss_dice: 1.6374  decode.d1.loss_cls: 0.3741  decode.d1.loss_mask: 1.6908  decode.d1.loss_dice: 1.5111  decode.d2.loss_cls: 0.3490  decode.d2.loss_mask: 1.7437  decode.d2.loss_dice: 1.5459  decode.d3.loss_cls: 0.3626  decode.d3.loss_mask: 1.7333  decode.d3.loss_dice: 1.5471  decode.d4.loss_cls: 0.3238  decode.d4.loss_mask: 1.7375  decode.d4.loss_dice: 1.5197  decode.d5.loss_cls: 0.3347  decode.d5.loss_mask: 1.7330  decode.d5.loss_dice: 1.5419  decode.d6.loss_cls: 0.3926  decode.d6.loss_mask: 1.7266  decode.d6.loss_dice: 1.5212  decode.d7.loss_cls: 0.3541  decode.d7.loss_mask: 1.7413  decode.d7.loss_dice: 1.5463  decode.d8.loss_cls: 0.3536  decode.d8.loss_mask: 1.7283  decode.d8.loss_dice: 1.5254  mix_decode.loss_cls: 0.2641  mix_decode.loss_mask: 0.5671  mix_decode.loss_dice: 0.7081  mix_decode.d0.loss_cls: 0.2928  mix_decode.d0.loss_mask: 0.5876  mix_decode.d0.loss_dice: 0.8114  mix_decode.d1.loss_cls: 0.3045  mix_decode.d1.loss_mask: 0.6003  mix_decode.d1.loss_dice: 0.7409  mix_decode.d2.loss_cls: 0.3669  mix_decode.d2.loss_mask: 0.5395  mix_decode.d2.loss_dice: 0.7160  mix_decode.d3.loss_cls: 0.3033  mix_decode.d3.loss_mask: 0.5568  mix_decode.d3.loss_dice: 0.7074  mix_decode.d4.loss_cls: 0.2529  mix_decode.d4.loss_mask: 0.6554  mix_decode.d4.loss_dice: 0.7593  mix_decode.d5.loss_cls: 0.3158  mix_decode.d5.loss_mask: 0.5571  mix_decode.d5.loss_dice: 0.7377  mix_decode.d6.loss_cls: 0.3088  mix_decode.d6.loss_mask: 0.6139  mix_decode.d6.loss_dice: 0.7300  mix_decode.d7.loss_cls: 0.3020  mix_decode.d7.loss_mask: 0.5698  mix_decode.d7.loss_dice: 0.7189  mix_decode.d8.loss_cls: 0.2891  mix_decode.d8.loss_mask: 0.5684  mix_decode.d8.loss_dice: 0.7128
2025/03/29 18:16:40 - mmengine - INFO - Iter(train) [19500/20000]  base_lr: 3.6155e-06 lr: 3.6155e-06  eta: 0:08:34  time: 1.1496  data_time: 0.0225  memory: 11219  loss: 47.5908  decode.loss_cls: 0.3605  decode.loss_mask: 1.5775  decode.loss_dice: 1.5247  decode.d0.loss_cls: 0.4135  decode.d0.loss_mask: 1.5886  decode.d0.loss_dice: 1.5418  decode.d1.loss_cls: 0.4120  decode.d1.loss_mask: 1.5592  decode.d1.loss_dice: 1.4895  decode.d2.loss_cls: 0.3085  decode.d2.loss_mask: 1.6042  decode.d2.loss_dice: 1.5321  decode.d3.loss_cls: 0.3085  decode.d3.loss_mask: 1.5969  decode.d3.loss_dice: 1.5188  decode.d4.loss_cls: 0.3203  decode.d4.loss_mask: 1.6202  decode.d4.loss_dice: 1.5639  decode.d5.loss_cls: 0.3196  decode.d5.loss_mask: 1.6146  decode.d5.loss_dice: 1.5634  decode.d6.loss_cls: 0.3198  decode.d6.loss_mask: 1.5978  decode.d6.loss_dice: 1.5315  decode.d7.loss_cls: 0.3621  decode.d7.loss_mask: 1.5850  decode.d7.loss_dice: 1.5050  decode.d8.loss_cls: 0.3618  decode.d8.loss_mask: 1.6042  decode.d8.loss_dice: 1.5365  mix_decode.loss_cls: 0.1952  mix_decode.loss_mask: 0.5095  mix_decode.loss_dice: 0.5797  mix_decode.d0.loss_cls: 0.2383  mix_decode.d0.loss_mask: 0.4946  mix_decode.d0.loss_dice: 0.6193  mix_decode.d1.loss_cls: 0.2256  mix_decode.d1.loss_mask: 0.5006  mix_decode.d1.loss_dice: 0.5781  mix_decode.d2.loss_cls: 0.1734  mix_decode.d2.loss_mask: 0.5064  mix_decode.d2.loss_dice: 0.5796  mix_decode.d3.loss_cls: 0.1472  mix_decode.d3.loss_mask: 0.5079  mix_decode.d3.loss_dice: 0.5776  mix_decode.d4.loss_cls: 0.1421  mix_decode.d4.loss_mask: 0.5240  mix_decode.d4.loss_dice: 0.5888  mix_decode.d5.loss_cls: 0.1697  mix_decode.d5.loss_mask: 0.5157  mix_decode.d5.loss_dice: 0.5918  mix_decode.d6.loss_cls: 0.1715  mix_decode.d6.loss_mask: 0.5139  mix_decode.d6.loss_dice: 0.6035  mix_decode.d7.loss_cls: 0.1753  mix_decode.d7.loss_mask: 0.5393  mix_decode.d7.loss_dice: 0.5845  mix_decode.d8.loss_cls: 0.1940  mix_decode.d8.loss_mask: 0.5211  mix_decode.d8.loss_dice: 0.5806
2025/03/29 18:17:38 - mmengine - INFO - Iter(train) [19550/20000]  base_lr: 3.2884e-06 lr: 3.2884e-06  eta: 0:07:43  time: 1.1468  data_time: 0.0227  memory: 11206  loss: 55.3949  decode.loss_cls: 0.5446  decode.loss_mask: 1.5865  decode.loss_dice: 1.7383  decode.d0.loss_cls: 0.5521  decode.d0.loss_mask: 1.6019  decode.d0.loss_dice: 1.8776  decode.d1.loss_cls: 0.5626  decode.d1.loss_mask: 1.5553  decode.d1.loss_dice: 1.7025  decode.d2.loss_cls: 0.5882  decode.d2.loss_mask: 1.5671  decode.d2.loss_dice: 1.7094  decode.d3.loss_cls: 0.5461  decode.d3.loss_mask: 1.6081  decode.d3.loss_dice: 1.6965  decode.d4.loss_cls: 0.5062  decode.d4.loss_mask: 1.6005  decode.d4.loss_dice: 1.7359  decode.d5.loss_cls: 0.5689  decode.d5.loss_mask: 1.5833  decode.d5.loss_dice: 1.7279  decode.d6.loss_cls: 0.5762  decode.d6.loss_mask: 1.5779  decode.d6.loss_dice: 1.7731  decode.d7.loss_cls: 0.5714  decode.d7.loss_mask: 1.5743  decode.d7.loss_dice: 1.7216  decode.d8.loss_cls: 0.5987  decode.d8.loss_mask: 1.5955  decode.d8.loss_dice: 1.7200  mix_decode.loss_cls: 0.2580  mix_decode.loss_mask: 0.6041  mix_decode.loss_dice: 0.7389  mix_decode.d0.loss_cls: 0.2895  mix_decode.d0.loss_mask: 0.5970  mix_decode.d0.loss_dice: 0.8167  mix_decode.d1.loss_cls: 0.3164  mix_decode.d1.loss_mask: 0.6028  mix_decode.d1.loss_dice: 0.7469  mix_decode.d2.loss_cls: 0.3237  mix_decode.d2.loss_mask: 0.6011  mix_decode.d2.loss_dice: 0.7236  mix_decode.d3.loss_cls: 0.3002  mix_decode.d3.loss_mask: 0.6036  mix_decode.d3.loss_dice: 0.7327  mix_decode.d4.loss_cls: 0.2791  mix_decode.d4.loss_mask: 0.6050  mix_decode.d4.loss_dice: 0.7315  mix_decode.d5.loss_cls: 0.3302  mix_decode.d5.loss_mask: 0.6037  mix_decode.d5.loss_dice: 0.7451  mix_decode.d6.loss_cls: 0.3346  mix_decode.d6.loss_mask: 0.6022  mix_decode.d6.loss_dice: 0.7238  mix_decode.d7.loss_cls: 0.3472  mix_decode.d7.loss_mask: 0.5853  mix_decode.d7.loss_dice: 0.7389  mix_decode.d8.loss_cls: 0.3162  mix_decode.d8.loss_mask: 0.5960  mix_decode.d8.loss_dice: 0.7324
2025/03/29 18:18:36 - mmengine - INFO - Iter(train) [19600/20000]  base_lr: 2.9576e-06 lr: 2.9576e-06  eta: 0:06:52  time: 1.1477  data_time: 0.0228  memory: 11219  loss: 50.1711  decode.loss_cls: 0.2438  decode.loss_mask: 1.7220  decode.loss_dice: 1.6253  decode.d0.loss_cls: 0.4211  decode.d0.loss_mask: 1.7468  decode.d0.loss_dice: 1.6981  decode.d1.loss_cls: 0.2741  decode.d1.loss_mask: 1.7419  decode.d1.loss_dice: 1.5990  decode.d2.loss_cls: 0.2785  decode.d2.loss_mask: 1.7207  decode.d2.loss_dice: 1.5921  decode.d3.loss_cls: 0.2669  decode.d3.loss_mask: 1.6917  decode.d3.loss_dice: 1.5601  decode.d4.loss_cls: 0.2717  decode.d4.loss_mask: 1.7290  decode.d4.loss_dice: 1.6195  decode.d5.loss_cls: 0.2854  decode.d5.loss_mask: 1.7180  decode.d5.loss_dice: 1.5484  decode.d6.loss_cls: 0.2723  decode.d6.loss_mask: 1.7172  decode.d6.loss_dice: 1.6328  decode.d7.loss_cls: 0.3072  decode.d7.loss_mask: 1.7073  decode.d7.loss_dice: 1.5920  decode.d8.loss_cls: 0.2549  decode.d8.loss_mask: 1.7327  decode.d8.loss_dice: 1.6680  mix_decode.loss_cls: 0.1869  mix_decode.loss_mask: 0.5639  mix_decode.loss_dice: 0.6190  mix_decode.d0.loss_cls: 0.2555  mix_decode.d0.loss_mask: 0.5374  mix_decode.d0.loss_dice: 0.7107  mix_decode.d1.loss_cls: 0.2227  mix_decode.d1.loss_mask: 0.5617  mix_decode.d1.loss_dice: 0.6274  mix_decode.d2.loss_cls: 0.2042  mix_decode.d2.loss_mask: 0.5511  mix_decode.d2.loss_dice: 0.6199  mix_decode.d3.loss_cls: 0.2111  mix_decode.d3.loss_mask: 0.5314  mix_decode.d3.loss_dice: 0.5933  mix_decode.d4.loss_cls: 0.1980  mix_decode.d4.loss_mask: 0.5567  mix_decode.d4.loss_dice: 0.6328  mix_decode.d5.loss_cls: 0.1610  mix_decode.d5.loss_mask: 0.5711  mix_decode.d5.loss_dice: 0.6501  mix_decode.d6.loss_cls: 0.1738  mix_decode.d6.loss_mask: 0.5685  mix_decode.d6.loss_dice: 0.6507  mix_decode.d7.loss_cls: 0.2142  mix_decode.d7.loss_mask: 0.5624  mix_decode.d7.loss_dice: 0.5989  mix_decode.d8.loss_cls: 0.2203  mix_decode.d8.loss_mask: 0.5587  mix_decode.d8.loss_dice: 0.6194
2025/03/29 18:19:33 - mmengine - INFO - Iter(train) [19650/20000]  base_lr: 2.6227e-06 lr: 2.6227e-06  eta: 0:06:00  time: 1.1537  data_time: 0.0226  memory: 11213  loss: 51.4545  decode.loss_cls: 0.2227  decode.loss_mask: 1.8360  decode.loss_dice: 1.5686  decode.d0.loss_cls: 0.3043  decode.d0.loss_mask: 1.8940  decode.d0.loss_dice: 1.6958  decode.d1.loss_cls: 0.2129  decode.d1.loss_mask: 1.8567  decode.d1.loss_dice: 1.6077  decode.d2.loss_cls: 0.2523  decode.d2.loss_mask: 1.8565  decode.d2.loss_dice: 1.5666  decode.d3.loss_cls: 0.2472  decode.d3.loss_mask: 1.8276  decode.d3.loss_dice: 1.5818  decode.d4.loss_cls: 0.1880  decode.d4.loss_mask: 1.8669  decode.d4.loss_dice: 1.6015  decode.d5.loss_cls: 0.1252  decode.d5.loss_mask: 1.8690  decode.d5.loss_dice: 1.6289  decode.d6.loss_cls: 0.1588  decode.d6.loss_mask: 1.8705  decode.d6.loss_dice: 1.6262  decode.d7.loss_cls: 0.1587  decode.d7.loss_mask: 1.8495  decode.d7.loss_dice: 1.6003  decode.d8.loss_cls: 0.2297  decode.d8.loss_mask: 1.8512  decode.d8.loss_dice: 1.5947  mix_decode.loss_cls: 0.1890  mix_decode.loss_mask: 0.5611  mix_decode.loss_dice: 0.6973  mix_decode.d0.loss_cls: 0.1948  mix_decode.d0.loss_mask: 0.5402  mix_decode.d0.loss_dice: 0.7849  mix_decode.d1.loss_cls: 0.2014  mix_decode.d1.loss_mask: 0.5560  mix_decode.d1.loss_dice: 0.7038  mix_decode.d2.loss_cls: 0.2389  mix_decode.d2.loss_mask: 0.5471  mix_decode.d2.loss_dice: 0.6927  mix_decode.d3.loss_cls: 0.2287  mix_decode.d3.loss_mask: 0.5517  mix_decode.d3.loss_dice: 0.6887  mix_decode.d4.loss_cls: 0.1889  mix_decode.d4.loss_mask: 0.5673  mix_decode.d4.loss_dice: 0.6830  mix_decode.d5.loss_cls: 0.1890  mix_decode.d5.loss_mask: 0.5788  mix_decode.d5.loss_dice: 0.7085  mix_decode.d6.loss_cls: 0.1816  mix_decode.d6.loss_mask: 0.5893  mix_decode.d6.loss_dice: 0.7126  mix_decode.d7.loss_cls: 0.2326  mix_decode.d7.loss_mask: 0.5453  mix_decode.d7.loss_dice: 0.7136  mix_decode.d8.loss_cls: 0.1880  mix_decode.d8.loss_mask: 0.5494  mix_decode.d8.loss_dice: 0.7006
2025/03/29 18:20:31 - mmengine - INFO - Iter(train) [19700/20000]  base_lr: 2.2830e-06 lr: 2.2830e-06  eta: 0:05:09  time: 1.1534  data_time: 0.0228  memory: 11216  loss: 54.6810  decode.loss_cls: 0.4263  decode.loss_mask: 1.7102  decode.loss_dice: 1.8503  decode.d0.loss_cls: 0.4941  decode.d0.loss_mask: 1.7043  decode.d0.loss_dice: 2.0100  decode.d1.loss_cls: 0.3862  decode.d1.loss_mask: 1.6940  decode.d1.loss_dice: 1.8895  decode.d2.loss_cls: 0.4531  decode.d2.loss_mask: 1.6892  decode.d2.loss_dice: 1.8879  decode.d3.loss_cls: 0.4277  decode.d3.loss_mask: 1.6948  decode.d3.loss_dice: 1.8890  decode.d4.loss_cls: 0.4136  decode.d4.loss_mask: 1.6854  decode.d4.loss_dice: 1.8595  decode.d5.loss_cls: 0.3946  decode.d5.loss_mask: 1.6675  decode.d5.loss_dice: 1.8803  decode.d6.loss_cls: 0.3859  decode.d6.loss_mask: 1.6864  decode.d6.loss_dice: 1.8757  decode.d7.loss_cls: 0.4307  decode.d7.loss_mask: 1.6762  decode.d7.loss_dice: 1.8830  decode.d8.loss_cls: 0.3782  decode.d8.loss_mask: 1.7032  decode.d8.loss_dice: 1.8671  mix_decode.loss_cls: 0.2892  mix_decode.loss_mask: 0.4847  mix_decode.loss_dice: 0.6833  mix_decode.d0.loss_cls: 0.3321  mix_decode.d0.loss_mask: 0.4743  mix_decode.d0.loss_dice: 0.7722  mix_decode.d1.loss_cls: 0.2827  mix_decode.d1.loss_mask: 0.4742  mix_decode.d1.loss_dice: 0.7035  mix_decode.d2.loss_cls: 0.3032  mix_decode.d2.loss_mask: 0.4728  mix_decode.d2.loss_dice: 0.7054  mix_decode.d3.loss_cls: 0.2837  mix_decode.d3.loss_mask: 0.4711  mix_decode.d3.loss_dice: 0.6967  mix_decode.d4.loss_cls: 0.2617  mix_decode.d4.loss_mask: 0.4759  mix_decode.d4.loss_dice: 0.6973  mix_decode.d5.loss_cls: 0.2785  mix_decode.d5.loss_mask: 0.4784  mix_decode.d5.loss_dice: 0.6822  mix_decode.d6.loss_cls: 0.2692  mix_decode.d6.loss_mask: 0.4748  mix_decode.d6.loss_dice: 0.6962  mix_decode.d7.loss_cls: 0.2864  mix_decode.d7.loss_mask: 0.4790  mix_decode.d7.loss_dice: 0.7060  mix_decode.d8.loss_cls: 0.2704  mix_decode.d8.loss_mask: 0.4941  mix_decode.d8.loss_dice: 0.7076
2025/03/29 18:21:28 - mmengine - INFO - Iter(train) [19750/20000]  base_lr: 1.9375e-06 lr: 1.9375e-06  eta: 0:04:17  time: 1.1514  data_time: 0.0225  memory: 11210  loss: 58.0986  decode.loss_cls: 0.2474  decode.loss_mask: 2.1237  decode.loss_dice: 1.8339  decode.d0.loss_cls: 0.3853  decode.d0.loss_mask: 2.1338  decode.d0.loss_dice: 1.8367  decode.d1.loss_cls: 0.2644  decode.d1.loss_mask: 2.1057  decode.d1.loss_dice: 1.8383  decode.d2.loss_cls: 0.2768  decode.d2.loss_mask: 2.1055  decode.d2.loss_dice: 1.8123  decode.d3.loss_cls: 0.2872  decode.d3.loss_mask: 2.1082  decode.d3.loss_dice: 1.8076  decode.d4.loss_cls: 0.2199  decode.d4.loss_mask: 2.1297  decode.d4.loss_dice: 1.8788  decode.d5.loss_cls: 0.2213  decode.d5.loss_mask: 2.1176  decode.d5.loss_dice: 1.8693  decode.d6.loss_cls: 0.2402  decode.d6.loss_mask: 2.1036  decode.d6.loss_dice: 1.8729  decode.d7.loss_cls: 0.2472  decode.d7.loss_mask: 2.1108  decode.d7.loss_dice: 1.8368  decode.d8.loss_cls: 0.2380  decode.d8.loss_mask: 2.1264  decode.d8.loss_dice: 1.8717  mix_decode.loss_cls: 0.2408  mix_decode.loss_mask: 0.6616  mix_decode.loss_dice: 0.6361  mix_decode.d0.loss_cls: 0.3014  mix_decode.d0.loss_mask: 0.6423  mix_decode.d0.loss_dice: 0.7003  mix_decode.d1.loss_cls: 0.2283  mix_decode.d1.loss_mask: 0.6675  mix_decode.d1.loss_dice: 0.6441  mix_decode.d2.loss_cls: 0.2792  mix_decode.d2.loss_mask: 0.6761  mix_decode.d2.loss_dice: 0.6289  mix_decode.d3.loss_cls: 0.2611  mix_decode.d3.loss_mask: 0.6627  mix_decode.d3.loss_dice: 0.6432  mix_decode.d4.loss_cls: 0.2292  mix_decode.d4.loss_mask: 0.6631  mix_decode.d4.loss_dice: 0.6579  mix_decode.d5.loss_cls: 0.2717  mix_decode.d5.loss_mask: 0.6703  mix_decode.d5.loss_dice: 0.6761  mix_decode.d6.loss_cls: 0.2294  mix_decode.d6.loss_mask: 0.6876  mix_decode.d6.loss_dice: 0.6736  mix_decode.d7.loss_cls: 0.2411  mix_decode.d7.loss_mask: 0.7244  mix_decode.d7.loss_dice: 0.6629  mix_decode.d8.loss_cls: 0.2523  mix_decode.d8.loss_mask: 0.6715  mix_decode.d8.loss_dice: 0.6630
2025/03/29 18:22:26 - mmengine - INFO - Iter(train) [19800/20000]  base_lr: 1.5850e-06 lr: 1.5850e-06  eta: 0:03:26  time: 1.1520  data_time: 0.0226  memory: 11213  loss: 51.5859  decode.loss_cls: 0.3790  decode.loss_mask: 1.5805  decode.loss_dice: 1.6548  decode.d0.loss_cls: 0.4775  decode.d0.loss_mask: 1.6579  decode.d0.loss_dice: 1.7871  decode.d1.loss_cls: 0.4298  decode.d1.loss_mask: 1.6295  decode.d1.loss_dice: 1.6139  decode.d2.loss_cls: 0.3972  decode.d2.loss_mask: 1.6279  decode.d2.loss_dice: 1.6043  decode.d3.loss_cls: 0.3697  decode.d3.loss_mask: 1.6191  decode.d3.loss_dice: 1.6840  decode.d4.loss_cls: 0.3893  decode.d4.loss_mask: 1.6472  decode.d4.loss_dice: 1.7156  decode.d5.loss_cls: 0.4008  decode.d5.loss_mask: 1.6178  decode.d5.loss_dice: 1.6627  decode.d6.loss_cls: 0.3765  decode.d6.loss_mask: 1.6071  decode.d6.loss_dice: 1.6433  decode.d7.loss_cls: 0.3995  decode.d7.loss_mask: 1.6312  decode.d7.loss_dice: 1.6852  decode.d8.loss_cls: 0.4298  decode.d8.loss_mask: 1.5998  decode.d8.loss_dice: 1.6390  mix_decode.loss_cls: 0.2310  mix_decode.loss_mask: 0.5176  mix_decode.loss_dice: 0.6643  mix_decode.d0.loss_cls: 0.2666  mix_decode.d0.loss_mask: 0.5148  mix_decode.d0.loss_dice: 0.7629  mix_decode.d1.loss_cls: 0.2558  mix_decode.d1.loss_mask: 0.5185  mix_decode.d1.loss_dice: 0.6735  mix_decode.d2.loss_cls: 0.2647  mix_decode.d2.loss_mask: 0.5222  mix_decode.d2.loss_dice: 0.6579  mix_decode.d3.loss_cls: 0.2661  mix_decode.d3.loss_mask: 0.5191  mix_decode.d3.loss_dice: 0.6706  mix_decode.d4.loss_cls: 0.2658  mix_decode.d4.loss_mask: 0.5197  mix_decode.d4.loss_dice: 0.6689  mix_decode.d5.loss_cls: 0.3151  mix_decode.d5.loss_mask: 0.5022  mix_decode.d5.loss_dice: 0.6700  mix_decode.d6.loss_cls: 0.2776  mix_decode.d6.loss_mask: 0.5079  mix_decode.d6.loss_dice: 0.6683  mix_decode.d7.loss_cls: 0.2833  mix_decode.d7.loss_mask: 0.5042  mix_decode.d7.loss_dice: 0.6815  mix_decode.d8.loss_cls: 0.2653  mix_decode.d8.loss_mask: 0.5079  mix_decode.d8.loss_dice: 0.6858
2025/03/29 18:23:24 - mmengine - INFO - Iter(train) [19850/20000]  base_lr: 1.2234e-06 lr: 1.2234e-06  eta: 0:02:34  time: 1.1470  data_time: 0.0226  memory: 11218  loss: 47.1127  decode.loss_cls: 0.3500  decode.loss_mask: 1.4202  decode.loss_dice: 1.4263  decode.d0.loss_cls: 0.4243  decode.d0.loss_mask: 1.4687  decode.d0.loss_dice: 1.6112  decode.d1.loss_cls: 0.4319  decode.d1.loss_mask: 1.3574  decode.d1.loss_dice: 1.3839  decode.d2.loss_cls: 0.3641  decode.d2.loss_mask: 1.3753  decode.d2.loss_dice: 1.4140  decode.d3.loss_cls: 0.4039  decode.d3.loss_mask: 1.3880  decode.d3.loss_dice: 1.4214  decode.d4.loss_cls: 0.4139  decode.d4.loss_mask: 1.3931  decode.d4.loss_dice: 1.4369  decode.d5.loss_cls: 0.4064  decode.d5.loss_mask: 1.4107  decode.d5.loss_dice: 1.4401  decode.d6.loss_cls: 0.4053  decode.d6.loss_mask: 1.3824  decode.d6.loss_dice: 1.4108  decode.d7.loss_cls: 0.4170  decode.d7.loss_mask: 1.3977  decode.d7.loss_dice: 1.4231  decode.d8.loss_cls: 0.3898  decode.d8.loss_mask: 1.4397  decode.d8.loss_dice: 1.4059  mix_decode.loss_cls: 0.2624  mix_decode.loss_mask: 0.5008  mix_decode.loss_dice: 0.6685  mix_decode.d0.loss_cls: 0.3131  mix_decode.d0.loss_mask: 0.5069  mix_decode.d0.loss_dice: 0.7202  mix_decode.d1.loss_cls: 0.2950  mix_decode.d1.loss_mask: 0.5002  mix_decode.d1.loss_dice: 0.6712  mix_decode.d2.loss_cls: 0.2279  mix_decode.d2.loss_mask: 0.5261  mix_decode.d2.loss_dice: 0.6905  mix_decode.d3.loss_cls: 0.2440  mix_decode.d3.loss_mask: 0.5311  mix_decode.d3.loss_dice: 0.6638  mix_decode.d4.loss_cls: 0.2906  mix_decode.d4.loss_mask: 0.5039  mix_decode.d4.loss_dice: 0.6652  mix_decode.d5.loss_cls: 0.2804  mix_decode.d5.loss_mask: 0.5155  mix_decode.d5.loss_dice: 0.6959  mix_decode.d6.loss_cls: 0.2866  mix_decode.d6.loss_mask: 0.4895  mix_decode.d6.loss_dice: 0.6831  mix_decode.d7.loss_cls: 0.3054  mix_decode.d7.loss_mask: 0.5343  mix_decode.d7.loss_dice: 0.6714  mix_decode.d8.loss_cls: 0.2721  mix_decode.d8.loss_mask: 0.5072  mix_decode.d8.loss_dice: 0.6761
2025/03/29 18:24:21 - mmengine - INFO - Iter(train) [19900/20000]  base_lr: 8.4936e-07 lr: 8.4936e-07  eta: 0:01:43  time: 1.1522  data_time: 0.0228  memory: 11214  loss: 56.9758  decode.loss_cls: 0.3112  decode.loss_mask: 2.0782  decode.loss_dice: 1.6675  decode.d0.loss_cls: 0.3749  decode.d0.loss_mask: 2.1288  decode.d0.loss_dice: 1.7444  decode.d1.loss_cls: 0.3765  decode.d1.loss_mask: 2.0252  decode.d1.loss_dice: 1.6256  decode.d2.loss_cls: 0.3611  decode.d2.loss_mask: 2.0620  decode.d2.loss_dice: 1.6308  decode.d3.loss_cls: 0.2793  decode.d3.loss_mask: 2.0925  decode.d3.loss_dice: 1.6925  decode.d4.loss_cls: 0.3146  decode.d4.loss_mask: 2.1121  decode.d4.loss_dice: 1.6882  decode.d5.loss_cls: 0.2743  decode.d5.loss_mask: 2.1409  decode.d5.loss_dice: 1.7045  decode.d6.loss_cls: 0.2957  decode.d6.loss_mask: 2.0649  decode.d6.loss_dice: 1.7208  decode.d7.loss_cls: 0.2847  decode.d7.loss_mask: 2.1174  decode.d7.loss_dice: 1.6863  decode.d8.loss_cls: 0.3083  decode.d8.loss_mask: 2.0946  decode.d8.loss_dice: 1.7043  mix_decode.loss_cls: 0.2028  mix_decode.loss_mask: 0.6758  mix_decode.loss_dice: 0.6958  mix_decode.d0.loss_cls: 0.2642  mix_decode.d0.loss_mask: 0.6802  mix_decode.d0.loss_dice: 0.8008  mix_decode.d1.loss_cls: 0.2151  mix_decode.d1.loss_mask: 0.6563  mix_decode.d1.loss_dice: 0.7086  mix_decode.d2.loss_cls: 0.1886  mix_decode.d2.loss_mask: 0.6501  mix_decode.d2.loss_dice: 0.6903  mix_decode.d3.loss_cls: 0.1819  mix_decode.d3.loss_mask: 0.6633  mix_decode.d3.loss_dice: 0.7081  mix_decode.d4.loss_cls: 0.1829  mix_decode.d4.loss_mask: 0.6530  mix_decode.d4.loss_dice: 0.7077  mix_decode.d5.loss_cls: 0.1511  mix_decode.d5.loss_mask: 0.7002  mix_decode.d5.loss_dice: 0.7343  mix_decode.d6.loss_cls: 0.1830  mix_decode.d6.loss_mask: 0.7231  mix_decode.d6.loss_dice: 0.7344  mix_decode.d7.loss_cls: 0.2012  mix_decode.d7.loss_mask: 0.7185  mix_decode.d7.loss_dice: 0.7264  mix_decode.d8.loss_cls: 0.1789  mix_decode.d8.loss_mask: 0.7084  mix_decode.d8.loss_dice: 0.7287
2025/03/29 18:25:19 - mmengine - INFO - Iter(train) [19950/20000]  base_lr: 4.5516e-07 lr: 4.5516e-07  eta: 0:00:51  time: 1.1505  data_time: 0.0228  memory: 11216  loss: 50.6979  decode.loss_cls: 0.1581  decode.loss_mask: 1.8905  decode.loss_dice: 1.5793  decode.d0.loss_cls: 0.4054  decode.d0.loss_mask: 1.8834  decode.d0.loss_dice: 1.5862  decode.d1.loss_cls: 0.1890  decode.d1.loss_mask: 1.9015  decode.d1.loss_dice: 1.5819  decode.d2.loss_cls: 0.1606  decode.d2.loss_mask: 1.9099  decode.d2.loss_dice: 1.5847  decode.d3.loss_cls: 0.1736  decode.d3.loss_mask: 1.8989  decode.d3.loss_dice: 1.5819  decode.d4.loss_cls: 0.2331  decode.d4.loss_mask: 1.8893  decode.d4.loss_dice: 1.5861  decode.d5.loss_cls: 0.1619  decode.d5.loss_mask: 1.8816  decode.d5.loss_dice: 1.5870  decode.d6.loss_cls: 0.1627  decode.d6.loss_mask: 1.8943  decode.d6.loss_dice: 1.5769  decode.d7.loss_cls: 0.1720  decode.d7.loss_mask: 1.8956  decode.d7.loss_dice: 1.5634  decode.d8.loss_cls: 0.1586  decode.d8.loss_mask: 1.8941  decode.d8.loss_dice: 1.5754  mix_decode.loss_cls: 0.1637  mix_decode.loss_mask: 0.5518  mix_decode.loss_dice: 0.6161  mix_decode.d0.loss_cls: 0.2140  mix_decode.d0.loss_mask: 0.5799  mix_decode.d0.loss_dice: 0.6970  mix_decode.d1.loss_cls: 0.1652  mix_decode.d1.loss_mask: 0.5731  mix_decode.d1.loss_dice: 0.6625  mix_decode.d2.loss_cls: 0.2059  mix_decode.d2.loss_mask: 0.5697  mix_decode.d2.loss_dice: 0.6352  mix_decode.d3.loss_cls: 0.1530  mix_decode.d3.loss_mask: 0.5802  mix_decode.d3.loss_dice: 0.6371  mix_decode.d4.loss_cls: 0.2336  mix_decode.d4.loss_mask: 0.5531  mix_decode.d4.loss_dice: 0.6387  mix_decode.d5.loss_cls: 0.1765  mix_decode.d5.loss_mask: 0.5772  mix_decode.d5.loss_dice: 0.6387  mix_decode.d6.loss_cls: 0.2025  mix_decode.d6.loss_mask: 0.5510  mix_decode.d6.loss_dice: 0.6214  mix_decode.d7.loss_cls: 0.2046  mix_decode.d7.loss_mask: 0.5678  mix_decode.d7.loss_dice: 0.6258  mix_decode.d8.loss_cls: 0.2014  mix_decode.d8.loss_mask: 0.5639  mix_decode.d8.loss_dice: 0.6202
2025/03/29 18:26:17 - mmengine - INFO - Exp name: u2r_20250329_114708
2025/03/29 18:26:17 - mmengine - INFO - Iter(train) [20000/20000]  base_lr: 0.0000e+00 lr: 0.0000e+00  eta: 0:00:00  time: 1.1633  data_time: 0.0226  memory: 11209  loss: 57.1453  decode.loss_cls: 0.3610  decode.loss_mask: 1.8342  decode.loss_dice: 1.8292  decode.d0.loss_cls: 0.4356  decode.d0.loss_mask: 1.8652  decode.d0.loss_dice: 1.8871  decode.d1.loss_cls: 0.3587  decode.d1.loss_mask: 1.8676  decode.d1.loss_dice: 1.8661  decode.d2.loss_cls: 0.3690  decode.d2.loss_mask: 1.8557  decode.d2.loss_dice: 1.8374  decode.d3.loss_cls: 0.4039  decode.d3.loss_mask: 1.8406  decode.d3.loss_dice: 1.8007  decode.d4.loss_cls: 0.3879  decode.d4.loss_mask: 1.8422  decode.d4.loss_dice: 1.8308  decode.d5.loss_cls: 0.3944  decode.d5.loss_mask: 1.8343  decode.d5.loss_dice: 1.8233  decode.d6.loss_cls: 0.4165  decode.d6.loss_mask: 1.8275  decode.d6.loss_dice: 1.8068  decode.d7.loss_cls: 0.3749  decode.d7.loss_mask: 1.8537  decode.d7.loss_dice: 1.8433  decode.d8.loss_cls: 0.3953  decode.d8.loss_mask: 1.8528  decode.d8.loss_dice: 1.8268  mix_decode.loss_cls: 0.2825  mix_decode.loss_mask: 0.6709  mix_decode.loss_dice: 0.7191  mix_decode.d0.loss_cls: 0.2954  mix_decode.d0.loss_mask: 0.6310  mix_decode.d0.loss_dice: 0.7966  mix_decode.d1.loss_cls: 0.2749  mix_decode.d1.loss_mask: 0.6385  mix_decode.d1.loss_dice: 0.7053  mix_decode.d2.loss_cls: 0.3060  mix_decode.d2.loss_mask: 0.6435  mix_decode.d2.loss_dice: 0.6963  mix_decode.d3.loss_cls: 0.2673  mix_decode.d3.loss_mask: 0.6611  mix_decode.d3.loss_dice: 0.6948  mix_decode.d4.loss_cls: 0.2311  mix_decode.d4.loss_mask: 0.6602  mix_decode.d4.loss_dice: 0.7226  mix_decode.d5.loss_cls: 0.2497  mix_decode.d5.loss_mask: 0.6571  mix_decode.d5.loss_dice: 0.7348  mix_decode.d6.loss_cls: 0.2594  mix_decode.d6.loss_mask: 0.6435  mix_decode.d6.loss_dice: 0.7067  mix_decode.d7.loss_cls: 0.2478  mix_decode.d7.loss_mask: 0.6631  mix_decode.d7.loss_dice: 0.7208  mix_decode.d8.loss_cls: 0.2678  mix_decode.d8.loss_mask: 0.6437  mix_decode.d8.loss_dice: 0.7314
2025/03/29 18:26:17 - mmengine - INFO - Saving checkpoint at 20000 iterations
2025/03/29 18:26:22 - mmengine - INFO - Iter(val) [  50/3968]    eta: 0:05:58  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 18:26:27 - mmengine - INFO - Iter(val) [ 100/3968]    eta: 0:05:53  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 18:26:32 - mmengine - INFO - Iter(val) [ 150/3968]    eta: 0:05:48  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 18:26:36 - mmengine - INFO - Iter(val) [ 200/3968]    eta: 0:05:43  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 18:26:41 - mmengine - INFO - Iter(val) [ 250/3968]    eta: 0:05:38  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 18:26:45 - mmengine - INFO - Iter(val) [ 300/3968]    eta: 0:05:34  time: 0.0910  data_time: 0.0018  memory: 3080  
2025/03/29 18:26:50 - mmengine - INFO - Iter(val) [ 350/3968]    eta: 0:05:29  time: 0.0911  data_time: 0.0018  memory: 3080  
2025/03/29 18:26:54 - mmengine - INFO - Iter(val) [ 400/3968]    eta: 0:05:25  time: 0.0909  data_time: 0.0017  memory: 3080  
2025/03/29 18:26:59 - mmengine - INFO - Iter(val) [ 450/3968]    eta: 0:05:21  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:04 - mmengine - INFO - Iter(val) [ 500/3968]    eta: 0:05:16  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:08 - mmengine - INFO - Iter(val) [ 550/3968]    eta: 0:05:12  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:13 - mmengine - INFO - Iter(val) [ 600/3968]    eta: 0:05:07  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:17 - mmengine - INFO - Iter(val) [ 650/3968]    eta: 0:05:03  time: 0.0910  data_time: 0.0016  memory: 3080  
2025/03/29 18:27:22 - mmengine - INFO - Iter(val) [ 700/3968]    eta: 0:04:58  time: 0.0910  data_time: 0.0017  memory: 3080  
2025/03/29 18:27:26 - mmengine - INFO - Iter(val) [ 750/3968]    eta: 0:04:53  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:31 - mmengine - INFO - Iter(val) [ 800/3968]    eta: 0:04:49  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:36 - mmengine - INFO - Iter(val) [ 850/3968]    eta: 0:04:44  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 18:27:40 - mmengine - INFO - Iter(val) [ 900/3968]    eta: 0:04:40  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:45 - mmengine - INFO - Iter(val) [ 950/3968]    eta: 0:04:35  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 18:27:49 - mmengine - INFO - Iter(val) [1000/3968]    eta: 0:04:31  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:54 - mmengine - INFO - Iter(val) [1050/3968]    eta: 0:04:26  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:27:58 - mmengine - INFO - Iter(val) [1100/3968]    eta: 0:04:21  time: 0.0912  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:03 - mmengine - INFO - Iter(val) [1150/3968]    eta: 0:04:17  time: 0.0911  data_time: 0.0017  memory: 3080  
2025/03/29 18:28:08 - mmengine - INFO - Iter(val) [1200/3968]    eta: 0:04:12  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:12 - mmengine - INFO - Iter(val) [1250/3968]    eta: 0:04:08  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:17 - mmengine - INFO - Iter(val) [1300/3968]    eta: 0:04:03  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:21 - mmengine - INFO - Iter(val) [1350/3968]    eta: 0:03:59  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:26 - mmengine - INFO - Iter(val) [1400/3968]    eta: 0:03:54  time: 0.0913  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:31 - mmengine - INFO - Iter(val) [1450/3968]    eta: 0:03:50  time: 0.0942  data_time: 0.0021  memory: 3080  
2025/03/29 18:28:35 - mmengine - INFO - Iter(val) [1500/3968]    eta: 0:03:45  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 18:28:40 - mmengine - INFO - Iter(val) [1550/3968]    eta: 0:03:41  time: 0.0913  data_time: 0.0017  memory: 3080  
2025/03/29 18:28:44 - mmengine - INFO - Iter(val) [1600/3968]    eta: 0:03:36  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 18:28:49 - mmengine - INFO - Iter(val) [1650/3968]    eta: 0:03:32  time: 0.0941  data_time: 0.0019  memory: 3080  
2025/03/29 18:28:54 - mmengine - INFO - Iter(val) [1700/3968]    eta: 0:03:27  time: 0.0918  data_time: 0.0019  memory: 3080  
2025/03/29 18:28:58 - mmengine - INFO - Iter(val) [1750/3968]    eta: 0:03:22  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:03 - mmengine - INFO - Iter(val) [1800/3968]    eta: 0:03:18  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:07 - mmengine - INFO - Iter(val) [1850/3968]    eta: 0:03:13  time: 0.0917  data_time: 0.0019  memory: 3080  
2025/03/29 18:29:12 - mmengine - INFO - Iter(val) [1900/3968]    eta: 0:03:09  time: 0.0914  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:17 - mmengine - INFO - Iter(val) [1950/3968]    eta: 0:03:04  time: 0.0915  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:21 - mmengine - INFO - Iter(val) [2000/3968]    eta: 0:03:00  time: 0.0917  data_time: 0.0019  memory: 3080  
2025/03/29 18:29:26 - mmengine - INFO - Iter(val) [2050/3968]    eta: 0:02:55  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:30 - mmengine - INFO - Iter(val) [2100/3968]    eta: 0:02:51  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:35 - mmengine - INFO - Iter(val) [2150/3968]    eta: 0:02:46  time: 0.0917  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:40 - mmengine - INFO - Iter(val) [2200/3968]    eta: 0:02:41  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:44 - mmengine - INFO - Iter(val) [2250/3968]    eta: 0:02:37  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:49 - mmengine - INFO - Iter(val) [2300/3968]    eta: 0:02:32  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:29:53 - mmengine - INFO - Iter(val) [2350/3968]    eta: 0:02:28  time: 0.0917  data_time: 0.0017  memory: 3080  
2025/03/29 18:29:58 - mmengine - INFO - Iter(val) [2400/3968]    eta: 0:02:23  time: 0.0917  data_time: 0.0019  memory: 3080  
2025/03/29 18:30:03 - mmengine - INFO - Iter(val) [2450/3968]    eta: 0:02:19  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:07 - mmengine - INFO - Iter(val) [2500/3968]    eta: 0:02:14  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:12 - mmengine - INFO - Iter(val) [2550/3968]    eta: 0:02:09  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:16 - mmengine - INFO - Iter(val) [2600/3968]    eta: 0:02:05  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:21 - mmengine - INFO - Iter(val) [2650/3968]    eta: 0:02:00  time: 0.0918  data_time: 0.0017  memory: 3080  
2025/03/29 18:30:26 - mmengine - INFO - Iter(val) [2700/3968]    eta: 0:01:56  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:30 - mmengine - INFO - Iter(val) [2750/3968]    eta: 0:01:51  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:35 - mmengine - INFO - Iter(val) [2800/3968]    eta: 0:01:46  time: 0.0915  data_time: 0.0017  memory: 3080  
2025/03/29 18:30:39 - mmengine - INFO - Iter(val) [2850/3968]    eta: 0:01:42  time: 0.0917  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:44 - mmengine - INFO - Iter(val) [2900/3968]    eta: 0:01:37  time: 0.0923  data_time: 0.0019  memory: 3080  
2025/03/29 18:30:49 - mmengine - INFO - Iter(val) [2950/3968]    eta: 0:01:33  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:53 - mmengine - INFO - Iter(val) [3000/3968]    eta: 0:01:28  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:30:58 - mmengine - INFO - Iter(val) [3050/3968]    eta: 0:01:24  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:02 - mmengine - INFO - Iter(val) [3100/3968]    eta: 0:01:19  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:07 - mmengine - INFO - Iter(val) [3150/3968]    eta: 0:01:14  time: 0.0918  data_time: 0.0019  memory: 3080  
2025/03/29 18:31:12 - mmengine - INFO - Iter(val) [3200/3968]    eta: 0:01:10  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:16 - mmengine - INFO - Iter(val) [3250/3968]    eta: 0:01:05  time: 0.0916  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:21 - mmengine - INFO - Iter(val) [3300/3968]    eta: 0:01:01  time: 0.0916  data_time: 0.0017  memory: 3080  
2025/03/29 18:31:25 - mmengine - INFO - Iter(val) [3350/3968]    eta: 0:00:56  time: 0.0917  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:30 - mmengine - INFO - Iter(val) [3400/3968]    eta: 0:00:52  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:35 - mmengine - INFO - Iter(val) [3450/3968]    eta: 0:00:47  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:39 - mmengine - INFO - Iter(val) [3500/3968]    eta: 0:00:42  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:44 - mmengine - INFO - Iter(val) [3550/3968]    eta: 0:00:38  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:48 - mmengine - INFO - Iter(val) [3600/3968]    eta: 0:00:33  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:53 - mmengine - INFO - Iter(val) [3650/3968]    eta: 0:00:29  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:31:58 - mmengine - INFO - Iter(val) [3700/3968]    eta: 0:00:24  time: 0.0921  data_time: 0.0019  memory: 3080  
2025/03/29 18:32:02 - mmengine - INFO - Iter(val) [3750/3968]    eta: 0:00:19  time: 0.0919  data_time: 0.0018  memory: 3080  
2025/03/29 18:32:07 - mmengine - INFO - Iter(val) [3800/3968]    eta: 0:00:15  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:32:12 - mmengine - INFO - Iter(val) [3850/3968]    eta: 0:00:10  time: 0.0921  data_time: 0.0018  memory: 3080  
2025/03/29 18:32:16 - mmengine - INFO - Iter(val) [3900/3968]    eta: 0:00:06  time: 0.0920  data_time: 0.0018  memory: 3080  
2025/03/29 18:32:21 - mmengine - INFO - Iter(val) [3950/3968]    eta: 0:00:01  time: 0.0918  data_time: 0.0018  memory: 3080  
2025/03/29 18:32:22 - mmengine - INFO - per class results:
2025/03/29 18:32:22 - mmengine - INFO - 
+--------------+-------+-------+
|    Class     |  IoU  |  Acc  |
+--------------+-------+-------+
|  background  | 57.88 | 84.61 |
|   building   |  62.4 |  71.9 |
|     road     | 46.49 | 51.53 |
|    water     | 66.77 | 75.18 |
|    barren    | 13.91 | 16.01 |
|    forest    | 27.03 | 30.32 |
| agricultural | 60.28 | 69.15 |
+--------------+-------+-------+
2025/03/29 18:32:22 - mmengine - INFO - Iter(val) [3968/3968]    aAcc: 72.2300  mIoU: 47.8200  mAcc: 56.9600  data_time: 0.0018  time: 0.0917
